{"pages":[],"posts":[{"title":"Github + Hexo 搭建博客(二)","text":"3 创建Github SSH连接Github支持https和ssh两种方式管理本地和远程代码的同步，作为个人repo的拥有者，推荐使用ssh方式，因为每次写博客上传远端库时可以省略用户名和密码的验证。 ① 创建ssh key 1$ ssh-keygen -t rsa -C &quot;your_email@example.com&quot; “your_email@example.com”使用github个人账户邮箱，创建成功后连续三次回车，以下分别说明 12Generating public/private rsa key pair.# Enter file in which to save the key (/c/Users/you/.ssh/id_rsa): [Press enter] 第一次要求输入保存公钥和秘钥的文件路径，直接回车视为默认，保存在/c/Users/you/.ssh/id_rsa下 12Enter passphrase (empty for no passphrase): # Enter same passphrase again: 接下来两年次回车是提交密码和密码确认，可以默认无密码。 当然，如果要设置密码需要说明，该密码仅为push本地代码到远端时的密码，而非账户密码，建议省略。 接下来，就会完成创建并保存到本地 1234Your identification has been saved in /c/Users/you/.ssh/id_rsa.# Your public key has been saved in /c/Users/you/.ssh/id_rsa.pub.# The key fingerprint is:# 01:0f:f4:3b:ca:85:d6:17:a1:7d:f0:68:9d:f0:a2:db your_email@example.com ② 绑定ssh key 在保存路径下找到id_rsa.pub，打开并复制其中的内容 登录github，在个人Setting下找到SSH and GPG keys 并将复制的内容添加一个新的SSH Key。 至此，绑定ssh完毕 ③ 测试ssh连接在git bash中输入以下命令 12345$ ssh -T git@github.comThe authenticity of host 'github.com (207.97.227.239)' can't be established.# RSA key fingerprint is 16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48.# Are you sure you want to continue connecting (yes/no)? 显示警告无妨，输入yes并继续，会得到最终测试的结果。 12Hi username! You've successfully authenticated, but GitHub does not# provide shell access. 如果显示successfully authenticated表示ssh授权成功，可以使用； 相反，如果提示access denied表示失败，可以删除已绑定的ssh并重新进行②步骤，注意粘贴内容不含其它字符包括空格和回车。 4 将初始化博客部署至github上一节最终已经在本地初始化了hexo博客并且正常运行，接下来需要部署至github的仓库并通过互联网域名进行访问 ① 首先，需要在根路径安装自动部署插件 1d:\\blog &gt; npm install hexo-deployer-git --save ② 配置部署信息 首先，访问github，并在上节创建的仓库中找到专属SSH连接地址，复制该地址 其次，在本地根路径下打开_config.yml文件，找到deploy对象并进行配置 说明1：由于建议使用ssh方式，这里type和repo都是ssh连接。当然也支持https方式，在②中就需要复制https对应得到地址配置在此处 说明2：github搭载的hexo只支持上传master分支，如果上传其他自定义分支是无法正常显示的。所以branch配置master分支 说明3: yml文件特别需要注意冒号后的一个空格，如果疏忽则会导致配置出错。并且yml文件受到缩进的严格限制来进行归类，这区别于properties文件和json文件。 ③ 自动化部 使用以下命令进行生成静态文件和部署操作。 1d:\\blog &gt; hexo d -g 或者 123d:\\blog &gt; hexo gd:\\blog &gt; hexo d 当显示部署成功后,就可以通过上节中配置的域名https:\\\\yourdomain.github.io进行访问，当然，你也可以访问github仓库查看本次上传的全部内容。 结尾 `OK，你期望的美好正在发生，但是觉得页面太丑？或者我如何发布自己写的博客，这将在下节展开`","link":"/2018/09/26/1%20Github%20+%20Hexo%20%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/2%20Github%20+%20Hexo%20%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2(%E4%BA%8C)/"},{"title":"Github + Hexo 搭建博客(四)","text":"7 Hexo写作和Markdown首先介绍Hexo博客的原理：本地编辑好的文本通过hexo generate命令，编译为静态html文本并发布到远端展示。换句话说，Hexo展示的内容都是Html文本形式的，这就意味着在本地编写的文本支持编译html。 这就引入了Markdown文本编辑 Markdown，是一种可以使用普通文本编辑器编写的标记语言，通过简单的标记语法，它可以使普通文本内容具有一定的格式(无法访问wiki，摘自百度百科解释)。 对于Markdown写作的规范和优势，可以自行百度，这里不多做解释，只需要记住一点，Markdown可以让我们更加关注内容的编写而不用花费更多精力在格式排版上。 既然作为标记语言，那么同样支持W3C规范的html标签。 比如: 一级标题、二级标题、三级标题等对应的Markdown格式为`#一级标题`、`##二级标题`、`###三级标题` 也可以使用Html标签`一级标题`、`二级标题`、`三级标题` Markdown文本编辑工具由于我个人在此之前长期使用印象笔记记录内容，搭配使用的是(马克飞象)[https://maxiang.io]Markdown编辑工具，良好的支持本地活在线编辑并和印象同步内容，然而马克飞象并不是当做其他md文本编辑的好工具。 在使用了一众md编辑工具后，最终还是选择安利Visual Studio Code。VS Code本身并不是md工具，只是微软做出来与WebStorm抗衡的前端IDE，然而它实在是太优秀了，并且很好的支持Markdown的编辑工作，甚至hexo根目录下的yml文本、前端各种文本都可以编辑和展示，软件反应灵敏且安装容量极小，这就让我爱不释手。 注意： 不同的md编辑工具可能某些语法并不相同，比如Latex公式、表格甚至链接和图片等。但总的md标记不变，只是在不同工具上渲染出的效果不一样. 这就好比不同的浏览器，内核不同，渲染出的网页效果也有一定的出入。具体最终渲染出的效果什么样还要遵循Hexo的md规范。 写作新建写作Ok，假设你已经掌握了md的基本语法，我们接下来可以正式进入编写博客了。 1d:\\blog &gt; hexo new myblog 使用该命令创建一个新的文章，myblog会保存在_posts文件中，该文件是博客存放和最终编译静态标记文本的路径。当然你也可以使用如下命令创建本次草稿，并最终移动到_posts下发布到远端（个人不推荐，原因是麻烦且没必要） 12345d:\\blog &gt; hexo new draft myblog #创建草稿myblog并在draft文件下保存# 编写草稿内容，完成后执行d:\\blog &gt; hexo publish [scaffold] #将草稿移动到_posts下，也可指定scaffold模板 写作模板说明1234567---title: Github + Hexo 搭建博客(四)date: 2018-9-30tags: hexotoc: hexo---# 内容 目前我采用的写作头文件是这样： title: 表示文章的标题，也是在博客首页最上方醒目显示的地方。 date： 文章写作时间 tags: 本所所属的标签，标签可以是单个标签，也可以是标签序列，如tags: [github, hexo, blog],需要在根目录的_config.yml中开启标签支持 toc： 文章目录。 申明头部之后，就可以在下方开始使用md书写内容了。 博客内图片Markdown内的图片常规引入方式为使用标记![text](imgurl)，然而在hexo中并不适用，这也是标记不一致的地方，由于Hexo要生成静态页面，需要在同级目录中创建保存静态文件的文件夹。具体方式如下：在根目录_config.yml中配置 1post_asset_folder: true 在使用命令d:/blog &gt; hexo new myblog后会在同级目录中创建同名文件夹，将需要引入文章的图片保存在该文件夹中并命名，比如test.jpg。在md文本中使用如下命令引入 1{% asset_img test.jpg Hexo %} 创建博客评论功能之——Gitment登录Gitment进行注册，注册成功后会获得client_id和client_secret,进入主题下的_congif.yml配置如下： 12345gitment_owner: alpaca #你的 GitHub IDgitment_repo: 'https://alpacaca.github.io/' #存储评论的 repogitment_oauth: client_id: 'xxx' #client ID client_secret: 'xxx' #client secret 配置成功后文章底部就会支持Gitment评论功能。 注意： Gitment目前只能登录git账号后才能评论 切勿滥用！可以看[这里](https://imsun.net/posts/gitment-introduction/) 结尾 `Github + Hexo搭建博客，暂且告一段落。当然，有深度定制Hexo需求甚至自建风格都可以参照官方文档进行开发，或者可以通过修改主题内的ejs文件和css文件。`","link":"/2018/10/02/1%20Github%20+%20Hexo%20%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/4%20Github%20+%20Hexo%20%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2(%E5%9B%9B)/"},{"title":"Github + Hexo 搭建博客(一)","text":"1 申请Github的page域名访问Github官网新建账号，并创建个人仓库。 在个人仓库操作框下进行设置settings，要满足三级域名要求必须进行如下操作。 Repository name 必须保证三级域名与账号一致，且以io顶级域名结束，否则访问路径会出现在路径分隔符后。例如：正常域名:alpacaca.github.io；错误域名www.github.com/alpacaca/xxxblog 如果正确创建，在GitHub Pages中正确显示已经发布的域名地址 可以在个人repo的master分支中创建readme.md用作说明或提示类信息（不要求）。 下载Github工具，安装带有Github Bash的命令提示符。 2 配置环境并下载HexoHexo下载安装依赖npm，npm是nodejs包管理器，nodejs安装依赖python环境 下载python3版本并安装，在环境变量path中添加python解释器的路径。添加项应是安装路径下的python3.exe 下载stable稳定版nodejs并安装，需要安装npm包管理器。使用以下命令查看npm 和node的版本是否正确。 12user &gt; npm -vuser &gt; node -v （由于万里长城的原因，npm默认中央仓库可能无法正常访问或者加载速度过慢）这时，需要通过以下命令查看metrics-registry参数 1user &gt; npm config list taobao提供国内npm仓库镜像https://registry.npm.taobao.org/，可以通过以下命令设置。 1user &gt; npm config set registry http://registry.npm.taobao.org/ 此时，nodejs和npm的准备工作已经完成。 通过npm安装hexo 12345#(此处为注释，下同)全局安装hexo组件user &gt; npm install hexo -g #安装完毕查看hexo版本user &gt; hexo -v 可以通过cmd进入硬盘任意位置 或者 在任意位置通过地址栏输入cmd打开命令提示符，初始化hexo组件、安装并初次运行 12345678910111213# 初始化hexo组件d:\\blog &gt; hexo init# 安装依赖组件d:\\blog &gt; npm install# 当全部完成后，生成静态文件d:\\blog &gt; hexo g# 本地浏览d:\\blog &gt; hexo s -o# hexo默认端口4000，如果此处端口被占用，可以使用下列命令，xxx为自定义端口d:\\blog &gt; hexo s -o -p xxx 至此，当在浏览器中出现下图，表示hexo已经在本地可以完美运行。接下来需要发布blog到Github，并在互联网上进行访问。 结尾`本地初始化博客已经跑起来了，如何部署到远端通过互联网域名方式访问，将在下节展开`","link":"/2018/09/19/1%20Github%20+%20Hexo%20%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/1%20Github%20+%20Hexo%20%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2(%E4%B8%80)/"},{"title":"Github + Hexo 搭建博客(三)","text":"5 Hexo全局配置初次浏览全局配置文件，在本地根目录下找到_config.yml文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: zhy's blogsubtitle: Stay Hungry , Stay Foolishdescription:keywords: zhy blogauthor: zhylanguage: zh-Hanstimezone:# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: http://yoursite.comroot: /permalink: :year/:month/:day/:title/permalink_defaults:# Directorysource_dir: sourcepublic_dir: publictag_dir: tagsarchive_dir: archivescategory_dir: categoriescode_dir: downloads/codei18n_dir: :langskip_render:# Writingnew_post_name: :title.md # File name of new postsdefault_layout: posttitlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0render_drafts: falsepost_asset_folder: truerelative_link: falsefuture: truehighlight: enable: true line_number: true auto_detect: false tab_replace: # Home page setting# path: Root path for your blogs index page. (default = '')# per_page: Posts displayed per page. (0 = disable pagination)# order_by: Posts order. (Order by date descending by default)index_generator: path: '' per_page: 10 order_by: -date # Category &amp; Tagdefault_category: uncategorizedcategory_map:tag_map:# Date / Time format## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DDtime_format: HH:mm:ss# Pagination## Set per_page to 0 to disable paginationper_page: 10pagination_dir: page# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: yilia# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: git@github.com:alpacaca/alpacaca.github.io.git branch: master 注意：yml文件相比于properties配置文件，更加简洁。类似于python代码，主从配属严格受到缩进的影响，特别特别需要强调的是，配置名后的冒号和参数之间是一定有空格的！ 接下来分别认识一下配置文件。 站点和url配置123456789101112131415# Sitetitle: zhy's blogsubtitle: Stay Hungry , Stay Foolishdescription:keywords: zhy blogauthor: zhylanguage: zh-Hanstimezone:# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: http://yoursite.comroot: /permalink: :year/:month/:day/:title/permalink_defaults title ， 是浏览器标签显示的名字 subtitle ， 子标题 keywords ， 主要用于SEO，可以采用列表表示[key1,key2],下同 author, 作者 language: 初次加载根据pc环境选择，可以手动修改 timezone： 时区，默认同系统 说明 如果你部署的博客地址并非三级以内的域名，而是地址分割符后的，比如`http://github.com/alpacaca`，那么需要配置url信息，如下 url： 默认不变，如果是地址分割符后地址需要配置http://yoursite.com/child root: /child/ permalink: 永久链接 目录123456789# Directorysource_dir: sourcepublic_dir: publictag_dir: tagsarchive_dir: archivescategory_dir: categoriescode_dir: downloads/codei18n_dir: :langskip_render: source_dir 资源文件夹，这个文件夹用来存放内容。 public_dir 公共文件夹，这个文件夹用于存放生成的站点文件。 tag_dir 标签文件夹 archive_dir 归档文件夹 category_dir 分类文件夹 code_dir Include code 文件夹 i18n_dir 国际化（i18n）文件夹 skip_render 跳过指定文件的渲染，可使用 glob 表达式来匹配路径。 写作123456789101112131415# Writingnew_post_name: :title.md # File name of new postsdefault_layout: posttitlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0render_drafts: falsepost_asset_folder: truerelative_link: falsefuture: truehighlight: enable: true line_number: true auto_detect: false tab_replace: new_post_name 新文章的文件名称 default_layout 预设布局 auto_spacing 在中文和英文之间加入空格 titlecase 把标题转换为 external_link 在新标签中打开链接 filename_case 把文件名称转换为 (1) 小写或 (2) 大写 render_drafts 显示草稿 post_asset_folder 启动 Asset 文件夹 relative_link 把链接改为与根目录的相对位址 future 显示未来的文章 highlight 代码块的设置 分页、主题和部署12345678910111213141516# Pagination## Set per_page to 0 to disable paginationper_page: 10pagination_dir: page# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: yilia# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: git@github.com:alpacaca/alpacaca.github.io.git branch: master per_page 每页显示博客数 pagination_dir 分页目录 theme 主题，将在下节介绍 deploy 部署 关于deploy当安装了hexo部署插件后，可以通过配置deploy自动进行部署，我选择的是以git方式发布到repo中的master分支 最终，使用hexo d -g发布到远程仓库 6 主题的选择和配置如果不喜欢Hexo的默认主题（没有人会喜欢[doge]），官方还提供了大量的主题类型，可以访问这里选择自己认为ok的主题 比如：中意首页展示的[Aria主题](https://sh.alynx.xyz/)，那么你可以在最下方footer标签范围找到该主题对应托管在github上的[资源](https://github.com/AlynxZhou/hikaru-theme-aria/)clone该项目到本地根目录下的themes文件夹下。 首先，需要在根目录下的/_config.yml中配置theme指定aria 1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: aria 之后进入主题内的置/themes/aria/_config.yml文件中进行自定义配置 关于主题这里就不展开了，因为不同的主题都有各自不同的配置要求，具体说明可以在打开的github资源首页READEME.md下找到 当配置完之后可以本地查看修改后的状态，运行 1d:\\blog &gt; hexo s -o 结尾 `经过定制化的个人博客成功运行，你可以动手写文章啦！什么？你还不知道怎么写？那么我们下一节介绍具体介绍。` 目前博客的书写均采用MarkDown进行编辑，如果你已经掌握可以跳过下节","link":"/2018/09/30/1%20Github%20+%20Hexo%20%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/3%20Github%20+%20Hexo%20%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2(%E4%B8%89)/"},{"title":"MySQL优化系列（一）","text":"MySQL优化系列（一） 0 总述按照Java当前框架式的开发模式，我总结单系统Mysql优化主要分为三类： 代码级优化。 数据库级优化。 框架级优化。 代码级优化主要包含： 尽量使用联接查询代替子查询（嵌套查询）。 避免使用关键字LIKE或正则表达式匹配，会导致全表扫描。 DELETE删除大量数据后，使用Optimize table tb_xxx释放残余空间。 避免在where中使用函数或表达式。 避免使用使索引失效的操作。 避免使用select *，会导致全表查询。 使用相对较高的性能分页，比如where id &gt; 100 limit 20 性能优于limit 100, 20。 Join时使驱动表为小数据量表。 数据库级优化主要包含： 建立索引，并使用explain调试接近最优查询。 合理使用存储过程。 选择适合业务特点的数据库引擎（事务、锁机制）。 掌握使用慢查询日志。 mysql buffer和cache。 物理资源使用分析（cpu使用率和io阻塞）。 框架级优化主要指多级缓存策略，其中包含： 关系型数据库中，ORM框架支持的一二级缓存。 非关系型数据库中，redis数据库缓存。 1 准备1.1 数据准备&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在展开研究Mysql数据库优化前，我们先创建一组数据，用于分析所有涉及的操作行为。四张数据表的数据字典如下：（主键id均为整型自增，非特别说明都使用varchar类型） tb_course : id，name（课程名），is_required（是否必修课 boolean），credit（学分 int） tb_teacher : id，name（教师名），age（年龄），course_id（所授课程id） tb_student : id，name（学生名），age（年龄 int） tb_student_course : id，student_id（学生id），course_id（课程id），is_pass（是否及格 boolean），score（得分 int） &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;创建数据库脚本代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788CREATE DATABASE `optdemo` CHARACTER SET 'utf8';USE `optdemo`;#创建课程表，包括id，名称，是否必修课，学分CREATE table `tb_course`( id int primary key auto_increment, name varchar(20), is_required boolean, credit int)ENGINE = InnoDB CHARACTER SET = utf8;#创建老师表，包括id，名称，年龄，教授课程CREATE table `tb_teacher`( id int primary key auto_increment, name varchar(20), age int, course_id int)ENGINE = InnoDB CHARACTER SET = utf8;#创建学生表，包括id，名称，年龄，所学课程CREATE table `tb_student`( id int primary key auto_increment, name varchar(20), age int)ENGINE = InnoDB CHARACTER SET = utf8;#创建学生--课程关系表，包括id，学生id，课程id，是否通过CREATE table `tb_student_course`( id int primary key auto_increment, student_id int not null, course_id int not null, is_pass boolean, score int)ENGINE = InnoDB CHARACTER SET = utf8;#初始化表INSERT INTO `tb_course` (name, is_required, credit) VALUES ('C', true, 4), ('JAVA', true, 4), ('数据结构', true, 3), ('操作系统', true, 4), ('C#', false, 2), ('Python', false, 3), ('人工智能', true, 4), ('TCP/IP', false, 2), ('计算机网络', true, 3),('Linux', false, 2);INSERT INTO `tb_teacher` (name, age, course_id)VALUES('老张', 45, 1),('老王', 46, 2),('老李', 48, 3),('老赵', 35, 4),('老吴', 55, 5),('老孙', 45, 6),('老刘', 40, 7),('老贾', 39, 8),('老钱', 32, 9),('老周', 36, 10);INSERT INTO `tb_student` (name, age)VALUES('小张', 18),('小王', 18),('小李', 18),('小赵', 18),('小吴', 18),('小孙', 18),('小刘', 18),('小贾', 18),('小钱', 18),('小周', 18);INSERT INTO `tb_student_course` (student_id, course_id, is_pass, score)VALUES(1,1,true, 80), (1,2,true, 90), (1,3,true, 92), (1,4,true, 93), (1,7,true,87), (1,9,true, 89), (1,10,true, 78), (2,1,true, 99), (2,2,true, 98), (2,3,true, 100), (2,4,true,100), (2,7,true,95), (2,8,true,91), (2,9,false,55),(3,1,true, 76), (3,2,true,68), (3,3,true,82), (3,5,true,77), (3,7,true,71), (3,9,false,59),(4,1,true,100), (4,2,true,100), (4,3,true,100), (4,4,true,100), (4,7,true,100), (4,9,true,100),(5,1,false, 42), (5,2,false,39), (5,3,false,33), (5,4,false,21), (5,5,true,60), (5,7,false,0), (5,9,false,58),(6,1,false,82), (6,2,true,78), (6,3,true,78), (6,4,false,49), (6,6,true,63), (6,7,false,47), (6,9,true,82),(7,1,true,98), (7,2,true,96), (7,3,true,92), (7,4,true,91), (7,5,true,96), (7,6,true,99), (7,7,true,93), (7,8,true,99), (7,9,true,94), (7,10,true,98),(8,1,false,32), (8,2,false,33), (8,3,false,42), (8,4,false,43), (8,7,false,52), (8,9,false,53),(9,1,false,36), (9,2,false,39), (9,3,false,56), (9,4,false,55), (9,7,false,55), (9,9,false,51),(10,1,true,86), (10,2,true,88), (10,3,true,92), (10,4,true,100), (10,5,true,100), (10,6,true,100), (10,7,false,100), (10,8,true,100), (10,9,true,100), (10,10,true,100); 1.2 Mysql逻辑分层结构&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先确定一个概念，通常开发中操作的是Mysql Client客户端，与Client相连并且处理数据和存储数据的称为Mysql Server或者Database Manager System（DBMS），Client职责就是输入并发送给Server处理，所以这里我们只讨论Server的逻辑分层结构。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从上图大体可以看出来，按逻辑共分为四层，概括一下包含：连接层、服务层、数据驱动层（引擎层）和数据层。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;① 连接层被设计为连接池形式，主要负责与Client的通信，同时提供授权和安全等策略； &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;② 服务层是Mysql Server的核心结构，主要包括：管理服务、SQL接口、SQL解析器、SQL优化器和缓存，具体介绍请看下一段。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;③ 数据驱动层（引擎层）是由Mysql提供的插件式数据引擎套件，按照业务实际需要选择合适的引擎，将会很大程度上提高算力。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;④ 数据层主要是物理层次的数据及相关信息的存储，例如慢查询日志等。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在服务层中，管理服务主要负责备份、恢复、数据迁移、集群等操作；SQL接口主要负责最熟悉的DML、DDL、存储过程、视图和触发器等处理；SQL解析器主要负责语法解析；SQL优化器主要负责重写系统判定不够优化的语句，也就是说，当我们通过Client提交一个SQL之后，优化器可能会对我们的SQL等效重写并继续向下执行，重写后的语句是Server认为足够优化的语句。SQL缓存将在之后章节具体介绍。 由于SQL优化器的存在，我们写的SQL并不一定是Server执行的SQL。 1.3 DQL执行顺序与SQL优化器 DQL : Data Query Language 数据查询语言，特指Select及相关的(group by etc.)操作统称。 DML : Data Manipulation Language 数据操作语言，包含INSERT、DELETE、UPDATE。 DDL : Data Defination Language 数据定义语言，包含CREATE、 DROP等。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在数据库DQL中，存在一条线性的关键字匹配和处理流程，这里所说的关键字就是与数据库操作有关的系统关键字。首先看下列关键字序列： 123456789101112131415SELECT DISTINCT &lt; select_list &gt;FROM &lt; left_table &gt; &lt; join_type &gt; JOIN &lt; right_table &gt; ON &lt; join_condition &gt;WHERE &lt; where_condition &gt;GROUP BY &lt; group_by_list &gt;HAVING &lt; having_condition &gt;ORDER BY &lt; order_by_condition &gt;LIMIT &lt; limit_number &gt; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这是一条日常开发中涉及较多关键字的SQL，也是SQL解析器认为合法的表示流程，但实际上，SQL解析器解析之后是按照如下顺序执行： 12345678910FROM &lt;left_table&gt;ON &lt;join_condition&gt;&lt;join_type&gt; JOIN &lt;right_table&gt;WHERE &lt;where_condition&gt;GROUP BY &lt;group_by_list&gt;HAVING &lt;having_condition&gt;SELECT DISTINCT &lt;select_list&gt;ORDER BY &lt;order_by_condition&gt;LIMIT &lt;limit_number&gt; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;掌握Mysql执行过程很重要，因为在后续优化过程中，经常需要确定驱动表以及多表查询时的执行顺序影响效率问题。 简化表示：FROM…ON…JOIN…WHERE…GROUP BY… HAVING…SELECT…DISTINCT…ORDER BY…LIMIT 简单来说，就是按顺序找到表，再从表中抽数据，再组织数据 2 代码级优化&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;代码级优化是指，针对手写或者ORM框架生成的原生SQL语句进行优化，优化目的是产生运行效率更高的执行语句。多数情况下，我们针对DQL进行优化。 2.1 使用联接代替子查询&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;子查询是指，使用SELECT查询出目标结果集，然后将该结果集当作其他查询的过滤条件使用。换句话说就是一个SELECT查询中的WHERE条件嵌套另一个或多个SELECT语句，这种写法不仅将多个逻辑联系起来更符合自然逻辑，而且避免了执行中的死锁和事务安全问题，但是，在执行过程中，Mysql会对子查询创建临时表，这就增加了IO消耗。 例：查询选修了Python课程的学生信息。 123456SELECT * FROM `tb_student`WHERE id IN ( SELECT student_id FROM `tb_student_course` WHERE course_id IN ( SELECT id FROM `tb_course` WHERE name = 'Python')); 声明：为了方便，使用SELECT *写法，实际业务处理中应使用具体字段SELECT id, name, age写法。 上述例子中，使用了三层SELECT语句组成的完整查询，逻辑清晰但实际运行效率并不高，下面改成JOIN写法。 12345SELECT t1.* FROM `tb_student` t1 JOIN `tb_student_course` t2 ON t1.id = t2.student_id JOIN `tb_course` t3ON t2.course_id = t3.idWHERE t3.name = 'Python'; 或者 123SELECT t1.* FROM `tb_student` t1, `tb_student_course` t2, `tb_course` t3 WHERE t1.id = t2.student_id AND t2.course_id = t3.id AND t3.name = 'Python'; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在排除SQL优化器和缓存条件（重复执行，效率会提高）时，从执行时间来看，后两者效率明显比前者效率高。 2.2 删除操作2.2.1 DELETE、DROP和TRUNCATE的区别 DELETE属于DML，执行操作时，每次从目标表中删除一行数据，并且删除行为作为日志保存以便进行”恢复“操作；而DROP和TRUNCATE属于DDL，其操作不能回滚。 DELETE时会执行相关的触发器，并且执行之后需要显示的commit才能完成删除动作[1]；而DROP和TRUNCATE会隐式commit，且不会执行触发器。 DROP删除表中所有数据，并释放表空间；TRUNCATE删除表中所有数据，重置高水位（high watermark）[2]；DELETE逐行删除，高水位保持不变。 [1] : 在Mysql中，默认DML开启了自动提交，所以执行DML之后无需commit，但并不代表它是隐式执行，可以通过SHOW VARIABLES LIKE 'autocommit'查看。 [2] : 举个例子，例如表中id主键自增且当前id=10，当DELETE 全表之后再新增，id为11；而TRUNCATE和DROP之后再新增，id=1。 2.2.2 DELETE大数据量优化&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DELETE删除大数据量后，不仅产生了许多日志空间，且所删除的空间并未被释放，此时需要使用OPTIMIZE TABLE [数据库]来释放。 123DELETE FROM `tb_student_course`;OPTIMIZE TABLE `tb_student_course`; 2.3 避免使用SELECT * 使用SELECT *会查询全表字段作为结果集，当我们只需要部分而不是全部字段作为结果集时，就会造成资源浪费，不仅降低了查询效率，还增加了网络IO使用率；当多人维护表时，如果表字段发生变化（增加或者删除）就会造成预期外的结果，或者需要额外的后台代码进行过滤，不利于维护。 安全性考虑，如果发生SQL注入风险，有可能被攻击者创建联表条件，从而更多信息被暴露。 SELECT 目标列正好是索引时，会更快的从内存（B+Tree）中读取数据并返回而不产生本地IO。 【本条转自】：https://blog.csdn.net/u013240038/article/details/90731874 连接查询时，* 无法进入缓冲池查询。 每次驱动表加载一条数据到内存中，然后被驱动表所有的数据都需要往内存中加载一遍进行比较。效率很低，所以mysql中可以指定一个缓冲池的大小，缓冲池大的话可以同时加载多条驱动表的数据进行比较，放的数据条数越多io操作就越少，性能也就越好。所以，如果此时使用select * 放一些无用的列，只会白白的占用缓冲空间。浪费本可以提高性能的机会。 2.4 分页优化&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据库层级的分页优化，实际上就是针对LIMIT关键字的优化，Mysql支持的LIMIT关键字系统中可以用作分页处理，表达式：LIMIT offset,rows 或 LIMIT rows。比如LIMIT 123456,50，从123456行开始查询50条数据；LIMIT 50将查询结果取前50条数据。LIMIT虽然使用方便但在大数据量时就会出现性能问题，随着offset增大，性能急剧下降，究其原因是因为LIMIT 123456,50会先扫描123456+50=123506条数据，然后返回50条，额外扫描的123456并不是我们需要的，事实上这样的设计显得多余，当数据量级增加之后，性能必然骤降。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;优化方式一：从实际情况来看，受影响的似乎只有LIMIT offset, rows形式，而LIMIT rows并不影响性能，实际上后者只扫描rows行数据，所以我们可以进行如下替换： 12345SELECT * FROM table LIMIT 123456,50;#LIMIT扫描123506条数据#转换后SELECT * FROM table WHERE id &gt;= 123456 LIMIT 50;#LIMIT只扫描50条数据 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;优化方式二：使用覆盖索引，在后续介绍索引时将深入介绍，在此简单提一下：覆盖索引是指，查询列正好全部具有索引，则结果直接从B+Tree中获取而不用回表操作。比如SELECT col1 FROM tb_test LIMIT 123456,50；，正好col1具有索引，那么执行该语句的时候会直接从col1索引对应的数据结构中获得结果，而不用回表到tb_test中再次查询，减少了磁盘IO。 12ALTER TABLE `tb_test` ADD INDEX col1_index(col1);SELECT col1 FROM tb_test LIMIT 123456, 50; 注：创建表并添加主键之后，会自动创建主键索引 Primary Key Index。 2.5 JOIN时使驱动表为小数据量表&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在前文中我们已经提到使用JOIN代替子查询可以明显提高效率，但JOIN本身也有需要更进一步的优化。首先介绍一下JOIN的原理：将驱动表的查询结果作为“输入”，当作条件逐条的在被驱动表内进行过滤，最终返回过滤后的数据，这种JOIN的方式被称作NEST LOOP。 1234567SELECT t1.* FROM tabel1 t1 LEFT JOIN table2 t2 ON t1.condition = t2.conditionJOIN table3 t3ON t1.condition = t3.condition AND t2.condition = t3.conditionWHERE ....ORDER BY t2.id DESC; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这是一个相对复杂的例子，其中用到了LEFT JOIN和JOIN，解析一下过程：将table1当作驱动表查询并返回结果result1，将result1当作被驱动表t2的“输入”，并将result1中的数据逐条取出在table2中匹配，并将最终的结果返回result2，将table1和table2联合查询后的结果result2当作table3的”输入“，并逐条匹配并返回最终结果result3，排序后当作最终的结果返回。所以在使用多表JOIN时，明确表的数据条的多少，有助于我们确定使用小表作为驱动表，从而减少循环次数，达到优化的目的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;庆幸的是，在不显式地指定驱动表时，SQL优化器帮助我们将最小的表作为驱动表来执行，所谓显式指定是指LEFT JOIN时左侧表为驱动表，RIGHT JOIN时右侧表为驱动表，而JOIN属于隐式，SQL优化器会自行决定小表为驱动表。 思考：上述SQL还存在严重影响性能的地方，可以先做思考，将在后续索引章节介绍。 2.6 避免使用LIKE和正则匹配 题：找出年龄是5或5的倍数的所有老师信息。 使用LIKE关键字查找如下： 12SELECT * FROM tb_teacherWHERE age LIKE \"%5\" OR age LIKE \"%0\"; 使用正则表达式如下： 12SELECT * FROM tb_teacherWHERE age REGEXP \".5|0\"; 两者效果等价，结果如下： id name age course_id 1 老张 55 1 4 老赵 45 4 5 老吴 35 5 6 老孙 45 6 7 老刘 40 7 可以看到LIKE和正则在进行内容过滤检索时很灵活，也很方便，但是 这两者查询都是基于全表的查询，大数据量时查询效率很低，所以给出一下两种解决办法。 当使用MyISAM引擎时，建议使用全文索引FULLTEXT，因为只有MyISAM引擎是支撑该特殊索引方式的，所以可以在创建表的时候使用FULLTEXT(字段)来定义，如下： 1234567CREATE table `tb_student`( id int primary key auto_increment, name varchar(20), age int, description text, FULLTEXT(description))ENGINE = Myisam CHARACTER SET = utf8; Mysql会创建索引表维护该索引列，查询时使用如下方式： 1SELECT * FROM tb_student WHERE MATCH(description) AGAINST('关键字'); 当业务中不得不实现LIKE模糊匹配时，最好在匹配字符中不以占位符开始LIKE '关键字%'，这样可以使查询字段的索引生效，相反则会索引失败进行全表查询；如果不得不以占位符开始，那么可以使用覆盖索引来提高效率。","link":"/2020/02/12/2%20MySQL/MySQL%E4%BC%98%E5%8C%96%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%80%EF%BC%89/"},{"title":"MySQL优化系列（二）","text":"MySQL优化系列（二） 3 数据库级优化3.1 索引数据结构&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;索引是DQL优化的核心，可以显著提高查询的效率，但值得注意的是凡事有利则有弊，数据检索性能的提高是依赖数据库内部维护的检索表，它是一个B+Tree的数据结构，本身就会占用空间且当对表做出DML操作时，需要同步维护该索引，所以DML的效率则会下降，效果可以类比线性表存储结构。建议对频繁进行数据查询的表创建索引，并选择合适的索引类型，同时以满足业务需求为目的创建，否则会适得其反。 3.1.1 B-Tree&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B-Tree是数据库中使用最为常见的数据结构，由平衡二叉查找树演化而来，现在多使用的B+Tree结构由B-Tree演化而来，所以有必要先介绍B-Tree。 B-Tree中的B是Balance而非Binary；”B-Tree”和”B+Tree”会被有些人称为”B减树“和”B加树“，这样称呼看似合人情但不合理，因为B-Tree创建之时不可能已经意识到B+Tree的存在，也就不存在名称对立，所以个人认为，“-”只是一个连接符，没有任何含义，应该称为“B树”和“B加树”。 B-Tree并不是二叉树，而是多叉树。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先需要理解，为什么需要B-Tree，这就需要从非关系型数据库的存储来讲，我们在开发过程中通常将MVC分层中数据库DML操作称为“持久化”过程，其实是因为数据库中的数据都是保存在磁盘中的。系统从磁盘读取数据并保存到内存是以“磁盘块（block）”为基础单位，位于同一磁盘块的数据会被一次性加载进来，而不是按需加载。那么，B-Tree的意义就是如何以最优的方式找到数据所在的磁盘块加载需要的数据，减少io损耗。 众所周知，磁盘IO是系统的瓶颈之一，且无法大幅优化提升效率，那么减少IO次数就很关键。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;定义一组普通映射[key, data]，key为索引的键（数据不同则键值不同），data为索引对应的一行表数据。B-Tree数据结构定义如下： 所有叶子节点都在同一层，且没有指向其他节点的指针。 每个非末端节点包含n个关键字或引用，保存每个节点保存更多的信息。 满足平衡二叉树的定义，即节点的索引值大于左子树索引值，小于右子树索引值，但可以有多个子节点。 索引值必包含对应的映射数据信息。 题：以查找索引值51为例，经过的步骤如下： 根节点磁盘块读取并存入内存中，48 &lt; 51，所以指向指针P2的子节点。（1次磁盘IO，1次内存加载） P3指向的磁盘块读取并存入内存，找到目标51并返回对应的行数据。（1次磁盘IO，1次内存加载） 题：以查找索引值15为例，经过的步骤如下： 根节点磁盘块读取并存入内存中，13 &lt; 15 &lt; 48，所以指向指针P2的子节点。（1次磁盘IO，1次内存加载） P2指向的磁盘块读取并存入内存，15 &lt; 20，所以指向指针P1的子节点。（1次磁盘IO，1次内存加载） P1指向的磁盘块读取并存入内存，找到目标15返回对应行数据。（1次磁盘IO，1次内存加载） 经过上述步骤查找，最终经过3次磁盘IO和3次内存加载，定位到索引15并读取的对应的行信息返回。所以在当前索引中最优结果是1次磁盘IO和1次内存加载，即根节点位置定位。最差结果是3次磁盘IO和3次内存加载，相比于平衡二叉树或红黑树等数据结构，由于节点保存的信息较多，所以树的高度偏扁平，这样减少了磁盘IO，从而提高了查找效率。 3.1.2 B+Tree&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B-Tree虽然已经很适合作为非关系型数据库的索引模型，但它还存在一些弊端：索引根节点是固定的，没必要每次查找都经过1次磁盘IO读取和加载；由于磁盘块容量有限，而节点保存数据会占用节点很大的空间，所以一个磁盘块能够保存的索引个数会减少，导致树高度可能增加；范围查找变得效率不高。所以在此基础之上，B+Tree做到了如下优化: 根节点常驻内存，减少1次IO损耗； 所有非叶子节点只保存索引值或引用，不保存映射数据，扩增节点保存关键信息的能力； 有两个全局指针，一个指向根节点，另一个指向索引值最小的节点（最左叶子节点）。 所有叶子节点保存索引值和映射数据，且叶子节点由单链表关联，指针指向相邻的右兄弟节点，结合第3条信息，这就提供了查找的多样性，可以选择从根节点开始检索树，也可以从顺序链表头开始进行查询，因为有链表的存在，支持范围查询且效率比B-Tree更好。 题：以查找索引值41为例，经过的步骤如下（与B-Tree一致）： 从内存加载根节点，找到 13 &lt; 41 &lt; 48，指向指针P2的节点。 读取磁盘块，找到 20 &lt; 41 &lt; 42，指向指针P2的几点。 读取磁盘块，找到目标并返回对应数据。 题：以查找索引值13为例，过程如下： 内存加载根节点，找到目标13（此时从右指针查找）。 加载磁盘块，13 &lt; 20，指向P1指针节点。 加载磁盘块，找到目标并返回。 注：为什么非叶子节点定位到目标值要从右侧指针查找，查找了大量网络资源都没有找到合适的答案，最终从维基百科和《高性能Mysql》一书中推测而来。欢迎各位交换意见。 3.1.3 扩展：B*Tree&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B*Tree是B+Tree的优化，进一步要求，在非根非叶子节点各层中的节点之间互相产生指向相邻右兄弟 节点的指针。 3.1.4 聚簇索引、非聚簇索引和辅助索引&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B+Tree在数据库中的实现方式即为聚簇索引，其含义是指在一张索引表中，既存在索引值同时存在映射数据；在Innodb引擎中使用聚簇索引，在MyISAM引擎中就使用非聚簇索引，其含义是指，在B+Tree结构中叶子节点索引值对应的并不是数据而是数据在数据页中的地址，通过地址就可以直接在内存中获得数据，数据页是管理磁盘块的最小单位；辅助索引是指，在索引表Index1中查找关键字key1时，需要先从索引表Index2中查找关键字key2对应的关键字key1。 3.2 索引的创建与修改&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在我们已经创建的四张表中，都设置了整型自增主键id，Mysql已经自动帮我们添加了主键索引（MySQL 5.7+版本），也就是说当我们使用id查询时是通过主键索引进行查询。索引按照创建方式可以分为：一般索引、唯一索引、复合索引、主键索引和全文索引。其中主键索引属于唯一索引范畴，全文索引已经在2.6章节中进行了介绍，是MyISAM引擎支持的文本类索引，在此不再深入讨论。 3.2.1 一般索引 创建tb_course表时对课程名name添加索引 12345678#创建课程表，包括id，名称，是否必修课，学分CREATE table `tb_course`( id int primary key auto_increment, name varchar(20), is_required boolean, credit int, INDEX course_name_index(name))ENGINE = InnoDB CHARACTER SET = utf8; 执行命令SHOW INDEX FROM tb_course \\G查看当前表的所有索引： 1234567891011121314151617181920212223242526272829mysql&gt; show index from tb_course \\G*************************** 1. row *************************** Table: tb_course Non_unique: 0 Key_name: PRIMARY Seq_in_index: 1 Column_name: id Collation: A Cardinality: 0 Sub_part: NULL Packed: NULL Null: Index_type: BTREE Comment:Index_comment:*************************** 2. row *************************** Table: tb_course Non_unique: 1 Key_name: course_name_index Seq_in_index: 1 Column_name: name Collation: A Cardinality: 0 Sub_part: NULL Packed: NULL Null: YES Index_type: BTREE Comment:Index_comment: 可以看到row1是系统自动添加的主键索引，row2是我们创建的自定义索引。 通过 CREATE 和 ALTER关键字创建索引 123CREATE INDEX course_name_index ON `tb_course`(name);ALTER TABLE `tb_course` ADD INDEX course_name_index(name); 删除索引 1DROP INDEX course_name_index ON `tb_course`; 3.2.2 唯一索引&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;唯一索引与一般索引的不同之处在于，它所持有的字段必须是唯一的，如果是唯一复合索引，那么复合字段必须是唯一的，其他用法一直，只需要将INDEX替换为UNIQE INDEX。 创建tb_course表时对课程名name添加唯一索引 12345678#创建课程表，包括id，名称，是否必修课，学分CREATE table `tb_course`( id int primary key auto_increment, name varchar(20), is_required boolean, credit int, UNIQUE INDEX course_name_index(name))ENGINE = InnoDB CHARACTER SET = utf8; 通过 CREATE 和 ALTER关键字创建唯一索引 123CREATE UNIQUE INDEX course_name_index ON `tb_course`(name);ALTER TABLE `tb_course` ADD UNIQUE INDEX course_name_index(name); 3.2.3 复合索引&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;复合索引语法与一般索引一致，只是持有多个字段。 创建tb_course表时对课程名、是否必修和学分添加复合索引 12345678#创建课程表，包括id，名称，是否必修课，学分CREATE table `tb_course`( id int primary key auto_increment, name varchar(20), is_required boolean, credit int, INDEX union_index(name, is_required, credit))ENGINE = InnoDB CHARACTER SET = utf8; 通过 CREATE 和 ALTER关键字创建复合索引 123CREATE INDEX union_index ON `tb_course`(name, is_required, credit);ALTER TABLE `tb_course` ADD INDEX union_index(name, is_required, credit); 通过命令SHOW INDEX FROM tb_course \\G查看结果 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152mysql&gt; show index from tb_course \\G*************************** 1. row *************************** Table: tb_course Non_unique: 0 Key_name: PRIMARY Seq_in_index: 1 Column_name: id Collation: A Cardinality: 0 Sub_part: NULL Packed: NULL Null: Index_type: BTREE Comment:Index_comment:*************************** 2. row *************************** Table: tb_course Non_unique: 1 Key_name: union_index Seq_in_index: 1 Column_name: name Collation: A Cardinality: 0 Sub_part: NULL Packed: NULL Null: YES Index_type: BTREE*************************** 3. row *************************** Table: tb_course Non_unique: 1 Key_name: union_index Seq_in_index: 2 Column_name: is_required Collation: A Cardinality: 0 Sub_part: NULL Packed: NULL Null: YES Index_type: BTREE*************************** 4. row *************************** Table: tb_course Non_unique: 1 Key_name: union_index Seq_in_index: 3 Column_name: credit Collation: A Cardinality: 0 Sub_part: NULL Packed: NULL Null: YES Index_type: BTREE 可以看到row 2.3.4信息中，索引名称相同，都是union_index，并且索引序列Seq_in_index分别是1,2,3，也就是说我们复合索引查找是有序列限制的（最左原则，后续章节介绍）。","link":"/2020/02/22/2%20MySQL/MySQL%E4%BC%98%E5%8C%96%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%8C%EF%BC%89/"},{"title":"MySQL优化系列（五）（缓存篇）","text":"MySQL优化系列（五）（缓存篇） 4 缓存&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;说明：由于是MySQL优化系列，本篇缓存旨在引导向，并不深入介绍，后续将开篇深入介绍，同样的，作为Java端持久化框架Mybatis、Hibernate和 JPA 也适用此说明，由于JPA基于Hibernate轻量化封装，所以框架缓存策略选择JPA介绍即可。 4.1 为什么使用缓存&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;缓存，是以提升数据响应为目的，以合理的缓存策略为条件，以数据中间件的形式为手段的一种技术。在计算机纵向世界，软硬件都有各自的缓存，并且缓存都是以提升现有框架约束的性能为价值，且成为衡量系统性能的重要指标之一。再此我们仅讨论软件领域以内存为驱动模型的缓存。 缓存的用途，使用缓存可以减少某条频繁进行数据处理链路上的性能损耗，通常是读多写少的场景，缓存是一种空间换时间的解决方案，通常在内存中进行。 缓存的位置，众所周知，在计算机科学发展历史长河中，困难的问题往往都是通过增加第三方中间件来解决的，而缓存的位置随着业务覆盖的不同而不同。 比如，在web前端页面，经常会缓存页面渲染或数据处理常用的信息，如多次浏览同一商品的信息，这一功能现代浏览器基本都支持。 比如，还是web前端，用户通过点击页面按钮多次进行查询，通过http访问后端控制器接口就可以添加相应的缓存，而无需经过服务层乃至持久层查询。 再比如，数据频繁的读场景，通常是在业务层和持久层中添加缓存，以减少数据库访问带来的IO性能损耗和数据连接开销。 缓存的分类，应用开发层面的数据结构如Map，持久层数据中间件如EhCache、Memcache、Redis等，数据库自带的缓存特性如Mysql和Oracle等。 缓存的策略，由于不同系统的数据访问模式不同，同一种缓存策略很难在不同的数据访问模式下取得满意的性能，通常使用如下几种策略： 基于访问的时间：按各缓存项被访问时间来组织缓存队列，决定替换对象。如 FIFO，LRU； 基于访问频率：用缓存项的被访问频率来组织缓存。如 LFU、LRU2； 访问时间与频率兼顾：兼顾访问时间和频率，使数据在变化时缓存策略仍有较好性能。多数此类算法具有一个可调或自适应参数，通过该参数的调节使缓存策略在基于访问时间与频率间取得一个平衡，如 FBR； 基于访问模式：某些应用有较明确的数据访问特点，进而产生与其相适应的缓存策略。 FIFO，First In Last Out，即先进先出，如果一个数据是最先进入的，那么可以认为在将来它被访问的可能性很小，当缓存空间不足，它最先被释放。 LRU：Least Recently Use，即最近最久未使用算法，选择最近最久未使用的数据予以淘汰，在缓存空间内，最近一直没有被使用的数据会被释放，由新访问数据代替 LFU：Least Frequently Used，即最近最少使用算法，如果一个数据在最近一段时间很少（频率）被访问到，那么可以认为在将来它被访问的可能性也很小。因此，当空间满时，最小频率访问的数据最先被释放。 LRU2：Least Recently Used 2，即LRU的改进版本，每当一次缓存记录的使用，会把它放到栈的顶端。当栈满了的时候，再把栈底的对象给换成新进来的对象。 FBR：Frequency-based replacement， 需要可调参数平衡访问时间和频率。 数据一致性，当分布式缓存中读写并发执行，有可能导致同一数据先读后写，那么缓存中保存的将是旧数据；先操作数据库，再清除缓存，如果缓存删除失败，就会出现数据不一致问题，解决方案如下： 前者可以采取将读写纳入同一节点的缓存按照同步处理，或者采取数据写入前后一并删除相关缓存。 缓存删除失败，可以将删除失败的key存入队列中并重复删除直到成功为止。 缓存的指标，命中率=命中次数/访问次数，命中率是缓存最重要的指标，它直接决定了缓存设计的合理性，若查询一个缓存，十次查询九次能得到正确结果，那么命中率就是90%。而直接影响缓存命中率的因素包含：缓存容量、内存空间和缓存策略。 4.2 MySQL缓存&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从前面介绍已经可以了解到，Mysql在执行时需要经过四层逻辑分层（客户端接口 -&gt; 服务 -&gt; 引擎 -&gt; 持久层），并在服务层和插件引擎层还需要经过解析、优化、执行过程，并最终在持久层完成IO操作，这一些列耗时耗力的操作在QPS峰值时是噩梦般的存在，所以Mysql自身也支持缓存策略，在MyIsam中使用缓存策略，在InnoDB中使用缓冲池策略。 12345678910111213141516171819202122# 查询是否支持查询缓存mysql&gt; show variables like 'have_query_cache';+------------------+-------+| Variable_name | Value |+------------------+-------+| have_query_cache | YES |+------------------+-------+# 查询当前缓存状态mysql&gt; show status like '%qcache%';+-------------------------+-------+| Variable_name | Value |+-------------------------+-------+| Qcache_free_blocks | 0 | 缓存空闲的内存块| Qcache_free_memory | 0 | 在query_cache_size设置的缓存中的空闲的内存| Qcache_hits | 0 | 缓存的命中次数| Qcache_inserts | 0 | 查询缓存区此前总共缓存过多少条查询命令的结果| Qcache_lowmem_prunes | 0 | 查询缓存区已满而从其中溢出和删除的查询结果的个数| Qcache_not_cached | 0 | | Qcache_queries_in_cache | 0 | 缓存查询次数| Qcache_total_blocks | 0 | 缓存总的内存块+-------------------------+-------+ 通过修改mysql配置文件来配置缓存 1234[mysqld]query_cache_type=1 #0不使用，1使用，2适时使用query_cache_size=10485760 #10M，单位字节query_cache_limit=1048576 #1M, 单个查询允许使用的最大缓存 4.3 Memcache和Redis&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;缓存界两位翘楚，都是基于内存的存储机制，亦可称为内存数据库。两者都是高性能的分布式缓存，在缓存层面都可以很好的满足业务要求。 4.3.1 Memcache&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Memcache是一套分布式的高速缓存系统，对于大型的需要频繁访问数据库的网站访问速度提升效果十分显著，以提升网站的访问速度。通过在内存里维护一个统一的巨大的hash表，它能够用来存储各种格式的数据，包括图像、视频、文件以及数据库检索的结果等。简单来说就是将数据调用到内存中，然后从内存中读取，从而提高读取速度。Memcached是以守护线程方式运行于一个或多个服务器中，随时接收客户端的连接和操作。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于完全依赖内存，使Memcache的容量根据部署节点的内存而定，在32位系统中容量最大限定2G。由于在内存中维护一个Hash表结构来缓存数据，所以它的数据保存形式遵从K-V简单结构，key默认最大不能超过128个字 节，value默认大小是1M，不过可以针对每条数据设定过期策略。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Memcache操作也相对简单，针对Hash表常用的操作如：set设置、get读取、replace替换、delete删除和flush刷新等。目前只支持文本类型的存取，所以在面向对象中可以将需要存储的对象经过序列化处理并保存，读取之后可以通过反序列化还原。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Memcache采用LRU过期策略，不支持持久化，所以当内存断电时会发生数据丢失，且无备份功能。 123456789101112131415161718192021222324252627282930/* Java 调用Memcache */public class MemcachedJava { public static void main(String[] args) { try{ // 连接本地的 Memcached 服务 MemcachedClient mcc = new MemcachedClient(new InetSocketAddress(\"127.0.0.1\", 11211)); // 存储数据 Future future = mcc.set(\"key1\", 900, \"This is Value1\"); // 查看存储状态 System.out.println(\"set status:\" + future.get()); // 缓存读取 System.out.println(\"value in cache : \" + mcc.get(\"key1\")); // 新增数据 mcc.add(\"key2\", 900, \"This is Value2\"); // 数据替换 mcc.replace(\"key2\", 900, \"is\"); // 数据后向追加 mcc.append(\"key2\", 900, \" Value2\"); // 数据前向追缴 mcc.prepend(\"key2\", 900, \"This \"); // 删除数据 mcc.delete(\"key1\"); // 关闭连接 mcc.shutdown(); }catch(Exception ex){ System.out.println( ex.getMessage() ); } }} 4.3.2 Redis&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;相比于Memcache，Redis具有更多的功能扩展性，可以满足更复杂的场景，并且支持String、Hash、Set、List和sorted set类型，还支持数据的持久化、虚拟内存等。它可以用作数据库、缓存和消息中间件。redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Redis支持主从同步，数据可以从主服务器向任意数量的从服务器上同步，从服务器可以是关联其他从服务器的主服务器。由于完全实现了发布/订阅机制，使得从数据库在任何地方同步树时，可订阅一个频道并接收主服务器完整的消息发布记录。同步对读取操作的可扩展性和数据冗余很有帮助 发布/订阅机制：是一种消息通信模式,发布者(pub)发送消息到特定的频道( channel)，订阅者(sub)通过观察频道( channe)接收消息。在分布式环境中，Redis分为客户端和服务端，消息的发布和订阅都是客户端行为，服务端提供channel，如果没有订阅消息，客户端会进入订阅监听状态，一旦接收到订阅消息就会进行消息同步。 类似于，有一档天气预报的节目，节目主持人（channel）实时获得气象局（pub）发布的气象信息，并把消息告诉正在观看节目的观众（订阅者），观众会一直盯着节目看直到收到气象预报并更新大脑关于今天的气象信息。 Redis的虚拟内存技术VM提升了数据保存的界限，其实际原理是当数据量已经达到存储边界，会对数据进行冷热隔离，将热数据继续保存在内存中，而冷数据通过压缩手段保存到磁盘上，压缩后的数据仅为原数据的1/10，虽然冷数据的查询效率不及热数据，但考虑到本身查询频率低，并不会影响整体性能。 12345678910111213141516171819202122232425262728293031323334353637public class RedisJava { public static void main(String[] args) { //连接本地的 Redis 服务 Jedis jedis = new Jedis(\"localhost\"); //查看服务是否运行 System.out.println(\"服务正在运行: \"+jedis.ping()); // redis 字符串 jedis.set(\"key1\", \"This is key1\"); System.out.println(jedis.get(\"key1\")); // redis list jedis.lpush(\"list\", \"val1\"); jedis.lpush(\"list\", \"val2\"); jedis.lpush(\"list\", \"val3\"); jedis.llen(\"list\"); // 长度 List&lt;String&gt; list = jedis.lrange(\"list\", 0 ,2); // 排序 SortingParams sortingParams = new SortingParams(); sortingParams.alpha(); sortingParams.limit(0, 3); jedis.sort(list, sortingParams) // keys Set&lt;String&gt; keys = jedis.keys(\"*\"); Iterator&lt;String&gt; it=keys.iterator() ; // 删除 if (jedis.exists(\"key1\")) jedis.del(\"key1\"); // 过期策略 jedis.persist(\"key1\"); jedis.ttl(\"key1\"); }} 4.3.3 Memcache和Redis的区别以及性能比较&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;实际上，两者既存在竞争又存在互补，由于实现细节的不同触手会伸向对方触及不到的业务场景。 内存空间：MemCached可以修改最大内存，但终究首先于内存。Redis增加了VM的特性，突破了物理内存的限制，实现冷热分离。 操作：MemCached数据结构单一，仅用来缓存数据，面向对象保存需要用到序列化和反序列化手段；Redis支持更加丰富的数据类型，也可以在服务器端直接对数据进行丰富的操作,这样可以减少网络IO次数和数据体积。 可靠性：MemCache不支持数据持久化，断电或重启后数据消失。Redis支持数据持久化和数据恢复，允许单点故障，但是同时也会付出性能的代价。 应用场景：Memcache动态系统中减轻数据库负载，提升性能，适合多读少写标准缓存场景。 Redis适用于对读写效率要求都很高，数据处理业务复杂和对安全性要求较高的系统。 性能：性能上都很出色，具体到细节，由于Redis只使用单核，而Memcached可以使用多核，所以平均每一个核上Redis在存储小数据时比 Memcached性能更高。而在100k以上的数据中，Memcached性能要高于Redis。 4.4 JPA缓存策略&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;JPA是Java持久层接口，是官方发布的ORM标准，Hibernate是对JPA的全量实现框架支持全自动处理，移植性更好；Mybatis仅部分遵循JPA规则实现半自动处理，灵活性更高。我们再此不展开讨论两者孰优孰劣，仅从缓存层面来看框架级对持久层缓存的支持情况。 4.4.1 Spring-JPA-Data(Hibernate)缓存策略&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Spring-JPA-Data是Spring框架对实现JPA接口的ORM框架的轻量级封装，默认使用Hibernate。Hibernate缓存包括两大类：一级缓存和二级缓存。一级缓存又被称为“Session的缓存”。Session缓存是内置的，不能被卸载，是事务范围的缓存，session级别缓存数据不共享；二级缓存又称为“SessionFactory的缓存”，由于SessionFactory对象的生命周期和应用程序的整个过程对应，因此Hibernate二级缓存是进程范围或者集群范围的缓存，是可以被不同Session所共享的，默认使用EhCache也可显示的替换为Memcache或其他缓存产品。 一级缓存默认开启，仅存在与session周期内，缓存时间短，范围小，效果不明显。 二级缓存默认不开启，需手动配置开启，是热拔插设计，不影响整体性能，可以显著提高效率，同样的将占用更多的内存空间。 4.4.2 Mybatis缓存策略&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mybatis同样采用一二级缓存策略，情况类似与Hibernate的使用。一级缓存称为SqlSession级缓存，与Hibernate的Session缓存一致，二级缓存属于SqlSessionFactory级别，类似于SessionFactory，不同的是，Mybatis二级缓存支持在单一对象映射中使用，比如针对特定的Mapper进行缓存，这就对选择业务场景使用更有帮助。 4.5 缓存击穿、穿透、雪崩、污染 缓存穿透：缓存和数据库中都没有真实数据，此时大量访问该无效数据造成系统压力增大，如果攻击者使用缓存穿透持续攻击，将造成持久层崩溃。 解决方案 对无效数据访问后可在缓存创建值为null的返回，并设定较短的过期时间，可以有效避免攻击。 业务层添加有效性校验，拦截较容易识别的风险。 缓存击穿：当缓存数据过期，此时QPS达到峰值并对该数据进行大规模并发访问，造成数据库瞬间访问量骤增导致奔溃。击穿的特点是少量数据过期，之后对这些数据高并发访问。 解决方案 甄别热点数据，采用合适的过期策略如LRU或LFU。 在高QPS来临前，设置Redis冷热数据隔离，假如冷数据发生高并发访问，也可以保证从缓存冷数据读取。 业务层创建互斥锁，当缓存数据不存在时，可以保证同一类请求只有一个可以访问数据库，其他请求阻塞，访问成功后刷新缓存，使其他请求通过缓存访问。 缓存雪崩：缓存数据大量过期，此时业务查询增大，同样导致数据库压力骤增容易发生奔溃。雪崩特点是大量数据过期，之后被大量访问，区别于缓存击穿。 解决方案 设置合理的缓存过期策略和过期时间，可以自定义一个过期时间范围，并将缓存单数据过期时间以哈希方式分散到不同时间范围。 可以设置热点数据永久不过期。 缓存污染：系统将不常用的数据从内存移到缓存，造成常用数据的失效，降低了缓存的利用率。缓存容量是弥足珍贵的，容量过大反而容易影响查询效率，所以在有效的空间内保证热点数据很重要。 解决策略 设置合理的过期策略，FIFO、LRU、LFU等。 业务层识别，避免大而全的数据添加进缓存中。","link":"/2020/03/10/2%20MySQL/MySQL%E4%BC%98%E5%8C%96%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%94%EF%BC%89/"},{"title":"MySQL优化系列（三）","text":"MySQL优化系列（三） 3.2 EXPLAIN 执行计划&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;执行计划的查看是进行SQL调优的重要步骤，也是收集可调优选项的信息集中地，Mysql中通过关键EXPLAIN来查看SELECT的查询效率。我们已经知道在逻辑分层中，服务层存在SQL优化器，它可以对我们的SQL进行优化并最终在引擎中执行，EXLAIN可以模拟SQL优化器执行结果。 3.2.1 使用和介绍1EXPLAIN SELECT * FROM `tb_course`\\G 该语句将模拟优化器执行，并将执行信息打印在控制台，如下： 1234567891011121314mysql&gt; EXPLAIN SELECT * FROM tb_course \\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_course partitions: NULL type: indexpossible_keys: NULL key: union_index key_len: 70 ref: NULL rows: 14 filtered: 100.00 Extra: Using index [id]： SELECT执行语句的编号，是一组整型数值。 ① 当Explain某一复杂语句时，可能包含多条子查询，那么查询顺序按照编号由大到小执行； ② 当查询编号一致时，按照从上到下的顺序执行。 例1： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455 mysql&gt; EXPLAIN SELECT * FROM tb_student WHERE id IN (SELECT student_id FROM tb_student_course WHERE course_id=(SELECT id FROM tb_course WHERE name='JAVA') ) \\G *************************** 1. row *************************** id: 1 select_type: PRIMARY table: &lt;subquery2&gt; partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: NULL filtered: 100.00 Extra: NULL*************************** 2. row *************************** id: 1 select_type: PRIMARY table: tb_student partitions: NULL type: eq_refpossible_keys: PRIMARY key: PRIMARY key_len: 4 ref: &lt;subquery2&gt;.student_id rows: 1 filtered: 100.00 Extra: NULL*************************** 3. row *************************** id: 2 select_type: MATERIALIZED table: tb_student_course partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 1 filtered: 100.00 Extra: Using where*************************** 4. row *************************** id: 3 select_type: SUBQUERY table: tb_course partitions: NULL type: refpossible_keys: union_index key: union_index key_len: 63 ref: const rows: 5 filtered: 100.00 Extra: Using index4 rows in set, 1 warning (0.04 sec) 该用例中查询序列按照3 -&gt; 2 -&gt; 1 -&gt; 1执行，首先查询SELECT id FROM tb_course WHERE name='JAVA'接着将子查询结果作为条件执行SELECT student_id FROM tb_student_course WHERE course_id=subquery，SQL优化器将最后主查询SELECT * FROM tb_student WHERE id IN (subquery) 与上一步子查询进行联接，先查询student_id，再查询最终结果。 例2： 1234567891011121314151617181920212223242526272829#查询选修了Java的学生课程关联表mysql&gt; EXPLAIN SELECT * FROM tb_student_course WHERE course_id IN (SELECT id FROM tb_course WHERE name LIKE 'JAVA%') \\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_student_course partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 1 filtered: 100.00 Extra: NULL*************************** 2. row *************************** id: 1 select_type: SIMPLE table: tb_course partitions: NULL type: eq_refpossible_keys: PRIMARY,union_index key: PRIMARY key_len: 4 ref: optdemo.tb_student_course.course_id rows: 1 filtered: 35.71 Extra: Using where 该例子中应当属于子查询嵌套类型且id顺序不一致，只是SQL优化器进行了优化，将嵌套查询优化为联接，所以编号一致，执行顺序从上到下。 [select_type]：表示查询类型：Simple简单查询，Primary外侧主查询，Subquery内测子查询，Derived驱动查询表示当前语句位于FROM后，Materialized被物化的子查询。 [table]：当前查询所在的目标表。 [partitions]：查询目标是分区表的位置，如果查询目标在其他分区的表中将显示出来。在早先Mysql版本中，需要使用Exlain Extends才会显示该选项。 [type]：访问类型，用于判断当前查询优化类别，是单表优化的重要指标，type效率指标由优到差依次为： system &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; all *system : * 查询结果只有一条数据，且扫描表中数据也只有一条，WHERE后使用主键索引查询。 *const : * 查询结果只有一条数据，且扫面表中有多条数据，WHERE后使用主键索引查询，system是const的特殊情况。 *eq_ref : * 查询结果为多条数据，使用唯一索引或主键索引查询。 *ref : * 查询结果为多条数据，使用普通索引或复合索引查询。 *range : * 查询结果为多条数据，使用索引查询且是范围索引，如使用&gt;、&lt;、BETWEEN、AND、OR、IN等 *index : * 查询结果为多条数据，使用索引查询，但对索引表进行全表扫描。 *all : * 查询结果为多条数据，未使用索引，对全表数据扫描。 一般来说，在实际业务中，system和const情况几乎不可能达到，而index和all的效率过低是主要的被优化目标，而期望的优化目标则为eq_ref、ref，range情况特殊它属于范围内的索引表扫描，实际优化应考虑索引扫描范围。 [possible_keys]： SQL优化器执行前预估的索引类型。 [key]： 实际执行时用到的索引类型。 [key_len]： 实际使用的索引长度，用于确认用到的索引。 [ref]： 联接查询时的联接条件 [rows]： 返回结果集时，所查询的总行数。 [filtered]： 表示返回数据在server层过滤后，剩下多少满足查询的记录数量的百分比，同partition，在5.7版本以前需要使用explain extended查询。 [Extra]： 表示查询时额外的说明信息，该字段与type一样，同样时需要优化时特别关注的，常遇到的类型包括：using filesort、using temporary、using index、using where、distinct等，特别需要关注前两项，它们出现是性能损耗过大的表现。 using filesort : ** 常见于使用order by 排序，由于索引查询后排序并未使用索引字段或索引字段失效，导致排序时将在内存中的“排序空间”进行，排序空间通过sort_buffer_size**设置，会额外产生空间和时间的浪费。 *using temporary : * 常见于group by或order by操作，当查询结果进行分组时会在内存中额外使用“临时表”空间存储和聚合，额外产生空间和时间的浪费，多见于order by或group by字段并非多表查询结果字段。 *using index : * 表示当前查询使用了索引查询，无需回表查询，性能提升。 using where : ** 返回的记录并不是所有的都满足查询条件，需要在server层进行过滤，即回表查询，属于“后置过滤”，在版本5.6之后出现了using index condition**，它的含义是先在索引表中查询复合索引过滤条件的数据，再将这些数据使用where其他条件进行过滤，与using where相比，将索引过滤提前到索引表内，所以where条件优先设置索引过滤（SQL优化器是否会自动优化还待确认）。 *distinct : * 表示数据查询后使用了distinct筛查，将查询结果进行二次全部扫描，排除重复项。 3.2.2 最左前缀原则&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在复合索引的使用过程中经常提到”最左前缀原则“，它的意思是说，当创建一个复合索引tb_index(col1, col2, col3)，使用索引必须按照严格的定义顺序，比如SELECT * FROM tb WHERE col=x and col2=xx and col3=xxx，这样联合索引才能达到最高效率。如果执行如下破坏顺序索引的例子将不能完全发挥索引功能或丧失索引功能：SELECT * FROM tb WHERE col1=x and col3=xxx（跳过中间索引col2，只有col1生效，col3无法使用索引），SELECT * FROM tb WHERE col2=xx and col3=xxx(跳过col1，索引全部失效)。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了探讨最左前缀原则的原理，重新回到索引的数据结构，我们已经讨论过无论是MyISAM还是InnoDB引擎都是用B+Tree数据结构进行索引表的保存，不妨以科学疑问的形式提出两种数据结构的假设： 一个列代表一个b+tree结构，多个列则分表对应多个b+tree，比如上例提到的复合索引，col1、col2、col3在物理模型上分别对应三个b+tree文件，当执行SELECT * FROM tb WHERE col1=x and col2=xx and col3=xxx时，首先从col1索引表中定位，定位到的data域保存指向col2索引的指针从而在col2对应的索引表中定位，col3同理，最终在col3的叶子节点data域中找到目标结果。 在物理模型上只有一个索引文件，即一个b+tree保存联合索引，并在key域中以严格的定义顺序保存多列索引字段按次序依次定位，从而最终获得目标结果。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;先确认第二条为正确结果，那么我们来讨论第一条为什么不是，为什么数据库大多都采用B+Tree作为结构模型而非红黑树之类，主要原因在于索引本身就是一张容量较大的表结构，使用内存是性能损耗和系统稳定性都受点影响的一件事，那么只能考虑作为文件保存在物理磁盘上，而物理磁盘的IO效率又过低容易影响系统整体的吞吐量，所以衡量索引最重要的标准就是在查询中减少磁盘IO的次数，显然第一条假设严重违背了这样的标准，因为如果复合索引的多列分别对应多张索引的话，那么磁盘上的文件也会一一对应，当我们执行覆盖索引的SQL语句时，就代表经过多次磁盘IO，效率很可能反而降低了。 3.2.3 避免索引失效&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在索引列上做任何操作(计算、 函数、自动\\手动类型转换)，会导致索引失效而转向全表扫描，究其原因，主要是破坏（不满足）B+Tree索引表的查询条件，以下常见场景需要注意（由于MYSQL优化器在不同版本之间表现不同，所以结果可能有出入）： 模糊查询LIKE后接匹配符 % 或 _时，只能出现在最末位置。 当前tb_course表中数据，以及索引情况如下，索引只有主键索引和联合索引union_index(name, is_required, credit)。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869mysql&gt; SELECT * FROM tb_course ;+----+-----------------+-------------+--------+| id | name | is_required | credit |+----+-----------------+-------------+--------+| 1 | C | 1 | 4 || 5 | C# | 0 | 2 || 11 | JAVA | 0 | 0 || 12 | JAVA | 1 | 1 || 13 | JAVA | 1 | 2 || 14 | JAVA | 1 | 3 || 2 | JAVA | 1 | 4 || 10 | Linux | 0 | 2 || 6 | Python | 0 | 3 || 8 | TCP/IP | 0 | 2 || 7 | 人工智能 | 1 | 4 || 4 | 操作系统 | 1 | 4 || 3 | 数据结构 | 1 | 3 || 9 | 计算机网络 | 1 | 3 |+----+-----------------+-------------+--------+mysql&gt; SHOW INDEX IN tb_course\\G*************************** 1. row *************************** Table: tb_course Non_unique: 0 Key_name: PRIMARY Seq_in_index: 1 Column_name: id Collation: A Cardinality: 14 Sub_part: NULL Packed: NULL Null: Index_type: BTREE*************************** 2. row *************************** Table: tb_course Non_unique: 1 Key_name: union_index Seq_in_index: 1 Column_name: name Collation: A Cardinality: 10 Sub_part: NULL Packed: NULL Null: YES Index_type: BTREE*************************** 3. row *************************** Table: tb_course Non_unique: 1 Key_name: union_index Seq_in_index: 2 Column_name: is_required Collation: A Cardinality: 11 Sub_part: NULL Packed: NULL Null: YES Index_type: BTREE*************************** 4. row *************************** Table: tb_course Non_unique: 1 Key_name: union_index Seq_in_index: 3 Column_name: credit Collation: A Cardinality: 14 Sub_part: NULL Packed: NULL Null: YES Index_type: BTREE 12345678910111213141516#MySQL 5.7mysql&gt; EXPLAIN SELECT * FROM tb_course WHERE name LIKE '%AVA%'\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_course partitions: NULL type: indexpossible_keys: NULL key: union_index key_len: 70 ref: NULL rows: 14 filtered: 11.11 Extra: Using where; Using index1 row in set, 1 warning (0.00 sec) 从结果可以看到，最终的执行计划显示使用了联合索引union_index，这是因为在当前测试使用的MYSQL版本为5.7，SQL优化器对%AVA%进行了优化处理使得可以进行索引查询，但在MYSQL5.6之前版本中将显示无法使用索引，执行计划显示使用全局查询，如下： 123456789101112131415#MySQL 5.6mysql&gt; EXPLAIN SELECT * FROM tb_course WHERE name LIKE '%AVA%'\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_course partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 10 filtered: 11.11 Extra: Using where 建议：尽量使用全文索引，如果必须使用模糊查询，建议匹配条件不以%或_开头。 2. OR的前后字段必须都为索引字段，否则索引失效 当OR前后字段只有一个是索引时，那么全部不使用索引，相反当所有字段是索引时才会使用索引。 123456789101112131415mysql&gt; EXPLAIN SELECT * FROM tb_teacher WHERE id=2 OR name='老张'\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_teacher partitions: NULL type: ALLpossible_keys: PRIMARY key: NULL key_len: NULL ref: NULL rows: 10 filtered: 19.00 Extra: Using where1 row in set, 1 warning (0.00 sec) id是主键索引，而name字段无索引，所以结果显示possible_keys: PRIMARY可能使用到主键索引，实际并未使用索引key: NULL。 3. 联合索引需要满足最左原则，否则索引部分失效或全部失效 见 3.2.2 4. 堤防隐式转换破坏索引查询 当字段为varchar类型索引时，如果使用整型类型当作条件查询，则会破坏索引规则，使失效。 1234567891011121314151617181920212223242526272829303132333435363738# 测试新增索引mysql&gt; ALTER TABLE tb_teacher ADD INDEX name_index(name);Query OK, 0 rows affected (0.07 sec)Records: 0 Duplicates: 0 Warnings: 0# 正常mysql&gt; EXPLAIN SELECT * FROM tb_teacher WHERE name='123' \\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_teacher partitions: NULL type: refpossible_keys: name_index key: name_index key_len: 63 ref: const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec)# 失效mysql&gt; EXPLAIN SELECT * FROM tb_teacher WHERE name=123 \\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_teacher partitions: NULL type: ALLpossible_keys: name_index key: NULL key_len: NULL ref: NULL rows: 11 filtered: 10.00 Extra: Using where1 row in set, 3 warnings (0.00 sec) 建议： 日常开发中在书写SQL时需要细心注意索引字段类型，此类问题一般比较隐蔽。 5. 不等表示( &lt;&gt;, !=) 和 空判断(is null, is not null) 使索引失效 因为不等和为空都不会进入索引表，所以即使针对索引列判断也无法生效，将进行全表扫描。 12345678910111213141516171819202122232425262728293031# 不等操作mysql&gt; EXPLAIN SELECT * FROM tb_teacher WHERE name &lt;&gt; '123' \\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_teacher partitions: NULL type: ALLpossible_keys: name_index key: NULL key_len: NULL ref: NULL rows: 11 filtered: 100.00 Extra: Using where # 为空判断 mysql&gt; EXPLAIN SELECT * FROM tb_teacher WHERE name is not null \\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_teacher partitions: NULL type: ALLpossible_keys: name_index key: NULL key_len: NULL ref: NULL rows: 11 filtered: 100.00 Extra: Using where 注意，possible_keys计划使用name_index索引，实际并未使用索引。 当针对整型类型进行不等操作时，被优化器优化处理： 12345678910111213141516171819202122232425262728293031# 主键索引mysql&gt; EXPLAIN SELECT * FROM tb_teacher WHERE id &lt;&gt; 10 \\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_teacher partitions: NULL type: rangepossible_keys: PRIMARY key: PRIMARY key_len: 4 ref: NULL rows: 10 filtered: 100.00 Extra: Using where # 联合索引mysql&gt; explain select * from tb_course where credit &lt;&gt; 0 \\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_course partitions: NULL type: indexpossible_keys: NULL key: union_index key_len: 70 ref: NULL rows: 14 filtered: 90.00 Extra: Using where; Using index 6. 计算、函数表达式使索引失效 当对索引字段进行函数或计算则有可能使其失效，之所以说有可能同样时因为不同版本中SQL优化器表现不同所致。 1234567891011121314mysql&gt; EXPLAIN SELECT * FROM tb_course WHERE credit*2 &gt; 2 \\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_course partitions: NULL type: indexpossible_keys: NULL key: union_index key_len: 70 ref: NULL rows: 14 filtered: 100.00 Extra: Using where; Using index 可以看到，此处使用了索引查询，也就表示在当前测试版本中优化器进行了优化操作，在MYSQL 5.6之前索引无效。 7. 当全表扫描速度更快时，索引失效 当优化器认为全表扫描速度优于索引查找时，使索引失效，这种场景往往牵扯到创建索引时涉及的块的读取成本问题。 3.2.4 单表优化实战&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了演示单表优化过程，以tb_course为例，首先删除已经存在的索引drop index union_index on tb_course;，再次申明，测试使用的MYSQL当前版本为5.8，不同版本SQL优化器执行计划不一定相同。 查询学分不为0的必修课的名称。 1234567891011121314mysql&gt; EXPLAIN SELECT name FROM tb_course WHERE is_required=1 AND credit &lt;&gt;0 ORDER BY credit DESC \\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_course partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 14 filtered: 9.00 Extra: Using where; Using filesort 当前type为ALL表示未使用索引，最差全表扫描查询。 是否可以将name, is_required, credit设置为联合索引提高效率？ 123456789101112131415161718mysql&gt; ALTER TABLE tb_course ADD INDEX test_index1(name,is_required,credit);Query OK, 0 rows affected (0.05 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; EXPLAIN SELECT name FROM tb_course WHERE is_required=1 AND credit &lt;&gt;0 ORDER BY credit DESC \\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_course partitions: NULL type: indexpossible_keys: NULL key: test_index1 key_len: 70 ref: NULL rows: 14 filtered: 9.00 Extra: Using where; Using index; Using filesort 在当前版本测试中显示type升级为index，Extra新增 Using index项，效率确实有少许提升（在MySQL5.6版本中可能显示任然无法使用索引或索引失效），但没有达到我们优化的目标级别(range以上)，此时我们仔细分析一下：在SQL执行顺序中（1.3章节），执行顺序应该为 FROM &gt; WHERE &gt; SELECT &gt; ORDER BY，所以当前创建的索引test_index1(name,is_required,credit)并没有按照执行顺序执行，换句话说，当前SQL使用的索引顺序是乱序的。 按照执行顺序创建联合索引是否可行？ 12345678910111213141516171819202122mysql&gt; drop index test_index1 ON tb_course;Query OK, 0 rows affected (0.03 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; ALTER TABLE tb_course ADD INDEX test_index2(is_required, credit, name);Query OK, 0 rows affected (0.04 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; EXPLAIN SELECT name FROM tb_course WHERE is_required=1 AND credit &lt;&gt;0 ORDER BY credit DESC \\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_course partitions: NULL type: rangepossible_keys: test_index2 key: test_index2 key_len: 7 ref: NULL rows: 10 filtered: 100.00 Extra: Using where; Using index 很明显可以看到type效率达到了range级别，并且减少了Using filesort这个特别消耗性能的操作。 3.3 ORDER BY排序原理与优化&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在Mysql版本中ORDER BY的排序算法经过了两代演进，最初使用双路排序算法：将排序字段和对应的行指针从磁盘中读出并在内存中进行排序，遍历该排序列表并从磁盘读取原表匹配返回查询结果。可以看到双路排序算法经过了两次磁盘IO，这在效率上很受影响，属于空间优于时间策略。对应的优化版本则是单路排序算法：将排序字段及其对应的所有查询项一次从磁盘读取获得并在内存中进行排序，排序后的结果即是输出结果，在该算法下将磁盘IO降到最低，对时间效率进行了提升，但同时需要注意多字段读出到内存中，如果数据量巨大则容易导致OOM，如果在系统内存范围内但同样数据量较大则容易产生回表操作，反而不如双路排序算法有优势。在内存中进行排序操作依赖Mysql在内存中创建的缓存区buffer大小，如果数据量超出buffer就会导致创建临时表或回表操作甚至发生OOM，Mysql默认buffer为1024字节，所以预估数据量大小很重要或者使用explain执行计划查看低效率风险，将buffer设置在预估范围边界。 12345678910111213141516mysql&gt; show variables like &quot;max_length_for_sort_data&quot;;+--------------------------+-------+| Variable_name | Value |+--------------------------+-------+| max_length_for_sort_data | 1024 |+--------------------------+-------+mysql&gt; set global max_length_for_sort_data=2048;Query OK, 0 rows affected (0.01 sec)mysql&gt; show variables like &quot;max_length_for_sort_data&quot;;+--------------------------+-------+| Variable_name | Value |+--------------------------+-------+| max_length_for_sort_data | 2048 |+--------------------------+-------+ 以此类比，GROUP BY也是类似问题，只不过优先排序后分组，所以优化策略相同。 3.4 慢查询日志&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在复杂业务场景下，Mysql经常会因为某个复杂查询SQL影响整体性能，甚至出现假死状态，主要原因是因为数据业务量大致使该查询语句耗费太多时间，Mysql提供慢查询日志支持在线网环境下定位具体慢查询语句，默认情况下慢查询处于关闭状态，可以使用指令slow_query_log查看（配置文件开启可以永久生效），通过指定日志文件（默认文件为slow-query-log-file）和设定慢查询时间阈值来截取SQL。 1234567891011121314151617181920212223242526272829303132333435# 查询慢查询开关是否开启mysql&gt; show variables like &quot;slow_query_log&quot; ;+----------------+-------+| Variable_name | Value |+----------------+-------+| slow_query_log | OFF |+----------------+-------+# 开启慢查询mysql&gt; set global slow_query_log=1;Query OK, 0 rows affected (0.02 sec)mysql&gt; show variables like &quot;slow_query_log&quot;;+----------------+-------+| Variable_name | Value |+----------------+-------+| slow_query_log | ON |+----------------+-------+# 查询时间阈值mysql&gt; show variables like 'long_query_time';+-----------------+-----------+| Variable_name | Value |+-----------------+-----------+| long_query_time | 10.000000 |+-----------------+-----------+# 设置时间阈值mysql&gt; set global long_query_time=5;mysql&gt; show variables like 'long_query_time';+-----------------+-----------+| Variable_name | Value |+-----------------+-----------+| long_query_time | 5.000000 |+-----------------+-----------+ 配置my.ini或my.cnf配置慢查询（需重启服务） 12345# 配置文件[mysqld]slow_query_log =1long_query_time=5slow_query_log_file=D:\\\\software\\\\work\\\\mysql-5.7.23-winx64\\\\slowquery.log 1234567891011121314151617181920mysql&gt; show variables like &quot;slow_query_log&quot;;+----------------+-------+| Variable_name | Value |+----------------+-------+| slow_query_log | ON |+----------------+-------+mysql&gt; show variables like &quot;long_query_time&quot;;+-----------------+----------+| Variable_name | Value |+-----------------+----------+| long_query_time | 5.000000 |+-----------------+----------+mysql&gt; show variables like &quot;slow_query_log_file&quot;;+---------------------+----------------------------------------------------+| Variable_name | Value |+---------------------+----------------------------------------------------+| slow_query_log_file | D:\\software\\work\\mysql-5.7.23-winx64\\slowquery.log |+---------------------+----------------------------------------------------+ 通过sleep函数设置睡眠时间来测试： 1mysql&gt; select sleep(6); 手动查看日志 1234567MySQL, Version: 5.7.23-log (MySQL Community Server (GPL)). started with:TCP Port: 3306, Named Pipe: (null)Time Id Command Argument# User@Host: root[root] @ localhost [::1] Id: 3# Query_time: 5.999740 Lock_time: 0.000000 Rows_sent: 1 Rows_examined: 0SET timestamp=1586104027;select sleep(6); 使用mysql提供的慢查询命令mysqldumpslow查看 123# 按照平均时长排序并输出前十项C:\\Users\\zhy&gt;mysqldumpslow.pl -s at -t 10 D:\\software\\work\\mysql-5.7.23-winx64\\slowquery.log# mysqldumpslow.pl需要安装perl环境才能使用 3.5 使用Profiling&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;除了使用Explain执行计划查看单个SQL的执行效率外，还可以通过Profiling来查看当前会话中多条SQL的执行性能，主要指标是CPU、BLOCK IO、CONTEXT SWITCH、MEMORY、SWAPS等。 CPU：显示CPU使用相关开销 BLOCK IO：阻塞IO相关开销 CONTEXT SWITCH：上下文切换相关开销 MEMORY：内存相关开销 SWAPS：交换之间的开销 默认情况下是关闭的，可以开启并显示最近执行的SQL 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292# 查看开关和历史记录数mysql&gt; show variables like 'profiling%';+------------------------+-------+| Variable_name | Value |+------------------------+-------+| profiling | OFF || profiling_history_size | 15 |+------------------------+-------+# 设置开关和记录数mysql&gt; set global profiling=1;mysql&gt; set global profiling_history_size=20;mysql&gt; show variables like 'profiling%';+------------------------+-------+| Variable_name | Value |+------------------------+-------+| profiling | ON || profiling_history_size | 20 |+------------------------+-------+# 查看最近的执行SQLmysql&gt; show profiles;+----------+------------+----------------------------------+| Query_ID | Duration | Query |+----------+------------+----------------------------------+| 1 | 0.00040300 | select @@version_comment limit 1 || 2 | 0.00438400 | show variables like 'profiling%' || 3 | 0.00018800 | show version() || 4 | 0.00028400 | select version() || 5 | 0.00044975 | show engines || 6 | 0.00034125 | SELECT DATABASE() || 7 | 0.00255175 | select * from tb_teacher |+----------+------------+----------------------------------+# 查看Query_ID为7的所有性能信息*************************** 1. row *************************** Status: starting Duration: 0.000211 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: NULL Source_file: NULL Source_line: NULL*************************** 2. row *************************** Status: checking permissions Duration: 0.000016 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: check_access Source_file: sql_authorization.cc Source_line: 810*************************** 3. row *************************** Status: Opening tables Duration: 0.001980 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: open_tables Source_file: sql_base.cc Source_line: 5685*************************** 4. row *************************** Status: init Duration: 0.000026 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: handle_query Source_file: sql_select.cc Source_line: 121*************************** 5. row *************************** Status: System lock Duration: 0.000012 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: mysql_lock_tables Source_file: lock.cc Source_line: 323*************************** 6. row *************************** Status: optimizing Duration: 0.000004 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: JOIN::optimize Source_file: sql_optimizer.cc Source_line: 151*************************** 7. row *************************** Status: statistics Duration: 0.000016 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: JOIN::optimize Source_file: sql_optimizer.cc Source_line: 367*************************** 8. row *************************** Status: preparing Duration: 0.000017 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: JOIN::optimize Source_file: sql_optimizer.cc Source_line: 475*************************** 9. row *************************** Status: executing Duration: 0.000003 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: JOIN::exec Source_file: sql_executor.cc Source_line: 119*************************** 10. row *************************** Status: Sending data Duration: 0.000078 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: JOIN::exec Source_file: sql_executor.cc Source_line: 195*************************** 11. row *************************** Status: end Duration: 0.000042 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: handle_query Source_file: sql_select.cc Source_line: 199*************************** 12. row *************************** Status: query end Duration: 0.000010 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: mysql_execute_command Source_file: sql_parse.cc Source_line: 4937*************************** 13. row *************************** Status: closing tables Duration: 0.000009 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: mysql_execute_command Source_file: sql_parse.cc Source_line: 4989*************************** 14. row *************************** Status: freeing items Duration: 0.000107 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: mysql_parse Source_file: sql_parse.cc Source_line: 5594*************************** 15. row *************************** Status: cleaning up Duration: 0.000022 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: dispatch_command Source_file: sql_parse.cc Source_line: 1924 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107# 使用具体的性能分类，比如cpu和block iomysql&gt; show profile cpu,block io for query 7 \\G*************************** 1. row *************************** Status: starting Duration: 0.000211 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL*************************** 2. row *************************** Status: checking permissions Duration: 0.000016 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL*************************** 3. row *************************** Status: Opening tables Duration: 0.001980 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL*************************** 4. row *************************** Status: init Duration: 0.000026 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL*************************** 5. row *************************** Status: System lock Duration: 0.000012 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL*************************** 6. row *************************** Status: optimizing Duration: 0.000004 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL*************************** 7. row *************************** Status: statistics Duration: 0.000016 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL*************************** 8. row *************************** Status: preparing Duration: 0.000017 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL*************************** 9. row *************************** Status: executing Duration: 0.000003 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL*************************** 10. row *************************** Status: Sending data Duration: 0.000078 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL*************************** 11. row *************************** Status: end Duration: 0.000042 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL*************************** 12. row *************************** Status: query end Duration: 0.000010 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL*************************** 13. row *************************** Status: closing tables Duration: 0.000009 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL*************************** 14. row *************************** Status: freeing items Duration: 0.000107 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL*************************** 15. row *************************** Status: cleaning up Duration: 0.000022 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL 应重点关注以下几个情况： converting HEAP to MyISAM ：查询结果太大，内存转磁盘。 Creating tmp table :创建了临时表，性能损耗严重。 Copying to tmp table on disk：把内存中的临时表复制到磁盘，性能损耗严重 locked ：被加锁。","link":"/2020/02/24/2%20MySQL/MySQL%E4%BC%98%E5%8C%96%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89/"},{"title":"mysql-5.7.x zip版详细配置","text":"MySQL 5.7.x 解压版配置和服务安装 下载5.7.x zip版本MySQL并解压到本地 在文件根目录下新建my.ini文件，并编辑如下内容 1234567891011121314151617181920[mysqld]port = 3306basedir=A:\\dev\\mysql-5.7.23-winx64datadir=A:\\dev\\mysql-5.7.23-winx64\\data#如果不生效，则使用下列表达形式#basedir=A:\\\\dev\\\\mysql-5.7.23-winx64#datadir=A:\\\\dev\\\\mysql-5.7.23-winx64\\\\datamax_connections=200character-set-server=utf8default-storage-engine=INNODBsql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES[mysql]default-character-set=utf8 注意： 此时根目录下还不存在data文件，但需要提前配置好datadir 以管理员权限打开cmd，并进入根目录 123c:\\ &gt; cd A:\\dev\\mysql-5.7.23-winx64A:\\dev\\mysql-5.7.23-winx64 &gt; cd bin 先执行remove操作删除可能存在的服务，在安装新服务 123A:\\dev\\mysql-5.7.23-winx64\\bin &gt; mysqld -removeA:\\dev\\mysql-5.7.23-winx64\\bin &gt; mysqld -instal 初始化MySQL服务，并在根目录下创建data目录，并创建初始化用户root 1A:\\dev\\mysql-5.7.23-winx64\\bin &gt; mysqld --initialize-insecure --user=mysql 启动MySQL服务 1A:\\dev\\mysql-5.7.23-winx64\\bin &gt; net start mysql 为root用户创建登录密码 12A:\\dev\\mysql-5.7.23-winx64\\bin &gt; mysqladmin -u root -p password 新密码Enter password: 由于初始化密码为空，所以第二步骤可以直接enter 登录mysql成功！ 12345678910111213141516A:\\dev\\mysql-5.7.23-winx64\\bin &gt; mysql -u root -p Enter password:密码Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 9Server version: 5.7.23 MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.mysql&gt;show databases; 安装和破解Navicat Premium 12.x 在这里下载64位12.x版本Navicat Premium 在这里下载注册机 正常安装后使用注册机破解 毕竟不是光彩的事情，详情请戳这里","link":"/2018/10/08/2%20MySQL/mysql-5-7-x%20zip%E7%89%88%E8%AF%A6%E7%BB%86%E9%85%8D%E7%BD%AE/"},{"title":"MySQL优化系列（四）","text":"MySQL优化系列（四） 3.6 锁机制和执行引擎3.6.1 锁定义和分类&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在数据连接池提供数据库多线程操作时，资源的并发访问或操作就容易导致各种并发问题，例如脏读、幻读、不可重复读等。按照锁的控制粒度分类：行级锁（包括间隙锁和next-key）、表级锁、页锁； 按照锁的功能分类：共享锁（读锁）、排它锁（写锁）。由于锁的目的是保证在任务执行过程中避免并发问题，所以它通常由Mysql逻辑分层中的引擎层持有，对服务层隔离，同时由引擎层的特性决定了插件式使用方式。 行级锁：锁的持有对象是数据库表中的一行数据，是粒度最小的锁同时并发程度最高。 间隙锁（Gap-Lock）：是特殊的行级锁，物理空间不存在但逻辑上被锁定的行锁，具有行锁的特性。 next-key：是行锁和间隙锁的组合形式。 表级锁：锁的持有对象是数据库的表，粒度最大同时并发程度最低。 页锁：锁的持有对象是数据库中的磁盘管理最小单位——页，粒度和并发度介于行锁和表锁之间。 共享锁：又称为读锁，其他事务可读不可写。 排它锁：又称为写锁，其他事务不可读不可写。 粒度锁的性能比较： 行级锁： 锁的开销大，影响性能。 粒度最小，并发程度最高，锁冲突概率低，会出现死锁。 适合大量的并发更新同时又有并发查询的场景。 表级锁: 锁的开销小，性能影响低。 粒度最大，并发程度低，锁冲突概率大，不会出现死锁。 适合以查询为主，并发数小的场景。 页锁介于行级锁和表锁之间。 3.6.2 并发和事务&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;常见的关系型数据库事务并发问题包括：脏读、幻读、不可重复读。 脏读：当事务1查询某一行数据时，此时事务2进行了修改，那么事务1读取的数据并非当时查询需要返回的数据。 事务1 事务2 SELECT money FROM tb_account WHERE userId=1; UPDATE tb_account SET money = 1000WHERE userId=1;COMMIT; 查询成功（脏读） 不可重复读：当事务1在第一次查询后事务2进行了修改未提交，事务1再次进行查询发现数据与上一次不一致。 事务1 事务2 SELECT money FROM tb_account WHERE userId=1; UPDATE tb_account SET money = 1000WHERE userId=1; SELECT money FROM tb_account WHERE userId=1;(与上一次上旬结果不一致) ROLLBACK； 幻读：当事务1第一次进行范围查询后事务2新增/删除了一条数据，当事务1再次查询时范围总量与上一次不一致。 事务1 事务2 SELECT * FROM tb_account DELETE FROM tb_accountWHERE userId=1; SELECT * FROM tb_account ROLLBACK; 可以发现，幻读和不可重复读的区别在于前者是多数量查询且修改点是新增和删除，后者是单数据查询且修改点是更新操作。结合3.6.1节介绍，脏读可以通过对事务1查询添加行级共享锁从而避免其他事务写操作干扰，或者对事务2添加行级排他锁，使事务1读操作阻塞；不可重复读可以采取对事务2的写操作添加行级排他锁，使事务1的读取阻塞，在完成写操作后开放读取；幻读，通常采用MVCC + next-key方式保护并发执行。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;事务是数据库处理业务逻辑的基本单位，通常由一组SQL组成，执行状态分为成功和失败回滚。事务具有ACID（酸）特性： 原子性(Actomicity)：业务处理的最小单位，全部成功则事务执行成功，否则事务执行失败进行回滚。 一致性(Consistency)：事务操作中，期间的数据要保持一致状态，例如事务A由增删改查组成，当修改其中一条数据后，相关的比如查询操作也应该进行更改。事务执行结束，内部结构如B+Tree也应该是正确的。 隔离性(Isolation)：由于原子性的存在，不同的事务都应该是独立的个体且不能互相影响，这就需要隔离机制进行保护。 持久性(Durability)：事务执行结束后，对数据的更改应该是永久的。 在Mysql的InnoDB引擎中，所有的操作都被当作事务处理，如果存在并发行为就会产生上节所说的情况发生，而如何防止并发问题发生就依赖锁机制和隔离机制。 3.6.3 隔离机制&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mysql提供四种隔离级别，包括：Read Uncommitted、Read Committed、Repeatable Read、Serializable。隔离机制的存在使事务并发执行过程中保证数据对外的可见性，或者说针对隔离的场景不同（对并发问题的容忍程度）通常需要按业务实际情况选择，隔离级别越低则性能损耗越小。 Read Uncommitted（RU）：在该隔离级别，所有事务都可以看到其他未提交事务的执行结果，很少用于实际，因为性能不比其他级别好多少，允许脏读、不可重复读、幻读。 Read Committed（RC）：该隔离级别支持一个事务只能看见已经提交事务所做的改变，即未提交数据不可见，允许不可重复读和幻读，不允许脏读。 Repeatable Read（RR）：这是MySQL的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行，允许幻读，不允许脏读和不可重复读，InnoDB可以通过MVCC解决幻读问题。 Serializable：这是最高的隔离级别，它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题，不允许脏读、不可重复读、幻读。 隔离级别 脏读 不可重复读 幻读 Read Uncommitted √ √ √ Read Committed × √ √ Repeatable Read × × √ Serializable × × × 1234567891011121314151617# 查看当前隔离级别mysql&gt; show variables like '%isolation%';+-----------------------+-----------------+| Variable_name | Value |+-----------------------+-----------------+| transaction_isolation | REPEATABLE-READ || tx_isolation | REPEATABLE-READ |+-----------------------+-----------------+mysql&gt; set global transaction_isolation ='read-committed';mysql&gt; show variables like '%isolation%';+-----------------------+----------------+| Variable_name | Value |+-----------------------+----------------+| transaction_isolation | READ-COMMITTED |+-----------------------+----------------+ 3.6.4 MyISAM与InnoDB引擎及其锁和死锁&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MyISAM和InnoDB是Mysql最典型的两个插件式数据库引擎，两者最大的不同除了在事务支持上不一样（MyISAM不支持事务），以及B+Tree索引结构不同（非聚簇索引和聚簇索引），最主要的就是锁机制不同。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MyISAM在执行SELECT时，会自动给涉及的表加共享锁，在执行UPDATE、INSERT、DELETE前添加表排他锁，加锁过程不需要用户通过LOCK TABLE显示的干预。它支持在一个线程进行共享锁查询时，其他线程可以并发插入数据以减少线程对表的竞争，例如，在一个线程A对表添加共享锁之后执行SELECT查询，此时线程B可以同时将新数据从表的末尾插入，但这需要满足一个条件，就是当表中没有空闲块（由删除造成的一行数据缺失）时，才能触发此并发行为，否则并发插入是被禁止的。 12345678910111213141516# 查询MyISAM并发插入的状态mysql&gt; show variables like &quot;concurrent_insert&quot;;+-------------------+-------+| Variable_name | Value |+-------------------+-------+| concurrent_insert | AUTO |+-------------------+-------+# 查询表锁竞争mysql&gt; show status like 'table_lock%';+-----------------------+-------+| Variable_name | Value |+-----------------------+-------+| Table_locks_immediate | 111 || Table_locks_waited | 5 |+-----------------------+-------+ 可以通过查询表锁竞争来查看当前锁的阻塞情况，当 Table_locks_waited 过大时存在严重的竞争影响系统执行效率。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;InnoDB支持行级共享锁和排它锁，同时内部实现对表添加的意向锁（IS和IX），意向锁是内部实现的不需要用户手动干预，当InnoDB准备对一行数据添加共享锁时需要先获得该表的意向共享锁IS，同理添加排它锁需要获得意向排它锁IX。在执行UPDATE、INSERT、DELETE时会自动添加排它锁，对于SELECT，一般查询不会加锁。 1234567891011121314151617# 对SELECT手动添加共享锁mysql &gt; SELECT * FROM tb_course WHERE id=5 LOCK IN SHARE MODE;# 对SELECT手动添加排它锁mysql &gt; SELECT * FROM tb_course WHERE id=5 FOR UPDATE;# 查看innodb锁竞争情况mysql&gt; show status like 'innodb_row_lock%';+-------------------------------+-------+| Variable_name | Value |+-------------------------------+-------+| Innodb_row_lock_current_waits | 0 || Innodb_row_lock_time | 0 || Innodb_row_lock_time_avg | 0 || Innodb_row_lock_time_max | 0 || Innodb_row_lock_waits | 0 |+-------------------------------+-------+ 手动添加锁的场景一般时为了保证查询的数据在事务执行期内是最新的，排它锁更为严格因为它不运行其他线程添加锁且必须阻塞，而共享锁则有可能存在其他线程也对同一数据添加了共享锁而不能更新。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;死锁更容易在严格的隔离级别上以及InnoDB上发生，当事务试图以不同的顺序锁定资源时，就可能产生死锁，多个事务同时锁定同一个资源时也可能会产生死锁。所以InnoDB本身也有检测死锁的机制，可以检测到死锁的循环依赖并立即返回一个错误，当死锁发生后，只有部分或完全回滚其中一个事务，才能打破死锁，所以InnoDB采取将持有最少行级排他锁的事务进行回滚作为解除死锁的策略，或者设置死锁超时策略。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MyISAM由于在执行DML或DQL时，一次性添加锁，所以不会发生死锁现象，而InnoDB想要规避死锁发生通常建议在单个事务中一次性获得需要的锁避免加锁顺序不一致导致死锁发生，或者当事务中可能既存在共享锁又存在排它锁，那么直接使用排它锁，以一点效率来换取无死锁的发生；如果修改多个表，则先按顺讯依次添加锁；降低隔离级别。 3.7 Mysql主从复制与Binlog&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主从复制是指数据可以从一个Master节点数据库异步复制到其他服务器Slaver节点，是实现数据库集群的策略，支持横向扩展，提供”一主多从“或”主主复制“，使用主从复制具有以下优势： 实现数据的热备份，提高数据安全性和容灾率。 满足高性能要求，在QPS达到瓶颈时可以通过多个Slavers策略来实现负载均衡。 对于数据采集、建模等内部业务需求可以采取在Slaver进行而不影响生产性能。 数据下发，远程系统创建本地副本而无需建立远程连接访问。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mysql主从复制原理，如上图所示，在开启主从复制功能后，主节点Master会启动一个IO线程，将主服务器发生的改变通过线程写入转储文件bin-log中，从节点Slaver会启用两个线程——IO线程和SQL线程，IO线程负责从Master的binlog中读取数据并写入本地延迟文件Relay Log中，SQL线程从延迟文件中读取数据并写入本地数据库，这样就完成了一个主从复制周期。 Master的IO线程基于数据库改变（DDL和DML）来启动写binlog操作，而Slaver通过定期探查binlog改变状态来启动IO线程读取数据； 确保Master的数据库版本小于等于Slaver数据库版本； 保证Master和Slaver的时间同步。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;更进一步来说，Master向binlog写操作是基于DDL和DML的发生，SELECT除外，也就说是基于事件保存，所以Slaver读取的也是事件信息并在本地通过SQL线程进行回放重演来完成数据库副本的同步，当master发生高吞吐量导致数据量激增超过Slaver线程的处理能力，或者Slaver发生排它锁导致SQL线程同步数据长时间阻塞就会发生相对较高的延迟反应，为了解决这一状况可以采用如下措施： 实现读写分离，按照常规场景的经验DQL的频率远远高于DML（当然也有少量特殊的相反场景），所以将Master设为只写而从库只读，再通过横向扩展从库或者提高从库硬件性能并配合负载均衡，从而保护Master，并将性能均摊。 再应用层面通过在服务层和持久层添加缓存，缓解数据底层压力。 Master配置 1234567891011121314151617# 编辑配置文件my.ini或my.cnf[mysqld]# 保证log-bin权限开放log-bin=/var/mysql/log/master-log-bin//日志文件格式binlog_format=&quot;mixed&quot; # 集群标识server-id=1# 要进行复制的数据库名binlog-do-db=copied_table# 重启服务 12345678# 保证skip_networking关闭状态，否则Slaver无法完成基于网络的通信mysql&gt; show variables like '%skip_networking%';+-----------------+-------+| Variable_name | Value |+-----------------+-------+| skip_networking | OFF |+-----------------+-------+ Slaver配置 123456789101112131415# 同样修改配置文件my.ini或my.cnf[mysqld]server-id=2# Master IPmaster-host=192.168.XXX.XXX # 用于创建连接的验证信息master-user=user master-password=123456 master-port=3306master-connect-retry=60# 要进行复制的数据库名binlog-do-db=copied_table 1234# 开启Slavermysql&gt; slave start;# 查看状态mysql&gt; show slave status; 建议专门创建一个用于进行主从复制的数据库账号。 Binlog的另一重要作用 binlog除了是主从复制的重要文件，同时它还是数据恢复的重要文件，当数据库由于各种原因导致数据丢失时，通过mysqlbinlog工具从binlog文件中恢复最近的正常状态。 以上介绍的属于Mysql Replication主从同步方案，由于所有的IO线程（Master的IO Thread和Slaver的IO Thread）都是单向执行的，也就是说Slaver假如发生数据库的改变是无法同步到Master的，所以既要求有客观的管理规则来约束又要求在架构上设计严谨，可想而知如果应用在电商平台或者金融平台等对数据一致性和传输安全性有更高要求的地方则并不那么稳妥，PXC主从同步是为满足这一场景而设计，它以牺牲同步效率为代价，提高了数据传输安全性并通过任意节点的数据更高都会对整个集群节点进行广播更新，从而达到数据一致的要求。 3.8 并发控制&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在一般程序的并发控制过程中，都存在两种策略，分别是：悲观锁和乐观锁。在 Mysql 多事务并发过程中悲观锁策略称为 ”LBCC”，乐观锁策略称为 ”MVCC“（实际上，MVCC与乐观锁不同，但从非加锁策略上是一致的）。 3.8.1 LBCC&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LBCC （Lock Based Concurrency Control），是一种基于排它锁的并发策略。当多个事务对统一数据发生并发行为时，如果一个事务首先获得数据执行权，当他执行查询操作，那么会对数据加持共享所（写锁），查询对其他数据可见，而修改使用的排它锁需要被阻塞等待；当他执行修改操作（增删改）时，对数据加持排它锁，其他事务的查询和修改必须被阻塞，等待竞争锁。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在任何时候，悲观锁策略的效率都是很低的，特别对于数据库系统的高可用性影响很大，所以为了提高并发效率 Mysql 采用 MVCC策略。 3.8.2 MVCC&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MVCC（Multi-Version Concurrency Control）又称为多版本并发控制，MVCC 的核心是通过 版本链快照 和 一致性对象 ReadView 技术来完成，它也是目前主流数据库处理并发的首选方案。在具体的介绍 MVCC 技术前先对相关技术进行介绍。 3.8.2.1 redo日志和undo日志&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Redo日志是物理层面，针对”页（page）“修改而创建的日志，它记录了事务执行过程中的写动作，虽然功能上与 Binlog 日志相似，但在本质上有明显区别： Binlog 不区分数据库引擎，都会创建；Redo 日志仅在 InnoDB 引擎中使用； Binlog 虽然也是磁盘保存的二进制文件，但本质上它属于逻辑层日志（针对行数据的变动）；Redo 属于物理层日志，当 page 发生改变，就会被 redo 记录； 在事务处理时，事务中的变动会持续被 Redo 记录；只有当事务提交后才会被 Binlog 记录。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Undo 日志是属于逻辑层日志，针对每一行数据修改后记录，它是实现 MVCC 版本链的核心。 3.8.2.2 隔离级别可见性 和 MVCC使用场景&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当多个事务并发执行时，RU 级别的修改对其他事务始终保持可见性；RC 级别只有当事务被提交后对其他事务才具有可见性；RR 级别，当事务被提交后，其他事务内的同一查询始终保持结果一致；Serializable 级别，由于被数据库强制编排执行顺序，所以事务并发结果受数据库影响。因此发挥 MVCC 并发处理能力的场景只适合 RC 和 RR。 3.8.2.3 组织版本链&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在 MVCC 中，数据库表中的每条数据都会额外创建两个隐藏字段：当前事务id，记作 data_tx_id 和 回滚id，记作 data_rollback_id（如果数据库没有主键那么还会额外创建一个字段主键id，记作data_key_id）。data_tx_id 用于表示修改当前数据的事务id，data_rollback_id 是一个指针，指向当前被修改数据前的旧数据id，如果当前数据是新增数据则该指针为空。 insert 操作 新增一条数据后，它的 data_tx_id 为当前插入记录的事务 id，即data_tx_id = 事务id，data_rollback_id为空。 delete 操作 删除一条数据时，首先会将这行数据添加 排它锁 ，然后将该数据存入 undo 日志中，当事务提交时删除该数据。 update操作 对数据添加排它锁，将该数据存入 undo 日志中（data_tx_id，data_rollback_id不变）。 创建一行该数据的副本并进行更新，副本数据中的 data_tx_id 等于当前事务id，data_rollback_id 指向上一条存入 undo 日志中的数据，这样通过指针就创建了当前数据与历史数据的联系，如果存在多个 update 行为，那么就会形成一条连续的版本链，每条版本链对应的数据称为快照数据。 redo 日志记录，包括 undo 日中的修改。 delete 操作可以看成是特殊的 update 操作。 3.8.2.4 数据读操作的一致性——ReadView列表&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在解决了 DML 操作后，针对 select 操作就牵扯到 RC 和 RR 事务隔离级别对于数据可见性的问题。回顾一下：RC 不允许脏读、允许不可重复读和幻读，那么就意味着一个事务 A 在处理过程中，对事务 B 数据不可见，当 A 提交之后，对事务 B 可见；RR 不允许脏读和可重复读、允许幻读，那么就意味着当开启事务 B 后，无论事务 A 是否提交，都不会对 B 产生影响，即不可见。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;那么，在已经成型的版本链中，如果决定哪些版本对当前事务可见？为了解决这一问题，引入了ReadView 可读视图列表的概念。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当产生 ReadView 列表时，该列表会收集==当前活跃事务的 id（正在执行的事务）== 进行保存，甄别出这些 id 的最小值记为 tx_min_id，同时会从==已创建的事务（包含已提交和未提交的事务）==中甄别出最大的 id，记为tx_max_id，也假如 ReadView 列表中。 在 RR 隔离级别中，当首次触发 select 查询 （称为 touch first read）时，会创建 ReadView 列表并收集相关事务id，在该事务周期内所有的查询都使用这份 ReadView 保持不变。 在 RC 隔离级别中，一个事务周期内的不同 select 查询每次都会触发创建 ReadView 列表并收集当前的事务id信息，也就是说，每次的 select 对应的 ReadView 列表不尽相同。 也就是因为在创建 ReadView 列表时的策略不同，最终在 RC 和 RR 中产生了不同的结果。每个事务的 id 是由 innodb 分配的，呈现递增态势，用以却别事务开启的先后顺序，因此通过事务 id 的大小就可以判断记录的可见性，以下将根据具体实例阐述 MVCC 的判断过程。 RC 隔离级别下的 MVCC 执行过程。 假设 set val = 10时的事务 id 为 100， ,事务 A 的 id 为 200，事务 B 的 id 为 300 。 当事务 A 开启事务后，第一次 select 时触发创建 ==ReadView1== 列表 [100]，其中当前事务id tx_id = 200，由于tx_id &gt; 100 表示，100 的事务是已经被执行的事务所以可见，查询结果是10。 当事务 B 执行 update val = 20 后会将 val = 10 保存到 undo log，作为版本链最新的记录，并提交事务刷新数据。 当事务 A 再次执行 select 时，再一次触发创建 ==ReadView2== 列表为[100, 300]，当前事务id tx_id &lt; 300，表示事务 B 以执行完毕，对 A 可见，所以读取结果为 20。 RR 隔离级别下的 MVCC 执行过程。 假设 set val = 10时的事务 id 为 100， ,事务 A 的 id 为 200，事务 B 的 id 为 300 。 当事务 A 开启事务后，第一次 select 时触发创建 ==ReadView3== 列表[100]，当前事务id tx_id &gt; 100，表示100的事务已经执行完毕对 A 可见，所以结果为10。 当事务 B 执行 update 并提交事务后，A 再次执行查询，由于在 RR 隔离级别下 第二次查询依然在事务周期内，所以沿用第一次创建的列表 ==ReadView3== [100]，所以结果仍然是10。 3.9 分库分表策略&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;单机使用的关系型数据库，在遇到业务膨胀数据量递增之后，很容易出现性能瓶颈，例如单表或单库数据量达到千万级甚至更高或者磁盘容量超过100G后，IO损耗、内存占用和CPU占用率都会飙升；当业务请求增多会导致数据访问连接数受限长时间阻塞甚至出现锁竞争和死锁的发生。而此时就需要考虑对现有数据库进行拆分，目的为了减少数据库承载的压力，恢复甚至提升数据库性能。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;做数据拆分时从两个维度进行——垂直拆分和水平拆分，而针对业务场景调整拆分的目标包括数据库和表。拆分（Sharding）就是分区分片的意思。 垂直分库，严格来讲不应该算作优化范畴，更确切的应该归为项目架构的改变。随着业务发展，原先设计的关系型数据库架构中的某些子部分迅速膨胀，已经达到了可以单独成库的条件，那么根据业务耦合性，将这类表及附表抽出单独成库并部署在新环境中，达到垂直分库的目的。这类问题一般出现在关系型数据库设计失误或者初创公司为了考虑成本前期内聚式设计，随着业务攀升不得不面临数据分库，而垂直分库本身的思想与业务层面的大系统按照”微服务”思想拆分子系统类似，要求每个子库完成特定范围的数据持久化能力。 水平分库，当垂直分库之后，数据垂直拆分达到了最小原子，数据库中任然存在大量数据，单表性能仍然受限此时需要进行水平分库，参考3.7（主从复制），创建数据库副本形成集群，通过数据库负载均衡将压力均摊，减小单库压力。 垂直分表，配合业务逻辑将表按照字段进行拆分为原子粒度，减小单表内过多的列从而减少量，提高性能，同时达到了解耦的作用。 水平分表，当单表经过垂直拆分后，任然存在大量数据则针对数据行选择进行库内分表或分库分表策略，库内分表只解决单一数据库内表的拆分，对于单机压力的缓解有限；分库分表实际上就是分布式数据库的原理，在集群中相同的数据表按照一定策略保存不同的数据，但这对数据一致性和分布式事务提出了更高的要求。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在进行水平分表时，需要考虑采用怎样的策略来分配数据保存到不同表中，这种策略一般使用特定字段或哈希存储：（1）特定字段是指，可以按照自增整型主键id划分每张表保存的数据量，例如上图所示，每张子表保存id在百万以内的数据，如果超出则继续水平拆分；这样的特定字段根据业务选取，也可按照时间等信息划分。（2）哈希存储是指，按照某一特性的字段求哈希值的方式将数据均匀分布到多个表中，当然需要考虑极端的哈希冲突问题。特定字段可以是一个也可以是一组，这同样需要与业务逻辑相接壤，比如水平拆分5张用户表User，选择userId和userName作为特征字段求哈希值并保存到对应的一张表中，这样就可以保证该用户的多条数据落入同一个库中，减少简单查询的复杂度。 优缺点比较 垂直拆分 优点 以业务逻辑将数据库解耦，业务清晰； 在高并发场景下，可以有效解决IO、数据连接数的性能问题。 缺点 提高了分布式事务的处理难度； 当进行连接操作JOIN时，只能通过代码层进行高层次的聚合，提升了开发难度。 水平拆分 优点 降低了单个数据库的负载能力，保证了系统稳定，提高并发性。 非设计架构层面的解耦，不会带来应用层更大的修改。 缺点 分片的事务处理难度高，数据同步存在延时可能，一致性无法保障 维护难度大，特别是多次改造后。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在垂直拆分和水平拆分中都提到了分布式事务问题，可想而知在单库单表内的事务总是易于控制的，而分布式下需要考虑到节点之间的通信问题，这就导致分布式事务会损耗相对更多的时间，同时导致在共享资源访问时更容易产生并发冲突和死锁问题，并且在数据不断膨胀拆分越来越频繁时会，这种问题会更加严重并不断放大。分布式事务一般采用事务补偿机制，即当事务发生错误通过采用数据对账检查、日志比对、与标准数据对齐等方式进行检查，不同于事务回滚机制，补偿机制属于事后补救措施，会影响数据一致性。一般采用XA协议方式进行分布式事务控制。限于篇幅，只做简单介绍：XA是一个分布式事务协议，包含两个概念：事务管理器（Transaction Manager,TM）和本地资源管理器（Resources Manager, RM）。事务管理器属于协调者，负责各个本地资源的提交和回滚，资源管理器属于参与者，进行实际的执行操作。它通常有二阶段提交和三阶段提交两种，二阶段是指准备阶段和提交阶段： 准备阶段，协调者给每个参与者发送Prepare消息，每个参与者要么直接返回失败，要么在本地执行事务，执行完毕不进行提交并等待，如果参与者节点的事务操作实际执行成功，则它返回一个”同意”消息；如果参与者节点的事务操作实际执行失败，则它返回一个”中止”消息。 提交阶段，如果协调者收到了参与者的失败消息或者超时，直接给每个参与者发送回滚消息；否则，发送提交消息，参与者根据协调者的指令执行提交或者回滚操作，释放所有事务处理过程中使用的锁资源。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当前行业内使用的分库分表数据中间件有很多，通常分为两类：Client 和 Proxy。Client 方案的的优点在于不用部署，运维成本很低，但是各个服务之间都需要耦合 Client 依赖；Proxy 方案优点是对个各个服务都是透明的，但是得需要专门去部署运维。目前使用范围最广的是 MyCat，它的前身是阿里维护的 Cobar，目前已开源并且社区活跃，用户量也很多，属于 Proxy 层方案。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MyCat发展到目前的版本，已经不是一个单纯的MySQL代理了，它的后端可以支持MySQL、SQL Server、Oracle、DB2、PostgreSQL等主流数据库，也支持MongoDB这种新型NoSQL方式的存储，未来还会支持更多类型的存储。而在最终用户看来，无论是那种存储方式，在MyCat里，都是一个传统的数据库表，支持标准的SQL语句进行数据的操作，这样一来，对前端业务系统来说，可以大幅降低开发难度，提升开发速度。 MyCAT的目标是：低成本的将现有的单机数据库和应用平滑迁移到“云”端，解决数据存储和业务规模迅速增长情况下的数据瓶颈问题。从这一点介绍上来看，能满足数据库数据大量存储，提高了查询性能。 3.10 分布式事务控制&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分布式事务是分布式系统，特别是微服务架构中的痛点和难点，通常一个分布式事务中会设计到对多个数据源或业务系统的操作，并满足 ACID 原则。 3.10.1 CAP 理论和 BASE 理论&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CAP 理论是分布式系统的基础理论之一，它是指在一个分布式系统中，一致性（Consistency）、可用性（Availability）、分区容忍性（Partition tolerance）三者不可能同时满足，最多满足其中的两点。 一致性：在同一时刻，分布式中的各节点数据应该保证一致。 可用性：系统提供提供的服务应该一致保持可用，即使部分节点出现故障，系统整体也能正确相应客户端请求。 分区容忍性：系统在遭遇分区故障时，扔可以对客户端的请求做到一致性和可用性。 优点： 原理简单，容易理解和实现。 缺点： 单点风险，事务管理器执行过程中故障； 阻塞问题，事务执行过程中所有节点都处于阻塞状态； 脑裂问题，第二阶段执行过程中，由于某些故障，只有部分节点提交事务，出现出具不一致现象。 3PC &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于 2PC 的缺点，3PC（三阶段提交） 进行了改进：在管理器和所有节点间引入超时机制；在 2PC 的阶段前引入准备阶段，分为 CanCommit、PreCommit、DoCommit。 ==第一阶段==： 事务管理器向所有节点发出 CanCommit 指令，询问是否可以提交事务并等待回复； 如果节点认为没有问题反馈 yes，相反地如果被阻塞则反馈 no。 ==第二阶段==： 如果第一阶段反馈结果全部是 yes，则再次发出 PreCommit 指令并等待回应；各节点执行事务（不提交）并反馈； 如果第一阶段反馈结果存在 no 或者有节点超时，则向所有节点发出 abort 指令，各节点中断本次事务。 ==第三阶段==： 如果第二阶段反馈结果全部是 yes ，最后发出 DoCommit 指令并等待回应，所有节点提交事务并反馈，最终结束本次任务； 如果第二阶段反馈有 no 或者超时，则向所有节点发出中断指令，所有节点利用undo log 回滚并反馈，结束本次任务。 优点： 所有节点使用超时机制更加健壮； 增加CanCommit阶段，尽早发现阻塞节点。 缺点： 脑裂问题依然存在（数据不一致）。 3.10.3 TCC&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TCC 采用服务化的事务补偿机制，需要事务编码实现Try、Confirm、Cancel 三个接口方法，相比于 XA 的数据持久层设计，TCC 提高到业务逻辑层设计。 Try 主要对业务系统进行检查，并预留出可供事务执行的资源，只有所有参与者 Try 成功，才算该阶段成功。 Confirm 主要对业务系统做确认提交，在 Try 成功的基础上做最终执行。 Cancel 在 2 执行失败后，需要实现回滚的逻辑，以及释放 Try 时预留的资源。 优点： 与2PC逻辑类似，容易理解，由业务层实现，促使事务实现粒度更可控。 缺点： 由于需要编码提供接口实现，具有侵入性。 3.10.4 消息队列3.10.5 Saga","link":"/2020/03/03/2%20MySQL/MySQL%E4%BC%98%E5%8C%96%E7%B3%BB%E5%88%97%EF%BC%88%E5%9B%9B%EF%BC%89/"},{"title":"Java源码——HashMap","text":"HashMap &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;HashMap应该是Java世界应用最多的数据结构对象，HashMap从Java1.2开始存在，在Java8经过一次大改，Java5又出现了同步包java.util.concurrent其中就包含高性能的同步HashMap对象ConcurrentHashMap，这其中精彩世界以下揭晓。 0. 哈希表&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;哈希表，又称散列表，是计算机从业者最熟悉不过的数据结构，由于其在数据索引时只拥有O(n)时间复杂度而广泛应用。生活中我们也经常遇到类似情景，比如查字典过程、电话本黄页等等，通过索引关键字获得对应映射的值。 上图为哈希表的简易示意图，哈希表的基本组成包括： 桶，是保存一组数据的空间，通常是一个链表（单向链表）或者一棵树（红黑树）； 槽位，是通过关键字定位数据桶的索引位置，通过关键字定位到槽位我们一般称为落槽。 哈希冲突，理想状态下每个槽位对应的桶只保存一个数据，那么就形成了槽位和数据一一对应关系（例如图中0号槽位和k号槽位），然而由于哈希表中的槽位通常是有限的，当数据量大于槽位个数时势必会产生一个槽位对应的桶中保存了多个数据，这就称为哈希冲突。解决哈希冲突的方式一般采用单向链表或者红黑树。 哈希函数，将数据元素的关键字key作为自变量，通过一定的函数关系计算出的值，即为该元素的存储地址，这个函数称为哈希函数。哈希函数需要解决的两大问题分别是均匀落槽和哈希冲突。前者在极端情况下会产生所有数据落槽在同一个桶中，产生空间资源浪费和性能低效。 加载因子（load factor），理想状态下，如果空间无限大则槽位可以保证只映射一个数据那么它的时间复杂度是完美的O(1)，如果空间有限，为了保证数据查询时间，可以将数据均分到每个桶中形成较短链路的链表，设计合理的哈希表可以平衡空间与时间。空间和时间永远是计算机领域的哲学问题，加载因子就是平衡这一问题的系数(0~1的浮点数)，研究表明，加载因子系数值越大空间损耗越小时间损耗越大，相反如果系数值越小空间损耗越大时间损耗越小，在Java中默认为0.75。 再哈希（rehash），当一个哈希表向槽位更多的哈希表迁移时，由于槽位个数发生变化，这就要求原哈希表中的每个元素重新计算哈希值并落槽到新哈希表中，这一过程称为rehash，是哈希表的重要性能指标之一。 概念虽然较多，但都比较容易理解且重要 举个例子： 当我们需要从手机通讯录中查找某一个人的联系电话，这个过程可以拟化为一次哈希表的查找。 首先，有张三、李三、王三三个人的联系方式需要存入通讯录，我们假定他们的名字就是关键字，通过哈希函数计算关键字获得落槽位置分别是拼音首字母z、l和w，此时这个哈希表槽位数共有三个，每个槽位对应的桶分别是张三、李三、王三的电话号码，当我们需要拨打张三电话时直接找到z对应的电话拨出即可。 然而，又来了三个人需要保存电话，他们分别是张四、李四、王四，通过哈希函数计算名字关键字得到槽位还是z、l和w，这就是哈希冲突，三个槽位对应的桶分别保存了两个电话号码，当我们拨打张四电话时，需要先找到z，在从z的列表中找到张四电话并拨出。 最后，又来了两个人赵三和刘三，很明显目前的通讯录无法保存他们的电话，这时我们需要对通讯录扩容并将现在的六个人电话迁移过去，但是迁移时需要满足一个约束，原来的六个人必须都重新通过哈希函数计算落槽位置，这就是rehash。 1. JAVA 7的实现成员变量 HashMap的实现实际是一个桶数组或者称为链表数组，数组下标索引即槽位，同时定义了默认的加载因子、初始容量、最大范围和扩容条件。 12345678910111213141516171819202122232425262728public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable{ static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // HashMap的默认初始大小16 static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; // HashMap允许的最大范围2^30 static final float DEFAULT_LOAD_FACTOR = 0.75f; // 默认加载因子 /** * The table, resized as necessary. Length MUST Always be a power of two. */ // table就是哈希表，它实际是一个桶数组或者链表数组，官方注释表明哈希表容量必须满足2的幂(向上取值) transient Entry&lt;K,V&gt;[] table = (Entry&lt;K,V&gt;[]) EMPTY_TABLE; transient int size; // 当前HashMap以保存数据的个数 // HashMap扩容阈值，threshlod = capacity * loadFactor,如果size &gt;= threshlod则需要扩容，而并非size == table.length。 int threshold; /** * A randomizing value associated with this instance that is applied to * hash code of keys to make hash collisions harder to find. If 0 then * alternative hashing is disabled. */ // 哈希种子，哈希冲突更难被发现 transient int hashSeed = 0;} Java7采用单向链表来解决哈希冲突，以下是定义的私有内部类链表节点 1234567static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final K key; // 关键字 V value; // 值 Entry&lt;K,V&gt; next; // 后继 int hash; // 哈希值 ...} put方法 put方法是研究HashMap的精髓，掌握了该方法也就基本掌握HashMap的原理。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101public V put(K key, V value) { if (table == EMPTY_TABLE) { inflateTable(threshold); // 空表时扩容默认大小的哈希表 } if (key == null) return putForNullKey(value); // HashMap允许存在一个关键字为null的值（区别于HashTable） int hash = hash(key); // 哈希函数，求关键字对应的哈希值，如下方法 int i = indexFor(hash, table.length); // 计算落槽位置，如下方法 // 遍历槽位对应的链表，从第一个节点开始 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) { Object k; // 当哈希值相同并且key相同时则在链表中找到了对应数据，更改value值 if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++; // 如果链表没有找到数据则说明是新数据，将该数据添加到链表头部，如果容量达到阈值，则需要扩容，对旧表做迁移，需要rehash，如下方法。 addEntry(hash, key, value, i); return null;}// 关键的哈希函数，由复杂的移位和异或等位运算组成，为了保证发生哈希冲突时，数据可以均匀落槽// 当加载因子默认是0.75时，哈希冲突最多为8个。final int hash(Object k) { int h = hashSeed; if (0 != h &amp;&amp; k instanceof String) { return sun.misc.Hashing.stringHash32((String) k); } h ^= k.hashCode(); // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);}// 1.参数h是哈希函数计算的哈希值，length是哈希表的长度，将哈希值与length-1求与操作可以保证索引在length范围内// 2.在哈希表中每个元素的操作都离不开落槽，通常采用模运算，但模运算在计算机原理中是比较耗时的方式，所以HashMap要求容量必须是2次幂，这样依据二进制特点，length-1永远是多个1的排列，这样与运算就是与h的对位与，效率更高。static int indexFor(int h, int length) { // assert Integer.bitCount(length) == 1 : \"length must be a non-zero power of 2\"; return h &amp; (length-1);}void addEntry(int hash, K key, V value, int bucketIndex) { // 如果当前size达到阈值则扩容并数据迁移 if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) { resize(2 * table.length); // 扩容和rehash hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length); } createEntry(hash, key, value, bucketIndex);}// 新数据添加到链表头部void createEntry(int hash, K key, V value, int bucketIndex) { Entry&lt;K,V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); size++;}// 扩容新表容量是旧表的2倍（保证2次幂）void resize(int newCapacity) { Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return; } Entry[] newTable = new Entry[newCapacity]; transfer(newTable, initHashSeedAsNeeded(newCapacity)); // 数据迁移 table = newTable; threshold = (int) Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1);}// 数据迁移，需要对旧表每个数据重新计算哈希值void transfer(Entry[] newTable, boolean rehash) { int newCapacity = newTable.length; for (Entry&lt;K,V&gt; e : table) { while(null != e) { Entry&lt;K,V&gt; next = e.next; if (rehash) { e.hash = null == e.key ? 0 : hash(e.key); } int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; } }} 哈希函数 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;哈希函数对String类型做了特殊处理，实际上是调用了String类中的hash32()方法，计算获得String类型的32位哈希值，native底层使用murmur3_32()哈希方法而非String的hashCode()方法，原因是因为hashCode()方法将字符串的每个字符Unicode值累加获得，如果使用String作为key的话这样的处理方式会增加哈西冲突的概率。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;同时可以看到哈希函数入参是对象，计算哈希码过程中也使用到了对象的hashCode方法，也就是说，哈希函数依赖对象自身的哈希码，并以此额外计算了新的哈希码供HashMap使用，这样的好处是可以防止低质量的哈希函数。 123456789101112131415161718192021222324252627282930313233343536373839// HashMap 哈希函数final int hash(Object k) { int h = hashSeed; // 字符串哈希值 if (0 != h &amp;&amp; k instanceof String) { return sun.misc.Hashing.stringHash32((String) k); } h ^= k.hashCode(); // 对象自身哈希码 h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);}// String的hashCode方法和hash32方法// 每个字符Unicode码累加public int hashCode() { int h = hash; if (h == 0 &amp;&amp; value.length &gt; 0) { char val[] = value; for (int i = 0; i &lt; value.length; i++) { h = 31 * h + val[i]; } hash = h; } return h;}// 底层使用murmur3_32方法int hash32() { int h = hash32; if (0 == h) { // harmless data race on hash32 here. h = sun.misc.Hashing.murmur3_32(HASHING_SEED, value, 0, value.length); // ensure result is not zero to avoid recalcing h = (0 != h) ? h : 1; hash32 = h; } return h;} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在不同的编码规范中都要求软件工程师重写equals方法时要重写hashCode方法，从HashMap中就可以看出，当新元素落槽之后，链表从头结点开始比较的第一个条件就是对象的哈希值是否相等，且这一条件是短路的，即使key的值相同也会认为是不同的对象。 123456789101112131415public V put(K key, V value) { ... // 遍历槽位对应的链表，从第一个节点开始 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) { Object k; // 首先比较哈希码，且是短路条件，不满足立刻退出，即使key相等 if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } ...} 所以建议采用《Effective Java》中的第8、9条条例，小心的设计自己对象的equals和hashCode方法。假如，两个方法设计糟糕将会导致该对象作为key保存在HashMap中时，当对该对象执行get或put等操作，即使客观上两者是相同的对象，也会被区别对待，最终导致HashMap产生内存泄漏问题。 落槽方法indexFor &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在解决哈希冲突问题上，通常采用取余操作将数据均分到不同桶中，然而在计算机中取余操作相比于其他逻辑运算比较消耗性能，首先需要对数值转换十进制，然后不断的执行被除数和除数的除法操作最终获得余数，这在大范围使用上是不合适的，HashMap采用对2次幂减1并求异或是在二进制层面的位运算，效率很高更适合作为解决哈希冲突的手段，唯一约束就是要求容量必须保证是2次幂 123static int indexFor(int h, int length) { return h &amp; (length-1);} 假设，整型h=101，length=8，s=length-1=7 h对应二进制是 0000 0000 0000 0000 0000 0000 0110 0101‬ length对应二进制是 0000 0000 0000 0000 0000 0000 0001 0000 length-1对应的二进制是 0000 0000 0000 0000 0000 0000 0000 1111 按照异或的特性会将h的高位全部截断只保留与h对应的位数 0000 0000 0000 0000 0000 0000 0000 0101‬ 相当于求0101^1111，最终的哈希值是0101，即5","link":"/2020/05/07/4%20%E6%BA%90%E7%A0%81/Java%E6%BA%90%E7%A0%81%E2%80%94%E2%80%94HashMap/"},{"title":"Java源码——LinkedList","text":"LinkedList (version: JDK 11) 总结： LinkedList底层采用双向列表数据结构。 载JDK11中包含两个指针成员变量first和last分别指向链表的首位和末位，JDK7采用单指针header，末位对象指向header，也就是说header的后继next指向链表第一个节点，header的前驱previous指向链表最后一个节点。LinkedList实现前驱指针目的是为了实现单向队列和双端队列数据结构。源码以jdk11为例。 成员变量 1234transient int size;transient LinkedList.Node&lt;E&gt; first;transient LinkedList.Node&lt;E&gt; last;private static final long serialVersionUID = 876323262645176354L; 节点对象 123456789101112private static class Node&lt;E&gt; { E item; // 元素 LinkedList.Node&lt;E&gt; next; // 后继 LinkedList.Node&lt;E&gt; prev; // 前驱 Node(LinkedList.Node&lt;E&gt; prev, E element, LinkedList.Node&lt;E&gt; next) { this.item = element; this.next = next; this.prev = prev; }} add方法 123456789101112131415161718public boolean add(E e) { this.linkLast(e); return true;}void linkLast(E e) { LinkedList.Node&lt;E&gt; l = this.last; LinkedList.Node&lt;E&gt; newNode = new LinkedList.Node(l, e, (LinkedList.Node)null); this.last = newNode; if (l == null) { this.first = newNode; // 链表为空则指向对一个节点 } else { l.next = newNode; // 链表不为空则指向末尾节点 } ++this.size; ++this.modCount;} get方法 12345678910111213141516171819202122232425262728293031323334353637public E get(int index) { this.checkElementIndex(index); // 索引越界检查 return this.node(index).item; // 返回节点的元素}private void checkElementIndex(int index) { if (!this.isElementIndex(index)) { throw new IndexOutOfBoundsException(this.outOfBoundsMsg(index)); }}private boolean isElementIndex(int index) { return index &gt;= 0 &amp;&amp; index &lt; this.size;}// 因为指针遍历较慢，所以对链表从中间截取判断index落位上半位还是下半位LinkedList.Node&lt;E&gt; node(int index) { LinkedList.Node x; int i; if (index &lt; this.size &gt;&gt; 1) { // 上半位，从first节点查找 x = this.first; for(i = 0; i &lt; index; ++i) { x = x.next; } return x; } else { //下半位，从last节点查找 x = this.last; for(i = this.size - 1; i &gt; index; --i) { x = x.prev; } return x; }} 浅克隆 123456789101112131415161718192021// 浅克隆并重置modCount=0public Object clone() { LinkedList&lt;E&gt; clone = this.superClone(); clone.first = clone.last = null; clone.size = 0; clone.modCount = 0; for(LinkedList.Node x = this.first; x != null; x = x.next) { clone.add(x.item); } return clone;}private LinkedList&lt;E&gt; superClone() { try { return (LinkedList)super.clone(); } catch (CloneNotSupportedException var2) { throw new InternalError(var2); }} modCount 与ArrayList一样，当链表进行了增删改操作，modCount会自增，这是为了保证在并发访问和操作时保证链表数据统一，否则抛出ConcurrentModificationException异常，抛出异常的目的是快速反馈给调用者以发现并发问题并采取有效的措施。 12345final void checkForComodification() { if (LinkedList.this.modCount != this.expectedModCount) { throw new ConcurrentModificationException(); }} 队列实现方法 队列属于基础数据结构，采用FIFO先进先出原则，这就要求具有首位获得数据和末位添加数据的能力。 12345678910111213141516171819202122232425262728293031323334353637383940// 返回头结点元素，不删除，为空则返回nullpublic E peek() { LinkedList.Node&lt;E&gt; f = this.first; return f == null ? null : f.item;}// 返回头结点元素，不删除，为空则抛出异常public E element() { return this.getFirst();}public E getFirst() { LinkedList.Node&lt;E&gt; f = this.first; if (f == null) { throw new NoSuchElementException(); } else { return f.item; }}// 返回头结点元素，删除，为空则返回nullpublic E poll() { LinkedList.Node&lt;E&gt; f = this.first; return f == null ? null : this.unlinkFirst(f);}// 返回头结点元素，删除，为空则抛出异常public E remove() { return this.removeFirst();}// 末位添加元素public boolean offer(E e) { return this.add(e);}// 返回头结点元素，删除，为空则抛出异常public E pop() { return this.removeFirst();} 双端队列实现方法 双端队列是队列的变形，支持头结点和尾节点的添加元素和获得元素。 与链表类似使用的方法则不在额外说明。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// 头结点前驱添加元素public void push(E e) { this.addFirst(e);}public E pop() { return this.removeFirst();}public E peekFirst() { LinkedList.Node&lt;E&gt; f = this.first; return f == null ? null : f.item;}public E peekLast() { LinkedList.Node&lt;E&gt; l = this.last; return l == null ? null : l.item;}public E removeFirst() { LinkedList.Node&lt;E&gt; f = this.first; if (f == null) { throw new NoSuchElementException(); } else { return this.unlinkFirst(f); }}public E removeLast() { LinkedList.Node&lt;E&gt; l = this.last; if (l == null) { throw new NoSuchElementException(); } else { return this.unlinkLast(l); }}public E pollFirst() { LinkedList.Node&lt;E&gt; f = this.first; return f == null ? null : this.unlinkFirst(f);}public E pollLast() { LinkedList.Node&lt;E&gt; l = this.last; return l == null ? null : this.unlinkLast(l);}public boolean offerFirst(E e) { this.addFirst(e); return true;}public boolean offerLast(E e) { this.addLast(e); return true;}","link":"/2020/05/05/4%20%E6%BA%90%E7%A0%81/Java%E6%BA%90%E7%A0%81%E2%80%94%E2%80%94LinkedList/"},{"title":"Java源码——ArrayList","text":"ArrayList ArrayList和LinkedList &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ArrayList和LinkedList是List接口的重要实现类，也是开发中最常用到的线性表和链表数据结构。ArrayList底层采用数组结构，而LinkedList底层采用Entry节点和指针的链表结构，两者主要性能区别： 查询性能ArrayList更高 中间插入和删除数据LinkedList效率更高 总结： ArrayList底层使用数组结构，数据检索能力更高，默认初始容量10。 当size==length时触发扩容，每次扩容为1.5倍，扩容时旧数组和新数组并存，在GC之前占用两份空间，大数据量会不断扩容导致性能问题，一般建议评估保存数据的容量并赋初始值，如new ArrayList(10000)。 实现了RandomAccess接口，在使用Collections进行二分查找binarySearch时，会使用索引查找，效率更高。 非线程安全，并发修改和查找会影响modCount不匹配，导致快速抛出ConcurrentModificationException异常。 clone方法属于浅克隆shallow clone。 SubList是ArrayList在使用subList方法时的返回内部类，它同样继承自AbstracList但会共享当前ArrayList实例的数组，对SubList的修改其实是对原始数据的修改，一般建议，如果希望subList作为独立副本使用，使用Arrays.copyOf创建。 Arrays.asList方法返回的是Arrays的内部类ArrayList，区别于List中的ArrayList类，虽然同名但只提供了少量的功能，切不可作为后者进行数据处理，否则有NoSuchMethodException风险，一般建议，如果希望作为线性表List接口中的ArrayList，采用以下方式:List dupList = new Array(Arrays.asList(&quot;abc&quot;, &quot;def&quot;, &quot;ghi&quot;))。 成员变量 12345678// 默认容量private static final int DEFAULT_CAPACITY = 10; private static final Object[] EMPTY_ELEMENTDATA = new Object[0];private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = new Object[0];// 实际存储数组transient Object[] elementData;private int size; // size是已存数据容量，length是数组容量，一般size &lt;= lengthprivate static final int MAX_ARRAY_SIZE = 2147483639; add方法和扩容 123456789101112131415// 在数组末尾新增数据，当前数组size==length时触发扩容方法this.growprivate void add(E e, Object[] elementData, int s) { if (s == elementData.length) { elementData = this.grow(); } elementData[s] = e; this.size = s + 1;}public boolean add(E e) { ++this.modCount; this.add(e, this.elementData, this.size); return true;} 12345678910111213141516171819202122232425// Arrays.copyOf使用System.arrayCopy本地方法将原数组数据移植到新容量数组中private Object[] grow(int minCapacity) { return this.elementData = Arrays.copyOf(this.elementData, this.newCapacity(minCapacity));}private Object[] grow() { return this.grow(this.size + 1);}// 如果需要扩容，则会扩大为原来容量的1.5倍private int newCapacity(int minCapacity) { int oldCapacity = this.elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); // aka 1.5倍 if (newCapacity - minCapacity &lt;= 0) { if (this.elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { return Math.max(10, minCapacity); } else if (minCapacity &lt; 0) { throw new OutOfMemoryError(); } else { return minCapacity; } } else { return newCapacity - 2147483639 &lt;= 0 ? newCapacity : hugeCapacity(minCapacity); }} 浅克隆 1234567891011// 浅克隆并重置modCount=0public Object clone() { try { ArrayList&lt;?&gt; v = (ArrayList)super.clone(); v.elementData = Arrays.copyOf(this.elementData, this.size); v.modCount = 0; return v; } catch (CloneNotSupportedException var2) { throw new InternalError(var2); }} 实现RandomAccess接口 123// List接口只有ArrayList实现了RandomAccess声明式接口// 其目的是在使用Collections.binarySearch二分查找方法时采用索引遍历而非迭代器遍历public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, Serializable {...} 12345// Collections方法类// List实现了RandomAccess直接使用indexBinarySearch方法，该方法执行效率高于iteratorBinarySearch方法。public static &lt;T&gt; int binarySearch(List&lt;? extends Comparable&lt;? super T&gt;&gt; list, T key) { return !(list instanceof RandomAccess) &amp;&amp; list.size() &gt;= 5000 ? iteratorBinarySearch(list, key) : indexedBinarySearch(list, key); } modCount 当数组进行了增删改操作，modCount会自增，这是为了保证在并发访问和操作时保证线性表数据统一，否则会抛出ConcurrentModificationException异常，抛出异常的目的是快速反馈给调用者以发现并发问题并采取有效的措施。 123456789101112131415161718192021222324252627282930313233343536public boolean add(E e) { ++this.modCount; this.add(e, this.elementData, this.size); return true;}public void clear() { ++this.modCount; Object[] es = this.elementData; int to = this.size; for(int i = this.size = 0; i &lt; to; ++i) { es[i] = null; }}private void fastRemove(Object[] es, int i) { ++this.modCount; int newSize; if ((newSize = this.size - 1) &gt; i) { System.arraycopy(es, i + 1, es, i, newSize - i); } es[this.size = newSize] = null;}public void sort(Comparator&lt;? super E&gt; c) { int expectedModCount = this.modCount; Arrays.sort(this.elementData, 0, this.size, c); // 如果不相等则抛出异常 if (this.modCount != expectedModCount) { throw new ConcurrentModificationException(); } else { ++this.modCount; }} subList方法和SubList内部类 12345// subList方法返回ArrayList内部类SubListpublic List&lt;E&gt; subList(int fromIndex, int toIndex) { subListRangeCheck(fromIndex, toIndex, this.size); return new ArrayList.SubList(this, fromIndex, toIndex);} 123456789101112131415161718// 同样实现AbstractList抽象类与父类ArrayList相同private static class SubList&lt;E&gt; extends AbstractList&lt;E&gt; implements RandomAccess { private final ArrayList&lt;E&gt; root; private final ArrayList.SubList&lt;E&gt; parent; private final int offset; private int size; ... // 没有trimToSize()方法 // 共享修改父类 public E set(int index, E element) { Objects.checkIndex(index, this.size); this.checkForComodification(); E oldValue = this.root.elementData(this.offset + index); this.root.elementData[this.offset + index] = element; return oldValue; } ...} 12345678910111213141516// 当采用如下用法将抛出NoSuchMethodException异常List mainList = new ArrayList(Arrays.asList(1,2,3,4,5));List subList = mainList.subList(0,2);subList.trimToSize(); // NoSuchMethodException// 当想当然的以为对subList修改时，mainList也被修改subList.set(0, 10);print(mainList); // 10,2,3,4,5print(subList); // 10,2// 可以采取如下措施避免subList问题List dupSubList = new ArrayList(subList);dupSubList.set(0, 10);System.out.println(mainList); // 1,2,3,4,5System.out.println(subList); // 1,2System.out.println(dupSubList); // 10,2 Arrays.asList和Arrays内部类ArraysList 12345public class Arrays {... // 作为内部类虽然与ArrayList同名，但只有部分方法，容易产生NoSuchMethodException异常 private static class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements RandomAccess, Serializable {...} } 123456List list = Arrays.asList(1,2,3,4,5)list.trimToSize(); // NoSuchMethodException// 采取如下措施转换为ArraysListList _list = new Array(list);_list.trimToSize();","link":"/2020/05/01/4%20%E6%BA%90%E7%A0%81/Java%E6%BA%90%E7%A0%81%E2%80%94%E2%80%94ArrayList/"},{"title":"常用算法——贪心算法","text":"贪心算法 每一次站在十字路口，我们总是选择看起来最安全的大路。 概念&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;贪心算法，是指将一个问题拆解成多个子问题，而在解决子问题时，我们总是选择最优方案，也就是说，并不从整体考虑最优解，最终可能是接近最优解的解法。 算法框架 一个复杂问题可以拆分位多个最优子结构。 对子问题进行最优解法。 所有子问题的最优解可能是最终最优解的趋近解法。 算法举例 输入一组整数N的序列A， 满足0&lt;N&lt;10，A的个数为6，每组数据以,分隔 求：整数组合构成的最大hh:mm:ss时间组合，如果不满足则输出invalid。 例1 输入：[3, 2, 4, 9, 5, 5] 输出：23:59:54 例2 输入：[9, 9, 9, 9, 9, 9] 输出：invalid 思路：最大时间组合，从时分秒依次求得，按照贪心算法思想，每一个求得的最大值，最终组合的必然是最大值。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public static void main(String[] args) { Scanner sc = new Scanner(System.in); String input = sc.nextLine(); String[] array = input.substring(1, input.length() - 1).split(\",\"); List&lt;String&gt; inputList = new ArrayList&lt;&gt;(Arrays.asList(array)); // hour List&lt;String&gt; list1 = recombine(inputList); String hour; int intHour = list1.stream() .filter(x -&gt; !x.startsWith(\"0\") &amp;&amp; Integer.valueOf(x) &lt; 23) .map(Integer::valueOf).max(Integer::compare).orElse(-1); if (intHour == -1) { System.out.println(\"invalid\"); return; } hour = intHour + \"\"; inputList.remove(getIndex(hour.substring(0, 1), inputList)); inputList.remove(getIndex(hour.substring(1), inputList)); // min List&lt;String&gt; list2 = recombine(inputList); String min; int intMin = list2.stream().filter(x -&gt; !x.startsWith(\"0\") &amp;&amp; Integer.valueOf(x) &lt; 60) .map(Integer::valueOf).max(Integer::compare).orElse(-1); if (intMin == -1) { System.out.println(\"invalid\"); return; } min = intMin + \"\"; inputList.remove(getIndex(min.substring(0, 1), inputList)); inputList.remove(getIndex(min.substring(1), inputList)); // second List&lt;String&gt; list3 = recombine(inputList); String second; int intSecond = list3.stream().filter(x -&gt; !x.startsWith(\"0\") &amp;&amp; Integer.valueOf(x) &lt; 60) .map(Integer::valueOf).max(Integer::compare).orElse(-1); if (intSecond == -1) { System.out.println(\"invalid\"); return; } second = intSecond + \"\"; System.out.println(String.join(\":\", hour, min, second)); } private static List&lt;String&gt; recombine(List&lt;String&gt; list) { List&lt;String&gt; newList = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; list.size(); i++) { for (int j = 0; j &lt; list.size(); j++) { if (i != j) { newList.add(list.get(i) + list.get(j)); } } } return newList; } private static int getIndex (String target, List&lt;String&gt; list) { int ret = -1; for (int index = 0; index &lt; list.size(); index++) { if (list.get(index).equals(target)) { ret = index; break; } } return ret; }","link":"/2020/04/03/3%20%E7%AE%97%E6%B3%95%E5%92%8C%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/"},{"title":"常用算法——动态规划法","text":"动态规划法 概念&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;动态规划法，是指将一个问题拆解成多个子问题，若干子问题的解是其他子问题的条件，而这些子问题构成了局部最优解，并最终得到整体问题的解。它与分治法最大的区别在于具有公共子问题，即它不是独立的。常用递推方式解决。 有最优子结构特征。 无后效性，子问题的状态一旦确定，不受其后操作的影响。 重叠子问题：分治法讲求无公共子问题，而动态规划法要求，每个子问题并不是独立的，前一子问题状态会应用于后一子问题。 算法框架 定义状态方程F(i,j)=A(i,j) 定义状态转移方程F(i,j)=max(F(i-1,j),F(i-1,B(j))) | F(i-1,j) 构造关于i和j的动态规划表并填表。 算法举例 01背包问题： 描述：在N件物品取出若干件放在容量为W的背包里，每件物品的体积为W1，W2……Wn（Wi为整数），每件物品都有其价值P1,P2……Pn（Pi为整数）。求如何让背包装入最大价值的物品。 思路： 拆分：将该问题拆分，记录存入物品编号与背包容量的矩阵。 状态转移：假设当前的商品编号为i，背包容量为j，商品价值为P(i)，重量为W(i)，那么当前背包容量对应放入的商品最大价值为V(i,j) (1) 当前商品重量&gt;背包重量无法放入，那么最大价值等于该容量下的上一个商品的最大价值，即W(i) &gt; j, 则V(i,j) = V(i-1,j) (2) 当前商品重量&gt;背包重量可以放入，那么需要衡量如果将该商品放入背包是否能产生最大价值，如果不满足也不放。即W(i) &lt;= j, 则 V(i,j) = max(V(i-1, j - W(i)) + P(i), V(i-1, j)) 求V(i,j)中的最大值即背包容纳的最大价值。 12345678910111213141516171819202122232425262728293031public class Knapsack { static int maxValueOfKnapsack(int[] weights, int[] values, int capacity) { int[][] dpTable = new int[values.length][capacity + 1]; int maxValue = 0; // DP问题都是从(1,1)开始 for (int i = 1; i &lt; values.length; i++) { for (int j = 1; j &lt;= capacity; j++) { if (weights[i] &gt; j) { // 放不下 dpTable[i][j] = dpTable[i-1][j]; } else { // 衡量价值 // dpTable[i-1][j-weights[i]] + values[i]表示放入商品之后对应的价值 dpTable[i][j] = Math.max(dpTable[i-1][j-weights[i]] + values[i], dpTable[i-1][j]); } if (maxValue &lt; dpTable[i][j]) { maxValue = dpTable[i][j]; } } } return maxValue; } public static void main(String[] args) { int[] weights = {0, 8, 10, 6, 3, 7, 2}; int[] values = {0, 4, 6, 2, 2, 5, 1}; int capacity = 16; System.out.println(maxValueOfKnapsack(weights, values, capacity)); }}","link":"/2020/04/07/3%20%E7%AE%97%E6%B3%95%E5%92%8C%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"title":"常用算法——分治法","text":"分治法 分而治之，治而合之 说明：文中动态插图援引微信公众号：《五分钟学算法》 概念​ 分治法（Divide and Conquer, D&amp;C），将一个复杂问题纵向拆分为多个最优结构原子问题，再将原子问题的求解合并，组成复杂问题的最终解。递归是分治法的常用手段，也是经典手段。 使用条件 复杂问题拆分后就容易求解。 拆分后的原子问题具有最优子结构。 原子问题的解可以合并成复杂问题的解。 原子问题独立存在，不含公共子问题。 算法框架 分解：纵向拆分复杂问题为原子问题。 解决：对原子问题进行求解。 合并：将原子问题解合并，组成最终解。 算法复杂度​ 一个规模为n的实例可以划分为k个规模为n/k的实例，其中α个实例是需要求解的： O(N) = αT(n/k) + f(n) ​ f(n)与具体的拆分和合并算法有关。 经典算法归并排序（Merge Sort）： 拆分：将一个无序序列递归拆分，最终拆分为长度为1的子序列。 解决：一次将子序列两两返回并进行排序。 合并：自下而上的对子序列进行合并，最终组成完整的排序后序列。 自上而下拆分， 自下而上合并 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class MergeSorted implements IArraySort { @Override public int[] sort(int[] srcArray) { int[] arr = Arrays.copyOf(srcArray, srcArray.length); // 操作副本排序，不影响原数组 if (arr.length &lt; 2) { return arr; } int middle = srcArray.length / 2; int[] leftArr = Arrays.copyOfRange(arr, 0, middle); int[] rightArr = Arrays.copyOfRange(arr, middle, arr.length); return merge(sort(leftArr), sort(rightArr)); } private int[] merge(int[] leftArr, int[] rightArr) { int[] result = new int[leftArr.length + rightArr.length]; int index = 0; while (leftArr.length &gt; 0 &amp;&amp; rightArr.length &gt; 0) { if (leftArr[0] &lt;= rightArr[0]) { result[index] = leftArr[0]; index++; leftArr = Arrays.copyOfRange(leftArr, 1, leftArr.length); } else { result[index] = rightArr[0]; index++; rightArr = Arrays.copyOfRange(rightArr, 1, rightArr.length); } } while (leftArr.length &gt; 0) { result[index] = leftArr[0]; index ++; leftArr = Arrays.copyOfRange(leftArr, 1, leftArr.length); } while (rightArr.length &gt; 0) { result[index] = rightArr[0]; index ++; rightArr = Arrays.copyOfRange(rightArr, 1, rightArr.length); } return result; } public static void main(String[] args) { int[] arr = {3,23,4,13,4,5,63,6,2}; arr = new MergeSorted().sort(arr); System.out.println(arr.length); for (int i : arr) { System.out.print(i + &quot; &quot;); } }} 快速排序（Quick Sort） 解决：设置基准点pivot，并定位两端索引位置left和right，一边排序找到比pivot小的放在左边，比pivot大的放在右边。 拆分：一边排序后，以pivot为分割点，将左右两边序列依次执行步骤1。 合并：将原子序列排序后合并成为最终有序序列。 自上而下排序并拆分 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class QuikSort implements IArraySort { @Override public int[] sort(int[] sourceArray) { // 对 arr 进行拷贝，不改变参数内容 int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); quickSort(arr, 0, arr.length - 1); return arr; } private void quickSort(int[] arr, int left, int right) { if (left &gt; right) { return; } int leftIndex = left; int rightIndex = right; int pivot = arr[leftIndex]; // 基准 while (left &lt; right) { while (left &lt; right &amp;&amp; arr[right] &gt;= pivot) { right--; } while (left &lt; right &amp;&amp; arr[left] &lt;= pivot) { left++; } if (left &lt; right) { swap(arr, right, left); } } swap(arr, leftIndex, left); // 此时left==right quickSort(arr, leftIndex, left-1); quickSort(arr,left + 1, rightIndex); } private void swap(int[] arr, int i, int j) { int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; } public static void main(String[] args) { int[] arr = {6,1,2,7,9,3,4,5,10,8}; arr = new QuikSort().sort(arr); System.out.println(arr.length); for (int i : arr) { System.out.print(i + &quot; &quot;); } }} 注意：如果基准点pivot设置为左顶端数，那么应该保证先从右侧开始比较，这样可以保证最终left和right停留的相同位置是小于pivot的，所以可以交换，否则，交换的将是大于pivot的值，不满足快排条件。 汉诺塔 A、B、C三根柱子，其中A柱子上面有从小叠到大的n个圆盘，现要求将A柱子上的圆盘移到C柱子上去，期间只有一个原则：一次只能移到一个盘子且大盘子不能在小盘子上面，求移动的步骤和移动的次数 把n-1个盘子由A 移到 B；把第n个盘子由 A移到 C；把n-1个盘子由B 移到 C； 1234567891011121314151617181920212223242526272829public class Hanoi { private static int step = 1; // 步数 public static void hanoi(int plateNum, String a, String b, String c) { if (plateNum == 1) { move(plateNum, a, c); } else { hanoi(plateNum - 1, a, c, b); move(plateNum, a, c); hanoi(plateNum - 1, b, a, c); } } private static void move(int plateNum, String from, String to) { System.out.printf(&quot;%d step, %d plate move from %s to %s \\n&quot;, step++, plateNum, from, to); } public static void main(String[] args) { hanoi(3, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;); }}1 step, 1 plate move from A to C 2 step, 2 plate move from A to B 3 step, 1 plate move from C to B 4 step, 3 plate move from A to C 5 step, 1 plate move from B to A 6 step, 2 plate move from B to C 7 step, 1 plate move from A to C","link":"/2020/04/01/3%20%E7%AE%97%E6%B3%95%E5%92%8C%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E5%88%86%E6%B2%BB%E6%B3%95/"},{"title":"Java源码——TreeMap和红黑树","text":"TreeMap和红黑树 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;红黑树与AVL树类似，都可以在插入和删除时通过旋转来保持自身树的平衡，从而获得较高的查找性能。与AVL树相比，红黑树并不是严格的平衡树，只要保证从根节点出发到叶子节点的最长路径不超过最短路径的2倍，最坏情况算法复杂度依然可以保证O（logn）。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;红黑树的每个节点都会着色，要么是黑色要么是红色，通过重新着色和左右旋转完成自身平衡的调整，它需要满足以下4个要求： 节点只能是红色或者黑色。 根节点和NIL节点必须是黑色。 一条路径不能存在连续的两个红色节点。 任何树内，根节点到叶子节点的路径上包含相同黑色节点的个数。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NIL是每个叶子节点有两个NIL子节点，NIL节点物理上并不存在，只存在与逻辑空间，主要是为了满足红黑树自旋稳定性。红黑树的旋转在3次之内可以达到平衡。 TreeMap&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TreeMap适用于对key有排序要求的场景中，TreeMap使用红黑树作为底层数据结构，TreeMap继承自AbstractMap并实现了NavigableMap接口，该接口要求提供排序算法。作为TreeMap的key必须具有比较能力Comparable或者自定义实现比较器Comparator以支持排序规定，所以key不允许为null。 12public class TreeMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements NavigableMap&lt;K,V&gt;, Cloneable,Serializable &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TreeMap优先使用比较器Comparator，如果比较器不存在则使用key自然排序Comparable，如果两者都不存在则会抛出异常ClassCastException 成员变量和构造器 123456789101112131415161718192021222324252627282930313233343536373839404142// 全局比较器private final Comparator&lt;? super K&gt; comparator;// 根节点private transient Entry&lt;K,V&gt; root;private transient int size = 0;private transient int modCount = 0;// boolean类型表示红黑两色private static final boolean RED = false;private static final boolean BLACK = true;// 内部类，红黑树节点，其中color是节点的颜色static final class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { K key; V value; Entry&lt;K,V&gt; left; Entry&lt;K,V&gt; right; Entry&lt;K,V&gt; parent; boolean color = BLACK;｝// 默认构造器，比较器为nullpublic TreeMap() { comparator = null;}// 带自定义比较器的构造器public TreeMap(Comparator&lt;? super K&gt; comparator) { this.comparator = comparator; }// 使用有序Map的比较器，并将数据转移public TreeMap(SortedMap&lt;K, ? extends V&gt; m) { comparator = m.comparator(); try { buildFromSorted(m.size(), m.entrySet().iterator(), null, null); } catch (java.io.IOException cannotHappen) { } catch (ClassNotFoundException cannotHappen) { } } 结构调整 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;红黑树的结构变化发生在数据的插入和删除，一旦发生变化红黑树的平衡就有可能被破坏，这时就需要旋转重新达到平衡。需要考虑以下三种条件： 被调整的节点总是红色节点。 如果新增节点的父节点是黑色的就无需改变，因为可以保证红黑树的约束。 如果新节点的父节点是红色，需要进行重新着色、左右旋转最终达到约束条件重新保持红黑树的平衡。 插入数据 ​ TreeMap的插入就是按照key的比较进行遍历，按照二分查找的特点大于当前节点向右遍历，小于当前节点向左遍历，当确定节点之后再考虑着色和旋转，保证红黑树的约束。 put()方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public V put(K key, V value) { // t表示当前节点 Entry&lt;K,V&gt; t = root; // 如果当前是空树，则新插入数据设为根 if (t == null) { compare(key, key); // type (and possibly null) check root = new Entry&lt;&gt;(key, value, null); size = 1; modCount++; return null; } // 接收比较结果 int cmp; Entry&lt;K,V&gt; parent; Comparator&lt;? super K&gt; cpr = comparator; // 比较方式分支 if (cpr != null) { // 循环目标：入参key与当前节点key不断比较 do { parent = t; // 比较当前key和入参key cmp = cpr.compare(key, t.key); if (cmp &lt; 0) t = t.left; //小就置为左节点 else if (cmp &gt; 0) t = t.right; //大置为右节点 else return t.setValue(value); //相等覆盖 // 如果没有相等节点，则会进入NIL节点 } while (t != null); } else { // 使用Comparable不允许key为空 if (key == null) throw new NullPointerException(); @SuppressWarnings(\"unchecked\") Comparable&lt;? super K&gt; k = (Comparable&lt;? super K&gt;) key; do { parent = t; cmp = k.compareTo(t.key); if (cmp &lt; 0) t = t.left; else if (cmp &gt; 0) t = t.right; else return t.setValue(value); } while (t != null); } // 新节点创建并根据比较结果置于父节点的左或右节点 Entry&lt;K,V&gt; e = new Entry&lt;&gt;(key, value, parent); if (cmp &lt; 0) parent.left = e; else parent.right = e; // 对新节点着色和旋转达到平衡 fixAfterInsertion(e); size++; modCount++; return null;}// 选择比较方法final int compare(Object k1, Object k2) { return comparator==null ? ((Comparable&lt;? super K&gt;)k1).compareTo((K)k2) : comparator.compare((K)k1, (K)k2);} 由于TreeMap通过比较来判断key的唯一性，所以equals和hashCode方法不是必须覆写的。 fixAfterInsertion()方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879private void fixAfterInsertion(Entry&lt;K,V&gt; x) { // 新节点着色为红色，满足约束条件1 x.color = RED; // 遍历使树达到平衡的条件 while (x != null &amp;&amp; x != root &amp;&amp; x.parent.color == RED) { // 如果父节点是祖父节点的左子节点 if (parentOf(x) == leftOf(parentOf(parentOf(x)))) { // 查看父节点的兄弟节点（右叔）颜色 Entry&lt;K,V&gt; y = rightOf(parentOf(parentOf(x))); if (colorOf(y) == RED) { // 如果右叔节点是红色 setColor(parentOf(x), BLACK); // 父节点着黑色 setColor(y, BLACK); // 右叔节点着黑色 setColor(parentOf(parentOf(x)), RED); // 祖父节点着红色 x = parentOf(parentOf(x));// 当前节点指向红色的祖父节点 } else { // 如果右叔节点是黑色 if (x == rightOf(parentOf(x))) { x = parentOf(x); // 当前节点指向红色的父节点 rotateLeft(x); // 左旋调整平衡 } setColor(parentOf(x), BLACK); // 父节点着黑色 setColor(parentOf(parentOf(x)), RED); // 祖父节点着红色 rotateRight(parentOf(parentOf(x))); // 左旋红色的祖父节点 } } else {// 如果父节点是祖父节点的右子节点，过程与上述类似 Entry&lt;K,V&gt; y = leftOf(parentOf(parentOf(x))); if (colorOf(y) == RED) { setColor(parentOf(x), BLACK); setColor(y, BLACK); setColor(parentOf(parentOf(x)), RED); x = parentOf(parentOf(x)); } else { if (x == leftOf(parentOf(x))) { x = parentOf(x); rotateRight(x); } setColor(parentOf(x), BLACK); setColor(parentOf(parentOf(x)), RED); rotateLeft(parentOf(parentOf(x))); } } } root.color = BLACK;}// 左旋private void rotateLeft(Entry&lt;K,V&gt; p) { if (p != null) { Entry&lt;K,V&gt; r = p.right; p.right = r.left; if (r.left != null) r.left.parent = p; r.parent = p.parent; if (p.parent == null) root = r; else if (p.parent.left == p) p.parent.left = r; else p.parent.right = r; r.left = p; p.parent = r; }}// 右旋private void rotateRight(Entry&lt;K,V&gt; p) { if (p != null) { Entry&lt;K,V&gt; l = p.left; p.left = l.right; if (l.right != null) l.right.parent = p; l.parent = p.parent; if (p.parent == null) root = l; else if (p.parent.right == p) p.parent.right = l; else p.parent.left = l; l.right = p; p.parent = l; }} 使用左旋或者右旋的条件： 父节点是红色，叔叔节点是红色，则重新着色。 父节点时红色，叔叔节点是黑色，如果新节点是父节点的左节点，右旋。 父节点时红色，叔叔节点是黑色，如果新节点是父节点的右节点，左旋。 插入举例 123456789TreeMap&lt;Integer, String&gt; treeMap = new TreeMap&lt;&gt;();treeMap.put(13,\"\");treeMap.put(14,\"\");treeMap.put(15,\"\");treeMap.put(16,\"\");treeMap.put(42,\"\");treeMap.remove(15);treeMap.put(20,\"\");// 从空树开始演示红黑树插入，调整平衡的过程。 插入13、14、15 插入16 插入42 删除15，插入20 AVL树和红黑树 时间复杂度：对于任意高度的节点，它的黑深度满足≥ height / 2，即对于任意包含n个节点的红黑树，它的根节点高度h≤2log2 (n+1)，当树失去平衡，时间复杂度有可能变为O(n)，即h=n。所以，可以保证树的高度始终保持在O(logn)时，所有操作的时间复杂度保持在O(logn)以内。 平衡性：AVL是严格的平衡二叉查找树，任意子树的高度差始终在1以内，红黑树平衡没有如此严格，所以当节点个数一致，红黑树的高度可能大于AVL树，换句话说，平均查找次数会高于相同情况下的AVL树。插入时，两者都可以保证最多两次旋转就可以使树恢复平衡；删除时，由于红黑树对高度差的不严格，最多三次就可以恢复平衡，而AVL可能需要更多的旋转。因此，频繁的插入和删除红黑树更合适；低频修改，高频查询AVL树更合适","link":"/2020/05/15/4%20%E6%BA%90%E7%A0%81/Java%E6%BA%90%E7%A0%81%E2%80%94%E2%80%94TreeMap%E5%92%8C%E7%BA%A2%E9%BB%91%E6%A0%91/"},{"title":"Java源码——HashMap","text":"HashMap（一） &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;HashMap应该是Java世界应用最多的数据结构对象，HashMap从Java1.2开始存在，在Java8经过一次大改，Java5又出现了同步包java.util.concurrent其中就包含高性能的同步HashMap对象ConcurrentHashMap，这其中精彩世界以下揭晓。 0. 哈希表&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;哈希表，又称散列表，是计算机从业者最熟悉不过的数据结构，由于其在数据索引时只拥有O(n)时间复杂度而广泛应用。生活中我们也经常遇到类似情景，比如查字典过程、电话本黄页等等，通过索引关键字获得对应映射的值。 上图为哈希表的简易示意图，哈希表的基本组成包括： 桶，是保存一组数据的空间，通常是一个链表（单向链表）或者一棵树（红黑树）； 槽位，是通过关键字定位数据桶的索引位置，通过关键字定位到槽位我们一般称为落槽。 哈希冲突，理想状态下每个槽位对应的桶只保存一个数据，那么就形成了槽位和数据一一对应关系（例如图中0号槽位和k号槽位），然而由于哈希表中的槽位通常是有限的，当数据量大于槽位个数时势必会产生一个槽位对应的桶中保存了多个数据，这就称为哈希冲突。解决哈希冲突的方式一般采用单向链表或者红黑树。 哈希函数，将数据元素的关键字key作为自变量，通过一定的函数关系计算出的值，即为该元素的存储地址，这个函数称为哈希函数。哈希函数需要解决的两大问题分别是均匀落槽和哈希冲突。前者在极端情况下会产生所有数据落槽在同一个桶中，产生空间资源浪费和性能低效。 加载因子（load factor），理想状态下，如果空间无限大则槽位可以保证只映射一个数据那么它的时间复杂度是完美的O(1)，如果空间有限，为了保证数据查询时间，可以将数据均分到每个桶中形成较短链路的链表，设计合理的哈希表可以平衡空间与时间。空间和时间永远是计算机领域的哲学问题，加载因子就是平衡这一问题的系数(0~1的浮点数)，研究表明，加载因子系数值越大空间损耗越小时间损耗越大，相反如果系数值越小空间损耗越大时间损耗越小，在Java中默认为0.75。 再哈希（rehash），当一个哈希表向槽位更多的哈希表迁移时，由于槽位个数发生变化，这就要求原哈希表中的每个元素重新计算哈希值并落槽到新哈希表中，这一过程称为rehash，是哈希表的重要性能指标之一。 概念虽然较多，但都比较容易理解且重要 举个例子： 当我们需要从手机通讯录中查找某一个人的联系电话，这个过程可以拟化为一次哈希表的查找。 首先，有张三、李三、王三三个人的联系方式需要存入通讯录，我们假定他们的名字就是关键字，通过哈希函数计算关键字获得落槽位置分别是拼音首字母z、l和w，此时这个哈希表槽位数共有三个，每个槽位对应的桶分别是张三、李三、王三的电话号码，当我们需要拨打张三电话时直接找到z对应的电话拨出即可。 然而，又来了三个人需要保存电话，他们分别是张四、李四、王四，通过哈希函数计算名字关键字得到槽位还是z、l和w，这就是哈希冲突，三个槽位对应的桶分别保存了两个电话号码，当我们拨打张四电话时，需要先找到z，在从z的列表中找到张四电话并拨出。 最后，又来了两个人赵三和刘三，很明显目前的通讯录无法保存他们的电话，这时我们需要对通讯录扩容并将现在的六个人电话迁移过去，但是迁移时需要满足一个约束，原来的六个人必须都重新通过哈希函数计算落槽位置，这就是rehash。 1. JAVA 7的实现成员变量 HashMap的实现实际是一个桶数组或者称为链表数组，数组下标索引即槽位，同时定义了默认的加载因子、初始容量、最大范围和扩容条件。 12345678910111213141516171819202122232425262728public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable{ static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // HashMap的默认初始大小16 static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; // HashMap允许的最大范围2^30 static final float DEFAULT_LOAD_FACTOR = 0.75f; // 默认加载因子 /** * The table, resized as necessary. Length MUST Always be a power of two. */ // table就是哈希表，它实际是一个桶数组或者链表数组，官方注释表明哈希表容量必须满足2的幂(向上取值) transient Entry&lt;K,V&gt;[] table = (Entry&lt;K,V&gt;[]) EMPTY_TABLE; transient int size; // 当前HashMap以保存数据的个数 // HashMap扩容阈值，threshlod = capacity * loadFactor,如果size &gt;= threshlod则需要扩容，而并非size == table.length。 int threshold; /** * A randomizing value associated with this instance that is applied to * hash code of keys to make hash collisions harder to find. If 0 then * alternative hashing is disabled. */ // 哈希种子，哈希冲突更难被发现 transient int hashSeed = 0;} Java7采用单向链表来解决哈希冲突，以下是定义的私有内部类链表节点 1234567static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final K key; // 关键字 V value; // 值 Entry&lt;K,V&gt; next; // 后继 int hash; // 哈希值 ...} put方法 put方法是研究HashMap的精髓，掌握了该方法也就基本掌握HashMap的原理。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101public V put(K key, V value) { if (table == EMPTY_TABLE) { inflateTable(threshold); // 空表时扩容默认大小的哈希表 } if (key == null) return putForNullKey(value); // HashMap允许存在一个关键字为null的值（区别于HashTable） int hash = hash(key); // 哈希函数，求关键字对应的哈希值，如下方法 int i = indexFor(hash, table.length); // 计算落槽位置，如下方法 // 遍历槽位对应的链表，从第一个节点开始 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) { Object k; // 当哈希值相同并且key相同时则在链表中找到了对应数据，更改value值 if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++; // 如果链表没有找到数据则说明是新数据，将该数据添加到链表头部，如果容量达到阈值，则需要扩容，对旧表做迁移，需要rehash，如下方法。 addEntry(hash, key, value, i); return null;}// 关键的哈希函数，由复杂的移位和异或等位运算组成，为了保证发生哈希冲突时，数据可以均匀落槽// 当加载因子默认是0.75时，哈希冲突最多为8个。final int hash(Object k) { int h = hashSeed; if (0 != h &amp;&amp; k instanceof String) { return sun.misc.Hashing.stringHash32((String) k); } h ^= k.hashCode(); // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);}// 1.参数h是哈希函数计算的哈希值，length是哈希表的长度，将哈希值与length-1求与操作可以保证索引在length范围内// 2.在哈希表中每个元素的操作都离不开落槽，通常采用模运算，但模运算在计算机原理中是比较耗时的方式，所以HashMap要求容量必须是2次幂，这样依据二进制特点，length-1永远是多个1的排列，这样与运算就是与h的对位与，效率更高。static int indexFor(int h, int length) { // assert Integer.bitCount(length) == 1 : \"length must be a non-zero power of 2\"; return h &amp; (length-1);}void addEntry(int hash, K key, V value, int bucketIndex) { // 如果当前size达到阈值则扩容并数据迁移 if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) { resize(2 * table.length); // 扩容和rehash hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length); } createEntry(hash, key, value, bucketIndex);}// 新数据添加到链表头部void createEntry(int hash, K key, V value, int bucketIndex) { Entry&lt;K,V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); size++;}// 扩容新表容量是旧表的2倍（保证2次幂）void resize(int newCapacity) { Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return; } Entry[] newTable = new Entry[newCapacity]; transfer(newTable, initHashSeedAsNeeded(newCapacity)); // 数据迁移 table = newTable; threshold = (int) Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1);}// 数据迁移，需要对旧表每个数据重新计算哈希值void transfer(Entry[] newTable, boolean rehash) { int newCapacity = newTable.length; for (Entry&lt;K,V&gt; e : table) { while(null != e) { Entry&lt;K,V&gt; next = e.next; if (rehash) { e.hash = null == e.key ? 0 : hash(e.key); } int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; } }} 哈希函数 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;哈希函数对String类型做了特殊处理，实际上是调用了String类中的hash32()方法，计算获得String类型的32位哈希值，native底层使用murmur3_32()哈希方法而非String的hashCode()方法，原因是因为hashCode()方法将字符串的每个字符Unicode值累加获得，如果使用String作为key的话这样的处理方式会增加哈西冲突的概率。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;同时可以看到哈希函数入参是对象，计算哈希码过程中也使用到了对象的hashCode方法，也就是说，哈希函数依赖对象自身的哈希码，并以此额外计算了新的哈希码供HashMap使用，这样的好处是可以防止低质量的哈希函数。 123456789101112131415161718192021222324252627282930313233343536373839// HashMap 哈希函数final int hash(Object k) { int h = hashSeed; // 字符串哈希值 if (0 != h &amp;&amp; k instanceof String) { return sun.misc.Hashing.stringHash32((String) k); } h ^= k.hashCode(); // 对象自身哈希码 h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);}// String的hashCode方法和hash32方法// 每个字符Unicode码累加public int hashCode() { int h = hash; if (h == 0 &amp;&amp; value.length &gt; 0) { char val[] = value; for (int i = 0; i &lt; value.length; i++) { h = 31 * h + val[i]; } hash = h; } return h;}// 底层使用murmur3_32方法int hash32() { int h = hash32; if (0 == h) { // harmless data race on hash32 here. h = sun.misc.Hashing.murmur3_32(HASHING_SEED, value, 0, value.length); // ensure result is not zero to avoid recalcing h = (0 != h) ? h : 1; hash32 = h; } return h;} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在不同的编码规范中都要求软件工程师重写equals方法时要重写hashCode方法，从HashMap中就可以看出，当新元素落槽之后，链表从头结点开始比较的第一个条件就是对象的哈希值是否相等，且这一条件是短路的，即使key的值相同也会认为是不同的对象。 123456789101112131415public V put(K key, V value) { ... // 遍历槽位对应的链表，从第一个节点开始 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) { Object k; // 首先比较哈希码，且是短路条件，不满足立刻退出，即使key相等 if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } ...} 所以建议采用《Effective Java》中的第8、9条条例，小心的设计自己对象的equals和hashCode方法。假如，两个方法设计糟糕将会导致该对象作为key保存在HashMap中时，当对该对象执行get或put等操作，即使客观上两者是相同的对象，也会被区别对待，最终导致HashMap产生内存泄漏问题。 落槽方法indexFor &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在解决哈希冲突问题上，通常采用取余操作将数据均分到不同桶中，然而在计算机中取余操作相比于其他逻辑运算比较消耗性能，首先需要对数值转换十进制，然后不断的执行被除数和除数的除法操作最终获得余数，这在大范围使用上是不合适的，HashMap采用对2次幂减1并求异或是在二进制层面的位运算，效率很高更适合作为解决哈希冲突的手段，唯一约束就是要求容量必须保证是2次幂 123static int indexFor(int h, int length) { return h &amp; (length-1);} 假设，整型h=101，length=8，s=length-1=7 h对应二进制是 0000 0000 0000 0000 0000 0000 0110 0101‬ length对应二进制是 0000 0000 0000 0000 0000 0000 0001 0000 length-1对应的二进制是 0000 0000 0000 0000 0000 0000 0000 1111 按照异或的特性会将h的高位全部截断只保留与h对应的位数 0000 0000 0000 0000 0000 0000 0000 0101‬ 相当于求0101^1111，最终的哈希值是0101，即5 内存泄漏 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过源码可以看到，比较元素相等时优先比较哈希码是否相等，再短路比较key相等，所以使用HashMap时需要保证： 作为键值Key必须是不可变的。 如果使用类作为Key，那该类最好是是不可变类。 如果不是不可变类，那应该保证同时重写equals方法和hashCode方法，并且重写的hashCode方法是无状态的，即状态不可变，不会使用可变的依赖值，否则会导致理论上同一对象却有两种不同的hashCode，最终使HashMap发生内存泄漏。 内存溢出：存储容量过多且GC无法回收，导致内存使用量达到JVM阈值，发生OOM。 内存泄漏：对象以后不会被访问，但由于某些原因（在可用的GC ROOT上）GC无法回收，导致无效数据占用内存空间并最终发生OOM。","link":"/2020/05/07/4%20%E6%BA%90%E7%A0%81/Java%E6%BA%90%E7%A0%81%E2%80%94%E2%80%94HashMap%EF%BC%88%E4%B8%80%EF%BC%89/"},{"title":"常用算法——分治法","text":"分治法 分而治之，治而合之 说明：文中动态插图援引微信公众号：《五分钟学算法》 概念​ 分治法（Divide and Conquer, D&amp;C），将一个复杂问题纵向拆分为多个最优结构原子问题，再将原子问题的求解合并，组成复杂问题的最终解。递归是分治法的常用手段，也是经典手段。 使用条件 复杂问题拆分后就容易求解。 拆分后的原子问题具有最优子结构。 原子问题的解可以合并成复杂问题的解。 原子问题独立存在，不含公共子问题。 算法框架 分解：纵向拆分复杂问题为原子问题。 解决：对原子问题进行求解。 合并：将原子问题解合并，组成最终解。 算法复杂度​ 一个规模为n的实例可以划分为k个规模为n/k的实例，其中α个实例是需要求解的： O(N) = αT(n/k) + f(n) ​ f(n)与具体的拆分和合并算法有关。 经典算法归并排序（Merge Sort）： 拆分：将一个无序序列递归拆分，最终拆分为长度为1的子序列。 解决：一次将子序列两两返回并进行排序。 合并：自下而上的对子序列进行合并，最终组成完整的排序后序列。 自上而下拆分， 自下而上合并 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class MergeSorted implements IArraySort { @Override public int[] sort(int[] srcArray) { int[] arr = Arrays.copyOf(srcArray, srcArray.length); // 操作副本排序，不影响原数组 if (arr.length &lt; 2) { return arr; } int middle = srcArray.length / 2; int[] leftArr = Arrays.copyOfRange(arr, 0, middle); int[] rightArr = Arrays.copyOfRange(arr, middle, arr.length); return merge(sort(leftArr), sort(rightArr)); } private int[] merge(int[] leftArr, int[] rightArr) { int[] result = new int[leftArr.length + rightArr.length]; int index = 0; while (leftArr.length &gt; 0 &amp;&amp; rightArr.length &gt; 0) { if (leftArr[0] &lt;= rightArr[0]) { result[index] = leftArr[0]; index++; leftArr = Arrays.copyOfRange(leftArr, 1, leftArr.length); } else { result[index] = rightArr[0]; index++; rightArr = Arrays.copyOfRange(rightArr, 1, rightArr.length); } } while (leftArr.length &gt; 0) { result[index] = leftArr[0]; index ++; leftArr = Arrays.copyOfRange(leftArr, 1, leftArr.length); } while (rightArr.length &gt; 0) { result[index] = rightArr[0]; index ++; rightArr = Arrays.copyOfRange(rightArr, 1, rightArr.length); } return result; } public static void main(String[] args) { int[] arr = {3,23,4,13,4,5,63,6,2}; arr = new MergeSorted().sort(arr); System.out.println(arr.length); for (int i : arr) { System.out.print(i + &quot; &quot;); } }} 快速排序（Quick Sort） 解决：设置基准点pivot，并定位两端索引位置left和right，一边排序找到比pivot小的放在左边，比pivot大的放在右边。 拆分：一边排序后，以pivot为分割点，将左右两边序列依次执行步骤1。 合并：将原子序列排序后合并成为最终有序序列。 自上而下排序并拆分 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class QuikSort implements IArraySort { @Override public int[] sort(int[] sourceArray) { // 对 arr 进行拷贝，不改变参数内容 int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); quickSort(arr, 0, arr.length - 1); return arr; } private void quickSort(int[] arr, int left, int right) { if (left &gt; right) { return; } int leftIndex = left; int rightIndex = right; int pivot = arr[leftIndex]; // 基准 while (left &lt; right) { while (left &lt; right &amp;&amp; arr[right] &gt;= pivot) { right--; } while (left &lt; right &amp;&amp; arr[left] &lt;= pivot) { left++; } if (left &lt; right) { swap(arr, right, left); } } swap(arr, leftIndex, left); // 此时left==right quickSort(arr, leftIndex, left-1); quickSort(arr,left + 1, rightIndex); } private void swap(int[] arr, int i, int j) { int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; } public static void main(String[] args) { int[] arr = {6,1,2,7,9,3,4,5,10,8}; arr = new QuikSort().sort(arr); System.out.println(arr.length); for (int i : arr) { System.out.print(i + &quot; &quot;); } }} 注意：如果基准点pivot设置为左顶端数，那么应该保证先从右侧开始比较，这样可以保证最终left和right停留的相同位置是小于pivot的，所以可以交换，否则，交换的将是大于pivot的值，不满足快排条件。 汉诺塔 A、B、C三根柱子，其中A柱子上面有从小叠到大的n个圆盘，现要求将A柱子上的圆盘移到C柱子上去，期间只有一个原则：一次只能移到一个盘子且大盘子不能在小盘子上面，求移动的步骤和移动的次数 把n-1个盘子由A 移到 B；把第n个盘子由 A移到 C；把n-1个盘子由B 移到 C； 1234567891011121314151617181920212223242526272829public class Hanoi { private static int step = 1; // 步数 public static void hanoi(int plateNum, String a, String b, String c) { if (plateNum == 1) { move(plateNum, a, c); } else { hanoi(plateNum - 1, a, c, b); move(plateNum, a, c); hanoi(plateNum - 1, b, a, c); } } private static void move(int plateNum, String from, String to) { System.out.printf(&quot;%d step, %d plate move from %s to %s \\n&quot;, step++, plateNum, from, to); } public static void main(String[] args) { hanoi(3, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;); }}1 step, 1 plate move from A to C 2 step, 2 plate move from A to B 3 step, 1 plate move from C to B 4 step, 3 plate move from A to C 5 step, 1 plate move from B to A 6 step, 2 plate move from B to C 7 step, 1 plate move from A to C","link":"/2020/04/01/3%20%E7%AE%97%E6%B3%95%E5%92%8C%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E2%80%94%E2%80%94%E7%BA%A2%E9%BB%91%E6%A0%91/"},{"title":"Java源码——HashMap","text":"HashMap（二） 说明：文章部分内容来自https://coolshell.cn/articles/9606.html &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;JDK7中的HashMap是线程不安全的，尽管官方文档明确声明并发情况下不能使用，但在问题列表例总能找到强行使用的童鞋，这就会导致一个严重的问题：死链，通过恢复现场就能找到线程在调用get()方法时被堵死，重启可以临时清空内存，但时间一长又会复现。 死链 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;死链又称为无限循环（HashMap Infinite Loop），一旦产生死链，系统的CPU就飙满，导致fatal级系统问题。其根本原因主要发生在扩容方法resize()时将旧数据移动到新容器的过程中，以下通过源码来看。 12345678910111213141516171819202122232425262728293031void resize(int newCapacity) { Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return; } Entry[] newTable = new Entry[newCapacity]; // 注意transfer方法 transfer(newTable, initHashSeedAsNeeded(newCapacity)); table = newTable; threshold = (int)Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1);}void transfer(Entry[] newTable, boolean rehash) { int newCapacity = newTable.length; for (Entry&lt;K,V&gt; e : table) { while(null != e) { // 数据迁移时发生死链场景 Entry&lt;K,V&gt; next = e.next; if (rehash) { e.hash = null == e.key ? 0 : hash(e.key); } int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; } }} 当两个线程Thread1和Thread2交替执行transfer注释部分的代码时，有可能导致如下图的链表指向情况。 当执行Thread1的get(key)方法时，恰好该key落槽到死链的桶中，链表的循环遍历就会在3和7之间无限循环，拉满CPU使用率。 虽然该问题很快被SUN发现，但被认为HashMap本来就是线程非安全并不应该用在并发场景下，可以使用ConcurrentHashMap代替，所以在多个大小版本中一直没有解决更新，直到JDK8发布。 2 JDK8 HashMap改进总结 将桶内数据结构仅支持链表改为同时支持链表+红黑树。 针对死链问题，改进扩容时的插入顺序。 新增lambda表达式支持函数，如forEach()。 新增API：replace()和merge() 在源码浏览时，如果与JDK7相同则不重复介绍 由于整体源码写作风格与当前流行的编码规范格格不入（圈复杂度高，变量不优先初始化而是首次调用初始化，排版有些随意）可能会引起阅读的不适…… 成员变量 123456// 红黑树转换阈值，当桶内数据容量达到8时，会将链表转变为红黑树static final int TREEIFY_THRESHOLD = 8; // 当删除或容器扩容，桶内容量减少到6时会还原回链表static final int UNTREEIFY_THRESHOLD = 6;// 红黑树桶的最小容量是64static final int MIN_TREEIFY_CAPACITY = 64; 一般来说，当哈希码分布均匀时，很少会产生红黑树桶，TREEIFY_THRESHOLD设置为8从离散随机分布的角度来看满足泊松分布，即加载因子在默认0.75时，红黑树转换阈值达到8的概率是比较低的，因为虽然红黑树提高了元素的遍历查找和修改的效率，但是空间上比链表占用二倍的内存，同时树化的过程是需要消耗时间的。总结，链表转换红黑树的前提是哈希函数设计不合理导致大量哈希冲突，设计良好的哈希函数不会也没有必要树化。 1234567891011121314// 官方注释/* Because TreeNodes are about twice the size of regularnodes, we use them only when bins contain enough nodes to warrant use(see TREEIFY_THRESHOLD). And when they become too small (due to removal or resizing) they are converted back to plain bins. In usages with well-distributed user hashCodes, tree bins are rarely used. Ideally, under random hashCodes, the frequency of nodes in bins follows a Poisson distribution(http://en.wikipedia.org/wiki/Poisson_distribution) with a parameter of about 0.5 on average for the default resizing threshold of 0.75, although with a large variance because of resizing granularity. Ignoring variance, the expected occurrences of list size k are (exp(-0.5) * pow(0.5, k)/factorial(k)). The first values are:0: 0.606530661: 0.303265332: 0.075816333: 0.012636064: 0.001579525: 0.000157956: 0.000013167: 0.000000948: 0.00000006*/static final int TREEIFY_THRESHOLD = 8; 泊松分布与加载因子无关，而是量化发生树化的概率；加载因子与哈希表的容量有关。 比如: 默认容量capacity = 16，可放数据量为threshold = capacity * loadfactor = 16 * 0.75 = 12，那么一个桶内放入8个元素（树化阈值）的概率是0.00000006（泊松分布概率）。 同理，当容器扩容到64时，threshold = capacity * loadfactor = 64 * 0.75 = 48, 那么一个桶内放入8个元素的概率还是0.00000006。 put方法 同样的，put方法永远是研究HashMap的精髓。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public V put(K key, V value) { // putVal先计算key的哈希码，hashCode值高16位与低16异或 return this.putVal(hash(key), key, value, false, true);}final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 空表初始化并赋值 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 落位i = (n-1) &amp; hash如果元素不存在则在桶中新建节点 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else { Node&lt;K,V&gt; e; K k; // 如果桶内头元素是目标元素，则更新数据 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 如果桶内是红黑树则调用红黑树方法保存节点 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 如果桶内是链表则调用链表方法保存节点 else { for (int binCount = 0; ; ++binCount) { // 插入数据（尾插），桶内元素个数达到树化阈值则跳出循环进行树化 if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } // 链表插入数据 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; } } if (e != null) { // existing mapping for key V oldValue = e.value; // onlyIfAbsent指如果元素相同新指替换旧值 if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;}// 以下三个方法是LinkedHashMap的实现方法，void afterNodeAccess(Node&lt;K,V&gt; p) { }void afterNodeInsertion(boolean evict) { }void afterNodeRemoval(Node&lt;K,V&gt; p) { } JAVA7的HashMap在新增数据时是将新数据插入桶内头部，而JAVA8采用在链表尾部插入数据。 源码中的变量初始化赋值发生在第一次调用点，所以阅读起来并不顺畅。 哈希函数和落位操作比JAVA7更加简洁且直观。 元素相等判断不变，还是以哈希码相等开始并且是短路的（考虑内存泄漏可能）。 落槽操作 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;JAVA8没有为落槽操作单独写一个方法，而是采用table[i = (n-1) * hash]来完成，其实这与JAVA7中的操作一样h &amp; (length-1)，都依赖2次幂的容量。但JAVA8利用按位与的特性巧（骚）妙（操）地（作）解决了开篇所说的死链问题，介绍扩容方法时将详细说明。 哈希函数 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;新的hash方法，对象hashCode与自身右移16位求异或，相比JDK7的实现“清爽”了许多， 我们知道int类型是32位，右移16位再异或，就相当于将高16位和低16位进行异或操作。 1234static final int hash(Object key) { int h; return key == null ? 0 : (h = key.hashCode()) ^ h &gt;&gt;&gt; 16;} 扩容方法 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;扩容方法resize()不仅支撑扩容，还会初始化容器（将容器初始化放到第一次调用位置，减少内存无效开销），与JAVA7一样，初始默认容量是16，加载因子0.75f，阈值=加载因子*容量，为了高效落位容量适中保持2次幂，扩容为2倍。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了解决开头说的并发死链问题，巧妙地利用落槽操作时用到的按位与操作特性保证了数据迁移的顺序问题，保证数据不会发生错位。介绍源码之前先简单的介绍一下原理： 说明：按位与会截断高位数据，只保留与低位一样的位数。 假设哈希码是1010 0101 1100 0011 1101 0100 1011 1001 假设，初始容量是capacity = 16，capacity - 1对应的二进制是 0000 0000 0000 0000 0000 0000 0000 1111 与哈希码按位与时，哈希码高位截断，只保留与容量位数一致，可以简写为： 1001 &amp; 1111，结果是1001 当容量扩容后capacity = 32, capacity -1 对应的二进制是 0000 0000 0000 0000 0000 0000 0001 1111 与哈希码按位与时，哈希码高位截断，只保留与容量位数一致，可以简写为： 11001 &amp; 11111，结果是11001 从两次的按位与结果来看，低位1001不变，只有高位1变化（11001），这个变化与具体的哈希码有关，但无非是0或者1，所以JAVA8扩容方法抓住这一特性使得：数据迁移时，如果rehash后高位是0，则该数据保持在原来桶的位置不变；如果高位是1，则该数据重新分配到新空间的桶中。这样就避免了rehash重新分配所有数据落槽位置，导致元素顺序引用发生改变，从而引起并发下的死链。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980final Node&lt;K,V&gt;[] resize() { Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; // 设定阈值 if (oldCap &gt; 0) { if (oldCap &gt;= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold } else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; // 初始化HashMap容量和阈值 else { // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); } threshold = newThr; @SuppressWarnings({\"rawtypes\",\"unchecked\"}) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) { for (int j = 0; j &lt; oldCap; ++j) { Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) { oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); // 官方备注“维持顺序” // hi前缀变量表示高位，lo前缀表示低位 else { // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do { next = e.next; // 注意是oldCap，而不是oldCap-1,此时与操作就是只看上面提到的二进制高位 // 二进制高位是0 if ((e.hash &amp; oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } // 二进制高位是1 else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); if (loTail != null) { loTail.next = null; newTab[j] = loHead; // 原来桶内 } if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; // 新分配空间桶内 } } } } } return newTab;} 至此，HashMap源码介绍完毕，新API将在未来lambda表达式中详细介绍（可能）（手动狗头）。","link":"/2020/05/07/4%20%E6%BA%90%E7%A0%81/Java%E6%BA%90%E7%A0%81%E2%80%94%E2%80%94HashMap%EF%BC%88%E4%BA%8C%EF%BC%89/"},{"title":"Java源码——ConcurrentHashMap（二）","text":"ConcurrentHashMap（二） &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Java7中ConcurrentHashMap采取了数据一致性和效率的折中办法——分段锁Segment，但为了进一步提高性能，Java8中完全抛弃了分段锁，采用CAS + synchronized方式解决并发问题。这是一种更加细粒度的加锁方式，直接针对哈希表的槽。 ReservationNode，对占位节点加锁，当执行compute方法和computeIfAbsent方法时使用。 TreeBin，并不保存实际红黑树，只是对红黑树所在桶进行读写锁维护，并指向红黑树的引用。 ForwardingNode，扩容转发节点，外部对原哈希槽的操作会转发到nextTable上。 ConcurrentHashMap属性 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374// 摘抄部分重要的属性// 同JDK8的HashMap，设定了树化和回退链表的阈值，唯一不同的是，在进行// 树化操作时，要求桶数组容量必须大于64static final int TREEIFY_THRESHOLD = 8;static final int UNTREEIFY_THRESHOLD = 6;static final int MIN_TREEIFY_CAPACITY = 64;// 桶数组，哈希表的具体数据承载结构transient volatile Node&lt;K,V&gt;[] table;// 扩容时的哈希表，扩容也是2倍增长且保持2次幂容量private transient volatile Node&lt;K,V&gt;[] nextTable;// 初始化和扩容控制// sizeCtl=-1， 正在进行初始化// sizeCtl=-n， 有n-1个线程正在扩容// sizeCtl=0， 默认值，使用默认容量进行初始化// sizeCtl&gt;0， 扩容需要用到的容量，即阈值private transient volatile int sizeCtl;// ForwardingNode节点的哈希值static final int MOVED = -1; // 红黑树根节点哈希值static final int TREEBIN = -2; // 数据节点的保存结构，同HashMapstatic class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final int hash; final K key; volatile V val; volatile Node&lt;K,V&gt; next;}// 扩容转发节点，当该节点置于桶中，外部对原来哈希表的操作会转移到nextTable上进行static final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; { final Node&lt;K,V&gt;[] nextTable; Node&lt;K,V&gt; find(int h, Object k) {...}}// 预置加锁节点，对桶内的第一个数据进行加锁static final class ReservationNode&lt;K,V&gt; extends Node&lt;K,V&gt; { ReservationNode() { super(RESERVED, null, null, null); } Node&lt;K,V&gt; find(int h, Object k) { return null; }}// 维护对桶内红黑树的读写所，保存红黑树节点的引用static final class TreeBin&lt;K,V&gt; extends Node&lt;K,V&gt; { TreeNode&lt;K,V&gt; root; volatile TreeNode&lt;K,V&gt; first; volatile Thread waiter; volatile int lockState; // values for lockState static final int WRITER = 1; // set while holding write lock static final int WAITER = 2; // set when waiting for write lock static final int READER = 4; // increment value for setting read lock ...}// 红黑树的数据存储节点static final class TreeNode&lt;K,V&gt; extends Node&lt;K,V&gt; { TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; ...} Java8中显式的取消了加载因此loadFactor，但并没有取消阈值的计算threshold = capacity * loadFactor，而是采用n - (n &gt;&gt;&gt; 2) 的方式代替，构造器同样支持传入自定义加载因子，但只会在初始化容器时使用 构造器 1234567891011public ConcurrentHashMap() {} // 空构造器，初始化放在第一次添加位置// 自定义容量，对容量向上取2次幂，对sizeCtl赋值public ConcurrentHashMap(int initialCapacity) { if (initialCapacity &lt; 0) throw new IllegalArgumentException(); int cap = ((initialCapacity &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(initialCapacity + (initialCapacity &gt;&gt;&gt; 1) + 1)); this.sizeCtl = cap;} put方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879public V put(K key, V value) { return putVal(key, value, false);}final V putVal(K key, V value, boolean onlyIfAbsent) { if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode()); // 哈希函数 int binCount = 0; // 记录相应链表长度 for (Node&lt;K,V&gt;[] tab = table;;) { Node&lt;K,V&gt; f; int n, i, fh; // 空哈希表初始化 if (tab == null || (n = tab.length) == 0) tab = initTable(); // CAS初始化 // tabAt是落槽操作 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) { // 如果落槽位置无数据，使用CAS放入新值，如果CAS失败说明产生并发初始化 if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; } // MOVED表示产生数据迁移，那么当前线程帮助进行数据迁移 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else { // 当前桶不为空，且指向头结点 V oldVal = null; // 对该位置的头结点加锁，进行put操作 // synchronized已经进行了优化，采取偏向所、轻量锁和重量锁的升级 synchronized (f) { if (tabAt(tab, i) == f) { // 再次确认当前落槽位置没有发生变化 if (fh &gt;= 0) { // 头结点的 hash 值大于 0，说明是链表， binCount = 1; // 链表长度计数器 // 遍历链表确认是覆盖还是新增，与HashMap同理 for (Node&lt;K,V&gt; e = f;; ++binCount) { K ek; if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) { oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; } Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) { pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; } } } // 红黑树 else if (f instanceof TreeBin) { Node&lt;K,V&gt; p; binCount = 2; // 调用红黑树的插值方法插入新节点 if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) { oldVal = p.val; if (!onlyIfAbsent) p.val = value; } } } } if (binCount != 0) { // 判断是否需要将链表树化 // 虽然树化阈值也是8，但与HashMap不同点在于，如果容量小于64则会对哈希表扩容， // 如果大于64才进行树化 if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; } } } addCount(1L, binCount); return null;} 这里可以看到Java8中采用CAS + synchronized方式保证线程安全，那为什么synchronized在HashTable中效率很低被放弃，但在这里又被重新启用？ 实际上此时JVM对synchronied进行了优化，不在是重量级的互斥锁而变成了可自动升级的锁。我们知道按照锁的效率级别可以分为：偏向锁、轻量级锁和重量级锁，synchronized也正是按照这样的升序级别进行不断升级。 在JVM字节码中，每个对象都有monitor隐藏参数，该参数是加锁的监视器，当初始化对象锁时monitor=1采用偏向锁，当线程重复进入加锁时会判断，如果当前线程已经存在锁则使用原有锁，这样就保证在低频并发时单线程的执行效率。如果产生并发则将偏向锁升级为轻量级锁，如果并发量增加则会进一步进化为重量级锁。按照Java8中的ConcurrentHashMap采用CAS方式就可以知道设计前提，认为并发并不总是频繁发生的，所以synchronized很少会进入重量级锁。 哈希函数spread &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;与Java8中的HashMap哈希函数类似，都是采用key的哈希码自身按照高16位和低16位异或，不同的时此处还进行了常量HASH_BITS的与运算，可以理解为计算更均匀平滑的哈希值，消除负哈希。 1234567static final int HASH_BITS = 0x7fffffff; // usable bits of normal node hashint hash = spread(key.hashCode()); // put中的方法static final int spread(int h) { return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS;} 初始化initTable &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过CAS来进行初始化，如果被其他线程初始化那么使用yield进行一次自旋，并在下次while循环中退出本次初始化，因为table时volitale的。 123456789101112131415161718192021222324private final Node&lt;K,V&gt;[] initTable() { Node&lt;K,V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) { // 被其他线程初始化 if ((sc = sizeCtl) &lt; 0) Thread.yield(); // lost initialization race; just spin // CAS操作，sizeCtl = -1，表示抢到了初始化的锁 else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) { try { if ((tab = table) == null || tab.length == 0) { int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; // 初始化数组 Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = tab = nt; sc = n - (n &gt;&gt;&gt; 2); // 阈值，其实等同于n*加载因子0.75 } } finally { sizeCtl = sc; // 哈希表容量控制器赋值，如果使用默认容量，该指为12 } break; } } return tab;} 显式的取消了加载因子loadFactor，采用 n - (n &gt;&gt;&gt; 2) 作为通过加载因子求阈值的替代。 扩容tryPresize &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;HashMap都一样，都是2倍扩容，但是在这里扩容操作稍显复杂，并且伴随着数据迁移的相关操作，整个过程需要sizeCtl参与和控制。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// sizeCtl=-1， 正在进行初始化// sizeCtl=-n， 有n-1个线程正在扩容// sizeCtl=0， 默认值，使用默认容量进行初始化// sizeCtl&gt;0， 扩容需要用到的容量，即阈值private final void tryPresize(int size) { // size已经是原容量的2倍值 // 假如size=32，那么c=64 int c = (size &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(size + (size &gt;&gt;&gt; 1) + 1); int sc; while ((sc = sizeCtl) &gt;= 0) { Node&lt;K,V&gt;[] tab = table; int n; // 同初始化原理 if (tab == null || (n = tab.length) == 0) { n = (sc &gt; c) ? sc : c; if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) { try { if (table == tab) { Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = nt; sc = n - (n &gt;&gt;&gt; 2); } } finally { sizeCtl = sc; } } } else if (c &lt;= sc || n &gt;= MAXIMUM_CAPACITY) break; else if (tab == table) { int rs = resizeStamp(n); if (sc &lt; 0) { // 有多个线程正在进行扩容 Node&lt;K,V&gt;[] nt; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; // CAS对sc+1操作并参与数据迁移 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); } // 没有其他线程进行扩容，当前线程进行扩容和数据迁移 else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); } }} 数据迁移transfer &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该方法应该是最难阅读的方法，目的是一个或多个线程协作从旧哈希表将数据迁移到新哈希表中，其原理是：假设原哈希表的容量是x，每个桶做数据迁移需要保持数据一致，可以将一个桶视作一个任务单元，所以有x个迁移任务，按照分治的思想，如果当前有y个线程参与数据迁移，就会把x个迁移任务拆分给每个线程去做。当某个线程完成迁移任务后，会检查是否还有未进行的迁移任务并参与其中。transferIndex用来调度安排线程执行的迁移任务。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) { int n = tab.length, stride; // stride可以理解为拆分任务的分片，与CPU（NCPU参数）有关 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range // 如果 nextTab 为 null，先进行一次初始化 if (nextTab == null) { try { Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; // 新的哈希表 nextTab = nt; } catch (Throwable ex) { sizeCtl = Integer.MAX_VALUE; return; } nextTable = nextTab; transferIndex = n; } int nextn = nextTab.length; // 如果当前桶正在被线程迁移，则会设置 ForwardingNode 进行加锁 ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); boolean advance = true; // 表示可以执行迁移操作 boolean finishing = false; // 表示完成当前迁移操作 // i 是位置索引，bound 是边界，注意是从后往前 for (int i = 0, bound = 0;;) { Node&lt;K,V&gt; f; int fh; while (advance) { int nextIndex, nextBound; if (--i &gt;= bound || finishing) advance = false; // 如果transferIndex小于等于0，说明每个任务都有线程正在执行 else if ((nextIndex = transferIndex) &lt;= 0) { i = -1; advance = false; } else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) { // CAS操作，获取迁移边界 bound = nextBound; i = nextIndex - 1; advance = false; } } if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) { int sc; // 完成数据迁移 if (finishing) { nextTable = null; table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); // 重新计算 sizeCtl return; } if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) { // 任务结束，方法退出 if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; // 完成迁移任务，设置标志位finishing=true finishing = advance = true; i = n; } } // 如果位置 i 处是空的，表示没有任何节点，该槽设置ForwardingNode，表示已迁移或正在迁移 else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); else if ((fh = f.hash) == MOVED) advance = true; else { // 对桶加锁，进行迁移工作 synchronized (f) { if (tabAt(tab, i) == f) { // 头结点的 hash 大于 0，说明是链表 Node&lt;K,V&gt; ln, hn; if (fh &gt;= 0) { // 具体迁移方法，原理同JAVA8的HashMap int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) { int b = p.hash &amp; n; if (b != runBit) { runBit = b; lastRun = p; } } if (runBit == 0) { ln = lastRun; hn = null; } else { hn = lastRun; ln = null; } for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) { int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); } setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); // 设置ForwardingNode表示迁移完成 // advance 设置为 true，表示该桶迁移完毕 advance = true; } else if (f instanceof TreeBin) { // 红黑树的迁移，原理同HashMap和以上 TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) { int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) { if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; } else { if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; } } ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; } } } } }} 数据迁移方法还是比较复杂的，具体的迁移方法与Java8的HashMap相同，困难主要体现在对多个线程参与数据迁移过程的控制，当然，理解了Doug Lea的设计原理，源码阅读也会相对明了一些。","link":"/2020/05/25/4%20%E6%BA%90%E7%A0%81/Java%E6%BA%90%E7%A0%81%E2%80%94%E2%80%94ConcurrentHashMap%EF%BC%88%E4%BA%8C%EF%BC%89/"},{"title":"Java源码——ConcurrentHashMap（一）","text":"ConcurrentHashMap（一） &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在并发编程领域，ConcurrentHashMap是最推崇使用的哈希式集合类型。JDK8对它进行了脱胎换骨的改造，大量运用了Lock-Free技术，从而减轻因锁的竞争导致的性能问题，该类涵盖了CAS、锁、volatile、链表、红黑树等知识点，是从官方学习Java并发编程的绝佳案例。 Lock-Free：是无锁编程(Non-Blocking Sync)的实现，对于共享数据不使用锁来控制并发访问（互斥锁等排它锁类型），而是多线程并行访问，旨在避免使用Lock带来的线程阻塞等性能问题，虽然无法代替Lock。 CAS（Compare And Swap）：是乐观锁的实现，应用于轻微冲突的并发场景，因为CAS在进行自旋操作时占用CPU较多，所以不适合高并发场景。在JUC的atomic包下大量用到了CAS操作，保证变量的原子性。它的原理是，首先获得值的内存地址、预期值和新值，每个线程在获取并更新的过程中先访问内存地址的预期值比较（Compare）是否一致，如果一致则更新为新值（Swap）；如果不一致，先对预期值轮询操作，比较之后达到预期值再更新新值或者退出。CAS需要避免发生ABA问题，可以通过版本号来增加比较条件进行解决。 经典的CAS用法来自于AtomicInteger的自增操作方法，如下： 1234567891011121314public final int getAndIncrement() { for(;;) { int current = get(); // 预期值 int next = current + 1; // 新值 if (compareAndSet(current, next)) { // 自旋 return current; } }}// unsafe来自native方法，支持直接调用硬件的原子性能力。public final boolean compareAndSet(int expect, int update) { return unsafe.compareAndSwapInt(this, valueOffset, expect, update);} ConcurrentHashMap的发展 我们知道HashTable是线程安全的字典集合Dictionary，但底层实现使用了性能极差的全互斥方式，所以已经被淘汰。 HashMap是非线程安全的Map集合，容易产生并发问题，特别是死链问题。 Collections.synchronizedMap(Map&lt;K,V&gt; m)方法是集合工具类对普通Map的同步代理类，原理是使用排它锁mutex来锁定对象。 JDK8以前的ConcurrentHashMap采用分段式锁设计，旨在平衡性能和线程安全，内部采用重入锁ReentrantLock进行并发控制，将每个HashEntry进行加锁管理。 JDK8之后的ConcurrentHashMap采用Lock-Free理念，取消了分段式锁，采用CAS和其他优化设计提高了并发能力并降低了冲突概率。 1 HashTable的线程安全&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;HashTable继承自Dictionary类，与HashMap结构大致相同，几个关键的不同点：1. HashTable的哈希函数使用key的哈希码，所以key不允许为null，否则会产生NPE；2. 落槽操作采用哈希表容量取余来完成，效率低，但不会强制要求哈希表容量是2次幂；3. 扩容都是2倍；4. HashTable是线程安全的，它采用重量级的synchronized对方法加锁，效率极低。 如上图，当线程1调用put方法向1号槽插入数据时会获得整个哈希表的锁，此时线程2同时调用put方法企图向2号槽插入数据就会被阻塞，直到线程1释放锁。很明显，并发put时，如果不是同一个槽位是可以不用对哈希表整体加锁的，效率极低。 123456// HashTable内都是全互斥锁public synchronized V put(K key, V value)public synchronized V get(Object key)public synchronized boolean contains(Object k)public synchronized boolean isEmpty()public synchronized V remove(Object key) 不同于在HashMap中已经介绍的fail-fast机制同步时如果数据不一致会直接抛出ConcurrentModificationException异常，HashTable采用fail-safe机制，其原理是先获得当前哈希表的副本，并对副本进行迭代，虽然不会受到同步修改的影响，但不能保证迭代的数据是最新的。 2 Collections.synchronizedMap的线程安全&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Collections提供了多个支持对普通集合同步化的代理类，包括List、Set、Map等，针对不同类型都实现了对应的内部代理类。 12345678910111213static class SynchronizedCollection&lt;E&gt; implements Collection&lt;E&gt;, Serializable {...}static class SynchronizedSet&lt;E&gt; extends SynchronizedCollection&lt;E&gt; implements Set&lt;E&gt; {...}static class SynchronizedSortedSet&lt;E&gt; extends SynchronizedSet&lt;E&gt; implements SortedSet&lt;E&gt;{...}static class SynchronizedList&lt;E&gt; extends SynchronizedCollection&lt;E&gt; implements List&lt;E&gt; {...}static class SynchronizedRandomAccessList&lt;E&gt; extends SynchronizedList&lt;E&gt; implements RandomAccess {...}private static class SynchronizedMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Serializable {...} static class SynchronizedSortedMap&lt;K,V&gt; extends SynchronizedMap&lt;K,V&gt; implements SortedMap&lt;K,V&gt; {...} 其主要实现是采用互斥锁mutex对对象加锁，在锁级别上与HashTable是一致的，之所以Collections会提供这样重量级的锁是为了保证如果业务涉及高度的数据安全性且性能要求不严苛的场景使用，稍后介绍的ConcurrentHashMap之所以采用CAS，前提是认为并发并不总是存在的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;内部类提供了两个构造器，默认和指定互斥锁对象，默认情况下互斥锁为当前对象this。 1234567891011SynchronizedMap(Map&lt;K,V&gt; m) { if (m==null) throw new NullPointerException(); this.m = m; mutex = this;}SynchronizedMap(Map&lt;K,V&gt; m, Object mutex) { this.m = m; this.mutex = mutex;} 实际调用方法采用指定集合对象的自身方法。 12345678910111213141516171819202122232425262728public int size() { synchronized (mutex) {return m.size();}}public boolean isEmpty() { synchronized (mutex) {return m.isEmpty();}}public boolean containsKey(Object key) { synchronized (mutex) {return m.containsKey(key);}}public boolean containsValue(Object value) { synchronized (mutex) {return m.containsValue(value);}}public V get(Object key) { synchronized (mutex) {return m.get(key);}}public V put(K key, V value) { synchronized (mutex) {return m.put(key, value);}}public V remove(Object key) { synchronized (mutex) {return m.remove(key);}}public void putAll(Map&lt;? extends K, ? extends V&gt; map) { synchronized (mutex) {m.putAll(map);}}public void clear() { synchronized (mutex) {m.clear();}} 3. Java7 ConcurrentHashMap的线程安全&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Java7中的ConcurrentHashMap使用了分段锁机制，内部类Segment是实现分段锁的对象，在集合初始化时会先初始化Segment数组，其数组容量与并发级别（concurrency level，或称为并发数）有关，默认并发级别为16与默认容量一致，也就是说默认支持16个线程的并发，默认级别可以通过构造器传入，但适中保持2次幂的值，这时为了使Segment锁可以均匀管理不同的集合，与HashMap落槽操作一样，使用并发级别减1按位与操作来定位分段锁。分段锁容量初始化后将不在扩容，而分段锁管理的HashMap可以继续扩容。逻辑结构如下图所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344// 主要的构造器实现// concurrencyLevel为并发级别，可手动传入，但会进行2次幂处理public ConcurrentHashMap(int initialCapacity,float loadFactor, int concurrencyLevel) { if (!(loadFactor &gt; 0) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); if (concurrencyLevel &gt; MAX_SEGMENTS) concurrencyLevel = MAX_SEGMENTS; int sshift = 0; int ssize = 1; // 使用ssize计算分段锁的个数，即2次幂处理 while (ssize &lt; concurrencyLevel) { ++sshift; ssize &lt;&lt;= 1; } // 假设使用默认值concurrencyLevel=ssize=16，sshift=4 // segmentShift=28 // segmentMask=15 this.segmentShift = 32 - sshift; this.segmentMask = ssize - 1; // initialCapacity是哈希表初始容量 if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; // 根据 initialCapacity 设置Segment可以管理多少个哈希表 // 假设initialCapacity=16，那么每个 Segment 可以管理1个哈希表，如果initialCapacity=32则可以管理2个 int c = initialCapacity / ssize; if (c * ssize &lt; initialCapacity) ++c; int cap = MIN_SEGMENT_TABLE_CAPACITY; while (cap &lt; c) cap &lt;&lt;= 1; // 创建 Segment 数组ss Segment&lt;K,V&gt; s0 = new Segment&lt;K,V&gt;(loadFactor, (int)(cap * loadFactor), (HashEntry&lt;K,V&gt;[])new HashEntry[cap]); Segment&lt;K,V&gt;[] ss = (Segment&lt;K,V&gt;[])new Segment[ssize]; // 往数组写入s0 UNSAFE.putOrderedObject(ss, SBASE, s0); this.segments = ss;} UNSAFE是直接通过java api调用底层CAS能力，只有是bootstrap类加载器加载的类才可以调用UNSAFE，否则只能通过反射来调用。 假设我们全部使用默认值来完成了容器的初始化，此时关键参数如下： Segment数组长度为16且不可扩容 segmentShift=32-4=28， segmentMask=16-1=15。 Segment数组只初始化了Segment[0]元素，其他元素还是null。 put操作 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分两部分，第一部分首先在数组中定位Segment，确认使用的分段锁对象。 123456789101112131415public V put(K key, V value) { Segment&lt;K,V&gt; s; if (value == null) throw new NullPointerException(); // 1. 计算 key 的 hash 值 int hash = hash(key); // 2. 类比落槽操作，通过hash无符号右移与掩码按位与，找到Segement数组j位置元素 int j = (hash &gt;&gt;&gt; segmentShift) &amp; segmentMask; // ensureSegment(j) 对 segment[j] 进行初始化 if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObject (segments, (j &lt;&lt; SSHIFT) + SBASE)) == null) s = ensureSegment(j); // 3. 插入新值到 槽 s 中 return s.put(key, hash, value, false);} 第二部分，再确定了分段锁对象后，对分段对象加独占锁，再在其中进行哈希表的put操作，类似与HashMap。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253final V put(K key, int hash, V value, boolean onlyIfAbsent) { // 4.先获取 segment 的独占锁 HashEntry&lt;K,V&gt; node = tryLock() ? null : scanAndLockForPut(key, hash, value); V oldValue; try { // 该 segment 管理的哈希表 HashEntry&lt;K,V&gt;[] tab = table; // 5.落槽并使用头插法插入数据 int index = (tab.length - 1) &amp; hash; HashEntry&lt;K,V&gt; first = entryAt(tab, index); // 同HashMap，检索桶内链表 for (HashEntry&lt;K,V&gt; e = first;;) { if (e != null) { K k; if ((k = e.key) == key || (e.hash == hash &amp;&amp; key.equals(k))) { oldValue = e.value; if (!onlyIfAbsent) { e.value = value; ++modCount; } break; } e = e.next; } else { // 如果不为 null，那就直接将它设置为链表表头；如果是null，初始化并设置为链表表头。 if (node != null) node.setNext(first); else node = new HashEntry&lt;K,V&gt;(hash, key, value, first); int c = count + 1; // 如果超过了该 segment 的阈值， segment的哈希表需要扩容 if (c &gt; threshold &amp;&amp; tab.length &lt; MAXIMUM_CAPACITY) rehash(node); else // 没有达到阈值，将 node 放到数组 tab 的 index 位置， setEntryAt(tab, index, node); ++modCount; count = c; oldValue = null; break; } } } finally { // 操作完毕，释放锁 unlock(); } return oldValue;} 初始化ensureSegment方法 12345678910111213141516171819202122232425262728private Segment&lt;K,V&gt; ensureSegment(int k) { final Segment&lt;K,V&gt;[] ss = this.segments; long u = (k &lt;&lt; SSHIFT) + SBASE; Segment&lt;K,V&gt; seg; if ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) { // 使用已初始化的 segment[0] 长度来初始化segment[k] Segment&lt;K,V&gt; proto = ss[0]; int cap = proto.table.length; float lf = proto.loadFactor; int threshold = (int)(cap * lf); // 初始化 segment[k] 管理的哈希表 HashEntry&lt;K,V&gt;[] tab = (HashEntry&lt;K,V&gt;[])new HashEntry[cap]; // 检查该Segment是否被其他线程初始化 if ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) { Segment&lt;K,V&gt; s = new Segment&lt;K,V&gt;(lf, threshold, tab); // 使用 CAS，当前线程成功设值或其他线程成功设值后，退出 while ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) { if (UNSAFE.compareAndSwapObject(ss, u, null, seg = s)) break; } } } return seg;} 加锁scanAndLockForPut &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;采用ReentrantLock可重入锁来显式的对Segment加锁。 1234567891011121314151617181920212223242526272829303132333435private HashEntry&lt;K,V&gt; scanAndLockForPut(K key, int hash, V value) { HashEntry&lt;K,V&gt; first = entryForHash(this, hash); HashEntry&lt;K,V&gt; e = first; HashEntry&lt;K,V&gt; node = null; int retries = -1; // 循环获取锁，ReentrantLock的tryLock while (!tryLock()) { HashEntry&lt;K,V&gt; f; if (retries &lt; 0) { if (e == null) { if (node == null) // 这里可能是因为 tryLock() 失败，所以该槽存在并发，不一定是该位置 node = new HashEntry&lt;K,V&gt;(hash, key, value, null); retries = 0; } else if (key.equals(e.key)) retries = 0; else e = e.next; } // 重试次数如果超过 MAX_SCAN_RETRIES，进入阻塞队列等待锁 else if (++retries &gt; MAX_SCAN_RETRIES) { lock(); // ReentrantLock的lock，加锁 break; } else if ((retries &amp; 1) == 0 &amp;&amp; // 如果发生并发新元素倍插入，那么重新执行 scanAndLockForPut方法 (f = entryForHash(this, hash)) != first) { e = first = f; retries = -1; } } return node;} 扩容rehash &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Segment数组本身初始化后不能扩容，只能针对Segment内部的哈希表进行扩容，扩容容量为2倍增长(满足2次幂容量)。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950private void rehash(HashEntry&lt;K,V&gt; node) { HashEntry&lt;K,V&gt;[] oldTable = table; int oldCapacity = oldTable.length; int newCapacity = oldCapacity &lt;&lt; 1; // aka 2倍 threshold = (int)(newCapacity * loadFactor); // 创建新哈希表 HashEntry&lt;K,V&gt;[] newTable = (HashEntry&lt;K,V&gt;[]) new HashEntry[newCapacity]; int sizeMask = newCapacity - 1; // 新的掩码 // 遍历原数组，将原数组位置 i 处的链表拆分到 新数组位置 i 或 i+oldCap 两个位置 for (int i = 0; i &lt; oldCapacity ; i++) { HashEntry&lt;K,V&gt; e = oldTable[i]; if (e != null) { HashEntry&lt;K,V&gt; next = e.next; // 假设原数组长度为 16，e 在 oldTable[3] 处，那么 idx 只可能是 3 或者是 3 + 16 = 19 int idx = e.hash &amp; sizeMask; if (next == null) newTable[idx] = e; else { HashEntry&lt;K,V&gt; lastRun = e; int lastIdx = idx; // 通过 for 循环找到一个 lastRun 节点，该节点后的所有元素放到一起 for (HashEntry&lt;K,V&gt; last = next; last != null; last = last.next) { int k = last.hash &amp; sizeMask; if (k != lastIdx) { lastIdx = k; lastRun = last; } } newTable[lastIdx] = lastRun; // 这些节点可能分配在另一个链表中，也可能分配到上面的那个链表中 for (HashEntry&lt;K,V&gt; p = e; p != lastRun; p = p.next) { V v = p.value; int h = p.hash; int k = h &amp; sizeMask; HashEntry&lt;K,V&gt; n = newTable[k]; newTable[k] = new HashEntry&lt;K,V&gt;(h, p.key, v, n); } } } } // 将新来的 node 放到新数组中链表的头部 int nodeIndex = node.hash &amp; sizeMask; node.setNext(newTable[nodeIndex]); newTable[nodeIndex] = node; table = newTable;} size()方法 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;size方法通过遍历获得当前分段对象的容量并求和就是ConcurrentHashMap的size，它会多次尝试（3次）非阻塞获得size大小，但如果期间被其他线程修改，（超过3次）则会对每个分段对象强制加锁进行容量获取，这就有点类似JVM的STW感觉。 12345678910111213141516171819202122232425262728293031323334353637383940public int size() { final Segment&lt;K,V&gt;[] segments = this.segments; int size; boolean overflow; // 判断是否整型值溢出 long sum; // 记录被线程修改的次数modCount long last = 0L; int retries = -1; // 自旋次数 try { for (;;) { // 如果自旋达到3次则强制给每个Segment加锁 if (retries++ == RETRIES_BEFORE_LOCK) { for (int j = 0; j &lt; segments.length; ++j) ensureSegment(j).lock(); // force creation } sum = 0L; size = 0; overflow = false; for (int j = 0; j &lt; segments.length; ++j) { // 定位一个segment，获得其修改次数和容量 Segment&lt;K,V&gt; seg = segmentAt(segments, j); if (seg != null) { sum += seg.modCount; int c = seg.count; if (c &lt; 0 || (size += c) &lt; 0) overflow = true; } } if (sum == last) break; last = sum; } } finally { // 如果尝试了三次说明segment被加锁，这里需要解锁 if (retries &gt; RETRIES_BEFORE_LOCK) { for (int j = 0; j &lt; segments.length; ++j) segmentAt(segments, j).unlock(); } } return overflow ? Integer.MAX_VALUE : size;} 因为每个Segment的哈希表容量都是整型，如果数据量足够大，那么多个Segment的size之和就可能超出Integer的最大范围，此时需要通过overflow变量记录，如果容量爆表则只返回Integer.MAX_VALUE。","link":"/2020/05/20/4%20%E6%BA%90%E7%A0%81/Java%E6%BA%90%E7%A0%81%E2%80%94%E2%80%94ConcurrentHashMap%EF%BC%88%E4%B8%80%EF%BC%89/"},{"title":"多线程基础（一）","text":"多线程基础（一） &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;目前的计算机硬件已经突破了摩尔定律，CPU的算力可以达到每秒百亿次甚至更高，个人PC可运行的软件包括操作系统在内，进程数可以是几十个，线程数甚至更多，为了更大程度的发挥计算机自身算力，提高系统效率，更多会采用多线程并发编程的方式实现。 1 进程、线程、并行、并发和同步&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;作为计算机理论的基础知识，进程，是资源分配的最小单位；线程，是CPU调度的最小单位；线程依赖进程而执行；进程资源相互之间是独立的，线程共享同一个进程内的资源。一个程序在计算机上启动并运行，它就需要获得资源，就是一个进程；这个程序在计算上如何被CPU调度运行，就是一个或多个线程。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CPU执行程序实际调用的是指令集，现代计算机基本都是多核CPU，这就允许创建更多的进程，而多个进程同时运行，称作并行；在一个进程内，多个线程同时运行，称作并发；多个线程由于共享进程资源，在并发执行时会竞争资源，需要采取策略避免因为资源竞争而产生脏数据等线程安全问题，这种策略就称为同步。 在线程同步执行过程中，可以对单个线程处理的资源独占加锁，其他线程需要等待独占锁被释放才能继续执行，这个等待的过程称为阻塞。 引用知乎一句话:(https://zhuanlan.zhihu.com/p/54297968) 有人的地方就有江湖， 有并发的地方就有资源竞争， 有竞争的地方就有线程安全， 有线程安全的地方就有同步， 有同步的地方就有锁。 2 多线程的状态和使用 创建状态，Java中创建线程的方式有三种：继承Thread类、实现Runnable接口、实现Callable接口。其中Callable接口属于具有回调结果并可以抛出异常的接口，其实现方法是call；而普通创建线程推荐使用Runnable接口，对外暴露接口少，实现方法是run()，其内部异常只能通过主线程调用setDefaultUncaughtExceptionHandler()来捕获。 就绪状态，执行start之后线程并不会立即执行，而是进入就绪状态等待CPU分配执行时间片。线程是“一次性消费”，并不能被多次执行start方法。 运行状态，线程执行run方法，但执行过程中会被各种原因打断进入阻塞状态。 阻塞状态，线程阻塞是多方面援引，包含主动阻塞，即调用sleep、yield、join等；等待阻塞，调用了wait需要通过notify唤醒；同步阻塞，当前线程执行的资源被其他线程独占。 结束状态，执行结束或异常退出，当前线程被释放或回归线程池。 使用Thread和Runnable创建线程 12345678910111213141516171819202122232425262728293031323334353637383940// 使用Thread执行public class RunTask extends Thread { @Override public void run() { System.out.printf(\"当前线程： %s 执行\", Thread.currentThread().getName()); } public static void main(String[] args) { RunTask runTask = new RunTask(); runTask.setName(\"runTask1\"); runTask.start(); }}// 使用Runnable执行public class RunTask implements Runnable { @Override public void run() { System.out.printf(\"当前线程： %s 执行\", Thread.currentThread().getName()); } public static void main(String[] args) { RunTask runTask = new RunTask(); Thread thread = new Thread(runTask); thread.setName(\"taskRun2\"); thread.start(); }}// 使用lambda执行Runnablepublic class RunTask { public static void main(String[] args) { new Thread(() -&gt; { System.out.printf(\"当前线程 %s 执行\", Thread.currentThread().getName()); }, \"runTask3\").start(); }} 使用Callable执行线程 123456789101112131415161718public class RunTask implements Callable&lt;String&gt; { @Override public String call() { System.out.printf(\"当前线程： %s 执行 \\n\", Thread.currentThread().getName()); return \"runTask4 over\"; } public static void main(String[] args) throws ExecutionException, InterruptedException { RunTask runTask = new RunTask(); FutureTask&lt;String&gt; futureTask = new FutureTask&lt;&gt;(runTask); new Thread(futureTask, \"runTask4\").start(); System.out.println(futureTask.get()); }}// output: // 当前线程： runTask4 执行 // runTask4 over 当然，在Java世界中多线程执行使用最广泛的还是线程池套件，稍后介绍。 3 线程安全机制&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;多线程并发下的线程安全是决定系统性能的重要因素，其核心理念是“要么只读，只要加锁”。在早期的Java版本中，经常需要手写线程池或线程安全代码，往往会造成严重的性能问题，随着Java引入java.util.concurrent包使得并发编程难度降低，且具有更优秀的性能。线程安全问题可以按照以下的顺序思考： 资源向线程内移动，全局的共享资源是产生线程安全的根本援引，并且每个线程在JVM中独立拥有栈帧，如果允许的话可以将资源设置为局部变量，从而屏蔽线程安全问题，这也是ThreadLocal的核心理念。 不可变对象设计，不可变对象因为外部无法修改，它永远只是只读的，所以它总是安全的，例如String类型和Number类型等。 使用线程安全的对象，java原生提供了很多线程安全的类，比如：集合领域使用concurrent下的集合ConcurrentXXX对象或者CopyOnWriteXXX对象等，或者Collections提供的对非线程安全集合创建互斥锁转变为线程安全；StringBuffer等等。 同步机制和锁，涉及并发修改需要考虑实现同步机制，比如synchronized或者锁Lock（ReetrantLock）等。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;回过头来，对线程安全下一个定义，这个定义没有官方的或者权威的定论，个人认为适时且恰当的定义来自《Java并发编程》中的描述：“当多个线程同时访问一个对象时，①如果不考虑这些线程在运行时环境下的调度和交替执行，②也不需要进行额外的同步，③或者在调用方进行任何其他的协调操作，调用这个对象的行为都可以获得正确的结果，那就称这个对象是线程安全的。” 4 多线程特性&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;原子性是多线程的重要特点之一，所谓原子性是指在JVM字节码中的一个执行指令，JVM可以保证一个指令的执行是原子的，但不保证多个原子的指令执行是安全的。与事务中的原子性概念类似，原子指令执行过程中要么全部成功，如果失败则全部退出。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;举例说明，赋值语句int i = 1;是典型的原子指令，而自增++i在字节码中需要分别执行”对i赋值给临时变量”、”数值加1”、”返回结果保存“，对应的指令是”ILOAD、IINC、ISTORE“，虽然每一步都是原子的，但是组合在一起失去了原子性，是非线程安全的。在java.util.concurrent.atom包下，定义了一批原子性的对象，如AtomicBoolean、AtomicInteger、AtomicLong、AtomicReference等等，其内部api涉及诸如自增之类的方法，通过CAS（稍后介绍）实现原子性。 再比如，long类型是8字节64位，在32位操作系统中，对long类型进行赋值就不具有原子性。 123456789101112131415// AtomicInteger的自增方法public final int getAndIncrement() { for(;;) { int current = get(); // 预期值 int next = current + 1; // 新值 if (compareAndSet(current, next)) { // 自旋 return current; } }}// unsafe来自native方法，支持直接调用硬件的原子性能力。public final boolean compareAndSet(int expect, int update) { return unsafe.compareAndSwapInt(this, valueOffset, expect, update);} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可见性，多线程在JVM内存中是基于内存共享的，因为不同的线程独占工作栈，当对共享对象进行修改需要重新刷新到主内存中，以便其他线程也可以基于最新的对象进行操作，当多个线程对同一个共享对象进行操作，容易造成内存同步不及时导致的线程安全问题。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;有序性，通常认为程序的运行是按照字节码顺序执行的，但实际上CPU会对信息进行指令优化，分析哪些动作可以合并进行，从而导致看上去后面的代码先执行了。但是可以保证整体顺序是不变的，不会造成由于优化引起的乱序现象。 5 内存模型和Volatile&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;每个线程都有独占的内存空间（操作栈、本地变量表等等）称为工作内存，同时还存在对所有线程开放的共享区域，称为主内存。线程在本地内存对共享对象修改后，会将结果同步到主内存中，既然有同步操作就会存在时间差，而再次期间的操作对于其他线程来说却是不可见的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Java内存模型（Java Memory Model，JMM）主要目的是定义程序中各种变量的访问规则，变量范围包括实例属性、方法属性等这些可以被共享的变量，而局部变量和方法参数等属于线程本地内存所有，是线程独占的。JMM同时规定：所有的变量保存在主内存中，工作内存保存线程使用的主内存中变量的副本，每个线程对变量的操作都必须在工作内存中完成，而不能直接读写主内存中的数据。 “不能直接读写主内存中的数据” 这一描述对volatile也不例外，虽然看上去volatile描述的对象修改就是在主内存中完成，但事实上，volatile是通过特殊的操作顺序来完成这一点（确保在读操作之前先执行写入操作），实际依然有工作内存的拷贝。 JMM 和 JVM内存区域如堆、栈、方法区等概念属于不同维度对内存的描述，实际上JMM主要是针对JVM在硬件访问层面的描述。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了更好的提供多线程可见性的特性，JMM专门为volatile定制了特殊的访问规则： 保证volatile描述的变量对所有线程的可见性，即保证一致性。 禁止指令重新排序优化。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对所有线程的可见性容易理解，它可以保证不同的线程修改共享变量后能够及时的刷新回内存中而对其他线程可见，从而保证了数据的一致性，但是，并不是volatile描述的变量就是线程安全的，这依赖于变量自身的原子性。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;前文已经提到过，字节码指令在计算机中被CPU调用时并不是严格按照顺序进行，而是会被重新排序进行合并优化，即只要保证结果正确性而不考虑执行顺序的乱序。禁止指令重排序其实是指在关键的动作之前，其依赖的动作已经完成，反应到volatile上就是共享变量在被线程B读取之前已经从线程A修改后的变量写入主内存中。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里提一下有名的单例模式创建机制——双检锁（Double Check Lock, DCL）,它是指在创建单例对象时，在加锁前后都判断对象是否为空，但DCL并不一定是线程安全的。 123456789101112131415161718// 双检锁机制的单例模式public class DCLSingleton { private volatile static DCLSingleton instance; private Object o = new Object(); private DCLSingleton() {} public static DCLSingleton getInstance() { if (instance == null) { // 第一次检查 synchronized (DCLSingleton.class) { if (instance == null) { // 第二次检查 instance = new DCLSingleton(); } } } }} 理论上，双检锁已经把单例创建发挥到极致，但为什么它还可能是线程不安全的？主要问题在于instance = new DCLSingleton();，这句话在字节码层面并不具有原子性，假如线程A创建单例时执行到new DCLSingleton();，此时对象已经在内存中分配了空间但并没有完全完成初始化（还没有创建Object对象），经过volatile修饰此时被刷新回了主内存，而线程B调用了getInstance方法返回的对象也是没有完全初始化的对象，从而引起线程安全问题。 6 Happern-Before原则&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Happen-Before原则又称为先行发生原则，它是JMM中关于两个操作之间的顺序关系的保证！比如A happen before B，就是说在B进行操作前，A的操作造成的影响可以被B观察到。用函数关于可以表示成 F(x,y) 其中 x 先行发生于 y 。如果有三个线程A、B、C，它们之间的函数关系是 F(A,B) 和 F(B,C)，那么可以推导出 F(A,C)是成立的。尽管会进行指令的重排序，到只要保证最终的happen-before规则，其中的重排序是可以被允许的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;“时间上的先发生”并不代表先行发生。 12345private int value;public int getValue() { return value;}public void setValue(int value) {this.value = value;} 假如线程A先执行setValue，线程B后执行getValue，虽然在时间上“A先于B执行”，但并不满足happen-before原则，所以它还是线程不安全的。然而，先行发生也不意味着就会是“时间上的先发生”，有以上描述可以得知，还有可能存在指令重排序。 7 ThreadLocal和引用类型引用类型 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Java中存在四种引用类型，主要适用于在GC机制不同阶段下的引用对象定义： 这里所说的GC机制基于HotSpot VM。 12// 强引用类型Object object = new Object(); 强引用类型，也就是最普通的引用类型，在从GC ROOTS开始对象可达性分析时，如果不在引用链上就会被GC当作垃圾对象回收。一般发生在 Minor GC 和 Major GC阶段。 12// 软引用类型SoftReference&lt;Object&gt; softObject = new SoftReference&lt;&gt;(new Object()); 软引用类型，是比强引用效果弱一些的引用类型，在JVM内存体积膨胀到即将OOM前，GC会统一清理软饮用类型对象，以获得更多的空间避免发生OOM。通常用于缓存中间数据，当OOM前清理了缓存会在后续重新建立，延长了系统运行时间。 12// 弱引用类型WeakReference&lt;Object&gt; weakObject = new WeakReference&lt;&gt;(new Object()); 弱引用类型，是比软引用效果更弱的引用类型，在GC执行Minor GC时就会清理的类型，由于Minor GC发生的频率大于Major GC且发生时间不定，所以被回收的时间也无法确定。通常用于保存容易在内存中消失的对象。 1234// 引用队列ReferenceQueue&lt;Object&gt; phantomQueue = new ReferenceQueue&lt;&gt;();// 虚幻引用类型PhantomReference&lt;Object&gt; phantomObject = new PhantomReference&lt;&gt;(new Object(), phantomQueue); 虚幻引用类型，是效果最弱的引用类型，在创建之后就无法获得引用对象，创建时还需要赋值引用队列ReferenceQueue，当GC回收虚幻引用时，会将该对象加入队列中再回收。它唯一应用的场景就是对象被回收时，通过引用队列被外界知道。 ThreadLocal &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ThreadLocal是一种新的多线程并发安全设计思想的实现，全局变量并发操作总是充满不安全性，那么如果将全局变量设计为线程私有副本呢？ThreadLocal内部通过ThreadLocalMap的哈希表结构来保存变量的副本，哈希表的key就是当前ThreadLocal对象，value是持有的变量副本，而每个ThreadLocalMap对象都是Thread对象私有的。 123456789101112131415161718192021222324252627282930313233// 源码public class Thread implements Runnable { ... ThreadLocalMap threadLocals; ...}public class ThreadLocal&lt;T&gt; { public void set(T value) { Thread t = Thread.currentThread(); ThreadLocal.ThreadLocalMap map = this.getMap(t); // 从当前线程获得ThreadLocalMap if (map != null) { map.set(this, value); // key为当前ThreadLocal对象，value是目标值 } else { this.createMap(t, value); } } public T get() { Thread t = Thread.currentThread(); ThreadLocal.ThreadLocalMap map = this.getMap(t);// 从当前线程获得ThreadLocalMap if (map != null) { // 获得当前对象的键值对，key为当前ThreadLocal对象,value是变量副本 ThreadLocal.ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) { T result = e.value; return result; } } return this.setInitialValue(); }} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当某个频繁操作需要一个临时对象，同时希望避免每次重新分配对象就可以采用这种方式，比如在开发数据库连接时，JDBC连接是容易产生并发问题的，可以将每个连接私有到当前处理的线程中，在通过ThreadLocal调用各自的连接获得不受干扰的执行结果并返回。当线程终止，这些值会被GC回收。 12345678910// 伪代码private static final String JDBC_URL = \"jdbc:///mysql\"private static ThreadLocal&lt;JDBCConnection&gt; connectionHolder = new ThreadLocal&lt;JDBCConnection&gt;(){ @Override public JDBCConnection initialValue() { return DriverManager.getConnection(JDBC_URL); }} 在多线程场景下，同一线程内的跨方法数据传递就需要使用ThreadLocal，如果没有使用它就必须通过返回值和参数形式，增加了相互间耦合度。 ThreadLocal弊端 内存泄漏，在ThreadLocal文档中要求一般声明ThreadLocal&lt;&gt;对象是private static的，通过GC可达性分析算法来看，静态变量会被选定为GC ROOT从而一直存在于内存当中，并不会随着线程结束而释放，所以在线程中使用完ThreadLocal之后必须调用remove()方法进行释放，否则就会在内存中堆积产生内存泄漏问题。 ThreadLocal产生内存泄漏的原因与Entry对象保存的key是否是弱引用WeakReference无关，因为如果key是强引用，当线程执行结束之后，虽然key在Minor GC之后会被回收，但Entry对象还存在于内存中。 脏数据，脏数据之所以会发生，可想而知，其根本原因在于绑定的线程Thread在执行一次之后并没有被销毁，而用于其他线程使用，此场景就是线程池的线程复用情况，假如之后线程不适用set()方法设置值，直接调用get()，那么就会得到之前线程执行后的值，造成脏数据。要解决脏数据问题，还是必须在使用完毕后调用remove()方法释放。 ThreadLocal和Synchronized区别 synchronized ThreadLocal 思路 采用对变量加锁的方式解决兵法问题，是一种“时间换空间”方式。 采用为每个线程提供一个变量的私有副本，来隔离不同线程间的影响，是一种“空间换时间”的方式。 处理方式 加锁解决同步问题。 1.共享数据隔离。2.同一线程内跨组件值传递。 8 Synchronized 和 锁&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上文中谈到了线程安全，那么实际操作中保证线程安全最常用的方式之一就是采用“互斥同步（Mutual &amp; Synchronized）”方式。 8.1 Synchronized8.1.1 synchronized的使用&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;synchronized关键字是最基础也是最常用的同步手段， 通过对一个实例对象或者类对象加锁以达到线程对方法的独占使用，对其他线程阻塞，直到持有锁的线程释放锁，阻塞队列里的线程才能再次竞争锁，即竞争同步方法的使用权。我们知道，synchronized不仅可以显式的指定加锁对象，也可以隐式的直接使用，如果不明确指定加锁对象，那么JVM会通过该方法判断（实例方法 or 类方法）使用当前对象的实例对象或者类对象。 1234567891011121314151617181920212223242526272829class SynchronizedDemo { private Object param; // 全局对象变量 private static int status = 0; // 全局类变量 public synchronized Object getParam() { // 锁定实例对象 return param; } public synchronized void setParam(Object param) { // 锁定实例对象 this.param = param; } public void handleParam() { synchronized (this) { // 锁定实例对象 param = new Object(); } } public static void statusAccumulate() { synchronized (SynchronizedDemo.class) { // 锁定类对象 status += 1; } } public static synchronized int getStatus() { // 锁定类对象 return status; } } 8.1.2 synchronized的字节码层实现原理&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;接下来，让我们从JVM底层字节码来分析synchronized的原理。当源码中使用synchronized锁定一段代码块时，字节码中表现为在该代码块前后添加monitorenter和monitorexit两个指令，这两个指令都依赖于一个明确指向的对象，即持有锁的对象（实例对象或者类对象）。当 JVM 执行monitorenter时，会先判断持有的锁对象是否存在，如果对像不存在则当前线程进入阻塞队列，等待锁被释放与其他阻塞线程竞争锁；如果对象存在，那么就会将锁的计数器递增1，执行monitorexit会将锁计数器递减1，直到锁计数器为0时，当前线程释放锁。也就是说synchronized是可重入的，也就存在线程自身的死锁情况发生。 8.1.3 synchronized的优化和原理&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在历史上JDK5及以前版本中，synchronized都是通过调用硬件层的完全互斥方式解决同步问题，这就遗留了两个主要的性能问题：1.互斥同步会阻塞其他线程，当阻塞线程过多时容易使CPU假死，严重影响系统吞吐率和故障率；2.JVM调用硬件线程能力，就需要不断的从用户态（JVM）和内核态（OS）之间切换，造成了浪费太少了系统压力。通常称这样的互斥同步方式是重量级同步，所以那时候的synchronized在并发数量递增过程中，性能会严重下降，在JDK6之后，HotSpot对并发做了大量优化，其中主要包括：偏向锁、轻量级锁、自适应自旋锁、锁消除、锁升级。 偏向锁 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;偏向锁的目的是，在无竞争环境中减少锁带来的性能开销，在对象的对象头中（对象头保存对象的元数据，包括哈希值、GC分代等信息）存在一个ThreadId属性，该属性用于捆绑调用线程的ID，在偏向锁状态下，如果当前线程访问了新的锁对象，那么会判断是否是同一个线程，即ThreadID是否一致，如果是同一个线程，则不会重复获得锁，从而提高了程序的运行性能；如果不是同一个线程，则会被认为存在竞争，会自动将锁对象由偏向锁升级为轻量级锁。 偏向（Biased），就是偏袒、偏护的意思，意味锁会偏向于第一个获得他的线程。 轻量级锁 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;轻量级锁是相对于传统互斥同步的“重量级锁”而言，它的使用前提是”大部分锁在同步周期内没有竞争“。当线程第一次持有锁对象时，会通过CAS更新持有状态，更新成功则会进入同步方法执行；更新失败表明存在其他线程竞争的状态，如果出现两个以上的锁竞争，那么就会自动升级为重量级锁。 自适应自旋锁 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在当代CPU多核普及的情况下，已经可以支持线程并行，并且并发的方法通常不会占用过长时间，为了避免用户态和内核态的频繁切换，可以将原本要阻塞的线程再持有时间片期间不阻塞，而是采取自旋的方式（死循环）等待前一个线程释放锁。由此看来，虽然节约了线程切换的开销，但也增加了CPU的运算时间，所以，”持锁时间越短，自旋效果越好；相反地，持锁时间越长，自旋效果越差。“因此控制自旋时间或者次数就很有必要，HotSpot默认自旋10次，如果超过10次还没有获得锁则会进入阻塞队列中。当然有个技巧，通常在自旋过程中会附加一些简单的程序用于处理一些业务。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;顾名思义，自适应自旋锁其实就是自旋锁的自旋次数可以满足自适应的能力，自适应的能力由JVM来判断和赋予，通常是通过判断同一个锁在前一次的自旋时间和持锁线程来决定：如果在同一个锁上自旋等待的时间成功获得了锁，并且持锁的线程正在运行，那么再下一次也被认为有可能成功获得锁，所以允许自旋的次数会增加；如果某个锁很少通过自旋获得，那下一次就直接跳过自旋的方式。 锁消除和锁粗化 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于即时编译器JIT来讲，代码本身有同步声明，但在逃逸分析过程中发现不存在共享数据，也就不会存在并发问题，因此编译时不会对该代码添加锁，这称为”锁消除“。当短时间内出现大量重复的锁，比如循环体内添加锁，编译时就会将锁的控制范围外扩至循环体外，这称为”锁粗化“。这两种技术都有效的避免了开发过程中或主观或客观存在的”低效率并发问题“。 8.2 锁对象Lock&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;JDK1.5新增了java.util.concurrent.Lock接口，旨在提供可以在Java类对象层面实现，并且更加灵活的同步方式。顾名思义，该接口就是“对象锁”，是一种源码级别灵活控制的同步方式，下图展现了Lock接口继承关系。 8.2.1 ReentrantLock&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ReentrantLock是Lock接口的重要实现类，称为”可重入锁“，其含义是指一个线程可以对同一个锁对象重复访问，访问一次锁的数量会自增，访问结束锁的数量会自减，当锁容量为0时释放该锁。可重入锁是synchronized的超集，在实现效果上与synchronized一致，只不过前者是代码级别的并发控制，而后者属于语法级别的并发控制。synchronized可以通过语法自动实现加锁和解锁操作，ReentrantLock可以使开发者灵活的加锁，并且需要主动在finally中释放锁，否则就会出现永远被加锁的情况。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;除此之外ReetranLock还支持以下主要功能： 可中断，当锁对象长时间被持有时，等待线程可以选择放弃从而处理其他事情（ tryLock() 方法）。 公平和非公平竞争，synchronized是典型的非公平竞争，当锁对象被释放之后，所有阻塞队列中的线程都可以竞争该锁。ReentrantLock不仅支持非公平竞争，还支持公平竞争，它是指锁对象被释放之后，阻塞队列中的线程按照时间顺序依次获得锁，但需要知道公平竞争将会产生性能问题影响系统吞吐率。ReentrantLock拥有私有属性Sync，而从上图中可以看到Sync的实现类包含公平锁FairSync和非公平锁UnFairSync，默认情况使用UnFairSync，也可以通过构造器来声明使用FairSync。 123456789101112// ReentrantLock重要属性和构造器public class ReentrantLock implements Lock, java.io.Serializable { private final Sync sync; public ReentrantLock() { sync = new NonfairSync(); } public ReentrantLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync(); }} 12345678910111213141516171819202122232425262728293031323334353637383940// synchronizedclass SynchronizedDemo { private Object param; public synchronized Object getParam() { return param; } public synchronized void setParam(Object param) { this.param = param; } public static void main(String[] args) { SynchronizedDemo demo = new SynchronizedDemo(); demo.getParam(); demo.setParam(new Object()); }}// ReentrantLockstatic class SynchronizedDemo { private Object param; public Object getParam() { return param; } public void setParam(Object param) { this.param = param; } public static void main(String[] args) { ReentrantLock lock = new ReentrantLock(); SynchronizedDemo demo = new SynchronizedDemo(); lock.lock(); demo.getParam(); demo.setParam(new Object()); lock.unlock(); }} 多条件绑定，synchronized通过wait()和notify()来实现一个关联条件，当需要更多的关联条件时就不得不添加更多的锁。ReentranLock只需要通过多次调用newCondition()方法即可满足。 8.2.2 多条件绑定——Condition对象&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Condition对象称为线程监视器，支持更加精准的线程调度动作。传统的线程调度通过Object对象的 wait() 方法和 notify() 或 notifyAll() 对象实现，这也是synchronized互斥锁支持的线程调度方式，但其中存在两个问题： notifyAll会唤醒当前所有阻塞线程竞争锁，是一种非公平竞争，唤醒粒度面向整个线程队列。 notify只会唤醒阻塞队列中的一个线程，置于唤醒哪一个线程则是由线程调度器决定，用户并不能指定线程唤醒，虽然唤醒粒度小但无法指定线程。 对于ReentranLock可以通过注册多条监视器来显式的、准确的实现线程调度，以下分别以简化版经典的生产者消费者问题对二者做以比较。 描述：假设有action1和action2两个动作，两动作需要前后依次循环执行。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354// synchronized版本实现public class SyncProducerAndConsumer { private static volatile AtomicInteger state = new AtomicInteger(0); // 调度状态，保证原子和可见 private static class Conductor { public void action1() throws InterruptedException { synchronized (this) { while (state.get() != 0) { // 状态不为0时等待 this.wait(); } // 状态为0时处理 System.out.println(Thread.currentThread().getName() + \" execute\"); // 执行结束前修改状态并唤醒其他线程 state.set(1); notifyAll(); // 此例中只有两个子线程，也可使用notify() } } public void action2() throws InterruptedException { synchronized (this) { while (state.get() != 1) { this.wait(); } System.out.println(Thread.currentThread().getName() + \" execute\"); state.set(0); notifyAll(); } } } public static void main(String[] args) { Conductor conductor = new Conductor(); new Thread(() -&gt; { try { for (int i = 0; i &lt; 10; i++) { conductor.action1(); } } catch (InterruptedException e) { e.printStackTrace(); } },\"Thread1\").start(); new Thread(() -&gt; { try { for (int i = 0; i &lt; 10; i++) { conductor.action2(); } } catch (InterruptedException e) { e.printStackTrace(); } },\"Thread2\").start(); }} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566// ReentrantLock实现public class LockProducerAndConsumer { private static volatile AtomicInteger state = new AtomicInteger(0); private static class Conductor { private final Lock lock = new ReentrantLock(); // 可重入锁 private final Condition action1Monitor = lock.newCondition(); // 注册action1监视器 private final Condition action2Monitor = lock.newCondition(); // 注册action2监视器 public void action1() throws InterruptedException { lock.lock(); try { while (state.get() != 0) { action1Monitor.await(); // 状态不为0，等待 } // 执行 System.out.println(Thread.currentThread().getName() + \" execute\"); state.set(1); action2Monitor.signal(); // 指定唤醒action2监视器的线程 } catch (Exception e) { e.printStackTrace(); } finally { lock.unlock(); } } public void action2() throws InterruptedException { lock.lock(); try { while (state.get() != 1) { action2Monitor.await(); } System.out.println(Thread.currentThread().getName() + \" execute\"); state.set(0); action1Monitor.signal(); } catch (Exception e) { e.printStackTrace(); } finally { lock.unlock(); } } } public static void main(String[] args) { Conductor conductor = new Conductor(); new Thread(() -&gt; { try { for (int i = 0; i &lt; 10; i++) { conductor.action1(); } } catch (InterruptedException e) { e.printStackTrace(); } },\"Thread1\").start(); new Thread(() -&gt; { try { for (int i = 0; i &lt; 10; i++) { conductor.action2(); } } catch (InterruptedException e) { e.printStackTrace(); } },\"Thread2\").start(); }} 总结： Synchronized使用 wait() 、notify()、notifyAll()方法实现线程调度。调度粒度大，无法指定固定线程的调度。 ReentranLock使用Condition提高的await()、signal()、signalAll()方法实现线程调度，可以准确唤醒指定线程。 8.2.3 Lock-Free：CAS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;笼统的从策略来讲，锁可以分为以 synchronized 和 reentrant lock 为代表的互斥阻塞式锁，称为悲观锁，悲观锁总是认为如果不采取同步措施那就一定会发生并发安全问题，所以无论变量是否存在竞争都加锁；另一类是以CAS为代表的非阻塞式锁，称为乐观锁，乐观锁认为无论如何先执行操作，如果操作的结果不符合预期再考虑补救措施。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CAS （Compare and Swap），是一种Lock Free编程类型，通过编码的形式保证并发安全，这个概念不仅应用于硬件层面，同样应用于软件层面，比如通过编程语言从源码层面解决，或者某些数据库也支持保证数据一致性。CAS执行包含三个要素，数据的内存位置M、预期值P、新值N，当且仅当M的数值与P相等才会更新新值N，否则就不执行更新操作，可以通过有限制次数的自旋来重试，直到更新成功或者退出。整个过程是具有原子性的，不会被其他线程中断。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在Java9以前，Java API支持的CAS属于sun.misc.Unsafe，只有通过Bootstrap类加载器加载的类才有使用权限，因此类库中的类比如JUC可以使用，用户类可以通过反射机制获得访问和调用权限，Java9面向用户开放了VarHandle类用于操作CAS。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;再前篇多线程特性——原子性时已经介绍过AtomicInteger类中自增操作使用CAS，这里就不再獒述。 8.3 死锁&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;设计不当的多线程任务很容发生死锁现象，而且死锁的发生通常意味着系统资源走向枯竭出现假死甚至宕机现象。死锁的发生主要是因为多线程在持有锁的执行过程中阻塞获得其他的锁，此时这个锁被其他线程持有并且尝试获得各自持有的锁。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class DeadLockDemo { public static void main(String[] args) { Object obj1 = new Object(); Object obj2 = new Object(); // 线程t1持有obj1对象锁，并尝试获得obj2对象锁 new Thread(() -&gt; { synchronized (obj1) { try { System.out.println(Thread.currentThread().getName() + \" get obj1\"); System.out.println(Thread.currentThread().getName() + \" try to get obj2,waiting\"); TimeUnit.SECONDS.sleep(1); synchronized (obj2) { System.out.println(Thread.currentThread().getName() + \" get obj2\"); } } catch (InterruptedException e) { e.printStackTrace(); } } }, \"t1\").start(); // 线程t2持有obj2对象锁，并尝试获得obj1对象锁 new Thread(() -&gt; { synchronized (obj2) { try { System.out.println(Thread.currentThread().getName() + \" get obj2\"); System.out.println(Thread.currentThread().getName() + \" try to get obj1,waiting\"); TimeUnit.SECONDS.sleep(1); synchronized (obj1) { System.out.println(Thread.currentThread().getName() + \" get obj1\"); } } catch (InterruptedException e) { e.printStackTrace(); } } }, \"t2\").start(); }}// 最终t1和t2产生死锁，系统进入无限阻塞/* * output:t1 get obj1t1 try to get obj2,waitingt2 get obj2t2 try to get obj1,waiting */ 通常解决死锁的方式，要么采用乐观锁通过有限次数的自旋解决；要么对互斥锁设置超时时限，超时的线程释放锁并重新进入阻塞队列。","link":"/2020/05/27/5%20%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E5%A4%9A%E7%BA%BF%E7%A8%8B%EF%BC%88%E4%B8%80%EF%BC%89/"},{"title":"多线程基础（二）","text":"多线程基础（二） 9 线程池及其源码&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在传统多线程创建中，用户通过Thread、Runnable或者Callable手动创建线程并执行，这样做会产生两个重要的问题： 线程的创建和销毁是会造成系统资源消耗的，如果过度创建线程执行有可能造成“过度切换”问题。 线程是重要资源，过度的创建不仅使执行效率降低，还容易引起内存消耗增加的问题，对于重要的大对象创建，通常采用“池”的概念来管理，达到线程复用的目的。 9.1 Executor&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Executor是java.util.concurrent下的重要异步执行接口，它的继承关系如下： 其定义了一个接收Runnable对象的方法executor，,该方法接收一个Runable实例，它用来执行一个任务，任务即一个实现了Runnable接口的类。 123public interface Executor { void execute(Runnable command);} 9.2 ExecutorService&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ExecutorService是扩展子接口，提供了线程生命周期相关的方法。 12345678public interface ExecutorService extends Executor { void shutdown(); // 平滑关闭，收回线程池 boolean isShutdown(); //判断关闭 // 提交线程，返回Future &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task); Future&lt;?&gt; submit(Runnable task); ...} submit支持Callable和Runnable执行并返回Future对象，通过Future对象内的方法可以感知和获得执行结果。 1234567891011121314public interface Future&lt;V&gt; { // 中断任务运行 boolean cancel(boolean mayInterruptIfRunning); boolean isCancelled(); // 判断是否线程执行结束有返回结果 boolean isDone(); // 获得返回结果 V get() throws InterruptedException, ExecutionException; // 带超时的返回结果 V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;} 需要说明的是，如果使用get方法直接获取Future结果，调用线程将会阻塞，直到执行线程执行结束。可以使用isDone非阻塞式的监听执行情况并最后通过get获得结果。 9.3 Executors&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Executors类，提供了一系列工厂方法用于创建线程池，返回的线程池都实现了ExecutorService接口。主要包含以下几种类型： Executors.newCachedThreadPool();，创建可以缓存的线程池，如果有空闲线程则进行复用，否则再未达到线程个数阈值时创建新的线程。常用于创建生命周期短的任务线程，如果空闲池中的线程超过空闲阈值（默认60s）则会被线程池关闭并清理。 Executors.newFixedThreadPool(int);，创建固定线程个数的线程池，原理基本与newCachedThreadPool类似，只不过在线程不足时不会再次创建线程（不存在空闲池概念），内部通过阻塞队列来管理调用线程。 Executors.newScheduledThreadPool(int)；，创建一个具有调度能力的线程池，其线程可以实现定时任务等，相比于Timer更加安全且功能丰富。 Executors.SingleThreadExecutor()；，创建一个单例线程，即任意时间线程池中只存在一个线程，按照任务提交顺序执行，调用之后即被销毁。 这些静态工厂方法虽然可以创建不同类型的线程池，但通过源码可以看到，都是调用ThreadPoolExecutor来构造目标线程池。 1234567891011121314151617181920212223242526272829303132public class Executors { // 非调度能力的都是通过ThreadPoolExecutor来构造目标线程池 public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()); } public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;())); } public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;()); } // 调度能力通过构造ScheduledThreadPoolExecutor构造 public static ScheduledExecutorService newSingleThreadScheduledExecutor() { return new DelegatedScheduledExecutorService (new ScheduledThreadPoolExecutor(1)); } public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) { return new ScheduledThreadPoolExecutor(corePoolSize); }} 9.4 ThreadPoolExecutor&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在孤尽的《阿里巴巴Java开发手册》并发编程规约的第4条指出：“线程池不允许使用Executors创建，而是通过ThreadPoolExecutor的方式创建，这样的好处是，可以使开发人员更加明确线程池的运行规则，规避资源耗尽的风险”，从企业安全风险把控角度来看，这样的规约是严谨且可被接纳的，当然如果开发人员对原理“烂熟于心”，可以通过Executors快速创建线程池也未尝不可。 构造器 线程池在构建时需要注意以下参数：核心线程数corePoolSize、最大线程数maximumPoolSize、心跳周期keepAliveTime、心跳周期时间单位TimeUnit、维护线程调度的阻塞队列BlockingQueue&lt;Runnable&gt;、线程工厂类ThreadFactory和拒绝策略RejectedExecutionHandler。 1234567891011121314151617181920212223242526272829// 构造函数public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler); // 自定义参数构建线程池BlockingQueue queue = new LinkedBlockQueue(10);// 线程工厂UserThreadFactory threadFactory = new ThreadFactory((task) -&gt; { String threadName = getThreadName(); Thread thread = new Thread(task,threadName); return thread;});// 自定义符合业务能力的拒绝策略UserRejectHandler rejectHandler = new RejectedExecutionHandler((task, executor) -&gt; { // 具体拒绝策略实现});// 或者使用提供的拒绝策略RejectedExecutionHandler rejectHandler = new ThreadPoolExecutor.AbortPolicy();// 默认策略// 核心线程数5、最大线程数20、心跳周期60sThreadPoolExecutor threadPool = new ThreadPoolExecutor(5, 20, 60, TimeUnit.SECONDS, queue, threadFactory, rejectHandler ); 拒绝策略 当不需要用户自定义拒绝策略时，可以使用ThreadPoolExecutor提供的拒绝策略： ThreadPoolExecutor.AbortPolicy：默认策略，丢弃任务并抛出异常RejectedExecutionException。 ThreadPoolExecutor.DiscardPolicy：丢弃任务不抛出异常。 ThreadPoolExecutor.DiscardOldestPolicy：丢弃等待队列中时间等待最长的任务，加入新任务。 ThreadPoolExecutor.CallerRunsPolicy：调用run方法时绕过线程池直接执行。 阻塞队列 阻塞队列是并发编程的重要数据结构，主要用于保存工作线程Worker，在后续详细介绍，此处列出常用阻塞队列实现对象： LinkedBlockingQueue：单向FIFO，因为基于链表实现，所以在队列频繁出队、入队过程中效率高所以是最常用的阻塞队列，当不显式定义该队列对象容量时，默认大小为Integer.MAX_VALUE。 ArrayBlockingQueue：单向FIFO，基于数组实现的有界队列，由于出队、入队涉及数组移位所以效率不如阻塞队列，当大容量时会严重影响吞吐量。 PriorityBlockingQueue：基本数据结构同LinkedBlockingQueue，区别在于队列内部排序（即调度优先级）由比较器Comparator维护。 SynchronousQueue：同步队列，也是“生产者-消费者”模型队列，产生的队列元素入队之后需要被另一端消费出队从而完成一个周期。 线程池对象状态属性 123456789101112131415161718192021// 将整型32位ctl按位拆分，高3位用于表示线程池状态，低29位用于表示工作线程数// 当线程数超过整型低29位可表示范围时，采用长整型AtomicLong代替// 比如，高三位2进制可以表示0~7不同的值private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));private static final int COUNT_BITS = Integer.SIZE - 3;private static final int COUNT_MASK = (1 &lt;&lt; COUNT_BITS) - 1;// runState is stored in the high-order bitsprivate static final int RUNNING = -1 &lt;&lt; COUNT_BITS; //private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;private static final int STOP = 1 &lt;&lt; COUNT_BITS;private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS;// Packing and unpacking ctl// 取反、与操作，获得当前线程池状态private static int runStateOf(int c) { return c &amp; ~COUNT_MASK; }// 与操作，获得工作线程数private static int workerCountOf(int c) { return c &amp; COUNT_MASK; }// 高3位和低29位或运算，合并成一个值private static int ctlOf(int rs, int wc) { return rs | wc; } ctl高3位表示了线程池的运行状态，包括： RUNNING：运行状态，接收新任务，111； SHUTDOWN：关闭状态，不接收新任务但可以继续执行队列内已有任务，000； STOP：停止状态，拒接接收新任务并中断所有任务，001； TIDYING：整顿状态，表示所有任务已被终止，010； TERMINATED：结束状态，表示已清理完所有现场，011； execute方法 execute方法是顶级接口Executor中唯一定义的方法，也是执行Runnable线程的方法。通过官方注释首先明确处理步骤： 如果运行的线程少于corePoolSize，会将入参任务通过调用addWorker启动执行，当然addWorker会检查运行状态和数量从而通过返回false防止在不应该添加线程的情况下添加线程。 当任务被添加进入工作队列，为了避免在第一次检查之后线程销毁或者线程池关闭，所以需要进行双重检查以确保进入工作队列，否则会执行拒绝策略。 如果无法添加任务进入工作队列，那么就执行拒绝策略。 12345678910111213141516171819202122public void execute(Runnable command) { if (command == null) throw new NullPointerException(); int c = ctl.get(); // 步骤1 if (workerCountOf(c) &lt; corePoolSize) { if (addWorker(command, true)) return; c = ctl.get(); } // 步骤2 if (isRunning(c) &amp;&amp; workQueue.offer(command)) { int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); } // 步骤3 else if (!addWorker(command, false)) reject(command);} addWorker方法 addWorker是线程池的重要方法，首先检查是否可以添加新任务，如果可以添加并最终添加成功会返回true；当线程池状态不为RUNNING或者县城创建失败会返回false。 特别说明：源码中用到了改进后的goto语句，关于goto语句争论很多，但需要说明的是虽然它弊端很多，但在多重循环中，恰当的使用可以快速跳过不必要的操作，节省时间提高效率，这也是为什么Java作为后起之秀依然没有彻底放弃的原因，反而进行了改进，我个人支持恰当的使用goto，虽然这样违反编码规约（微笑脸）。 （源码中都使用了，我在工作中是不是也可以使用的挺直腰杆，理直气壮 [doge]） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273private final HashSet&lt;Worker&gt; workers = new HashSet&lt;&gt;(); // 工作线程容器private final ReentrantLock mainLock = new ReentrantLock(); // 线程池主锁，敏感操作时使用/** * @Param firstTask:外部启动线程时构造的第一个线程 * @Param core:为true时表示新增工作线程，判断RUNNING状态下线程数是否达到corePollSize * 为false时表示新增工作线程，判断RUNNING状态下线程是否少与maximumPoolSize */private boolean addWorker(Runnable firstTask, boolean core) { retry: for (int c = ctl.get();;) { // 判断运行状态是否为RUNNING if (runStateAtLeast(c, SHUTDOWN) &amp;&amp; (runStateAtLeast(c, STOP) || firstTask != null || workQueue.isEmpty())) return false; for (;;) { // 最大线程数不能超过2^29，否则低29位无法保存线程 if (workerCountOf(c) &gt;= ((core ? corePoolSize : maximumPoolSize) &amp; COUNT_MASK)) return false; // CAS增加工作线程数 if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // 反复读取状态判断 // 如果线程池SHUNTDOWN，跳转至retry标签执行 if (runStateAtLeast(c, SHUTDOWN)) continue retry; } } // 开始创建工作线程 boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try { w = new Worker(firstTask); // 使用线程工厂创建线程并封装进Worker对象中 final Thread t = w.thread; if (t != null) { // 敏感操作，持有线程池主锁，避免添加和启动时产生并发 final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { int c = ctl.get(); // 当属于RUNNING状态或者STOP状态下firstTask为空，说明需要添加工作线程 if (isRunning(c) || (runStateLessThan(c, STOP) &amp;&amp; firstTask == null)) { if (t.isAlive()) // 如果添加工作线程时，该线程已经被创建则抛出异常 throw new IllegalThreadStateException(); workers.add(w); // 设定线程池最大并发个数 int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; } } finally { mainLock.unlock(); // 新增完毕，解锁 } // 工作线程添加成功，启动线程 if (workerAdded) { t.start(); workerStarted = true; } } } finally { if (! workerStarted) addWorkerFailed(w); } return workerStarted;} 最后，总结一下使用线程池的注意点： 必须通过线程池创建和调用线程，不能自定义线程执行，方便维护和管理。 根据实际业务场景使用ThreadPoolExecutor构建线程池。 对每个线程应该赋名，方便jstack分析。 9.5 线程异常捕获&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在子线程中抛出的异常是无法被主线程探知的，需要通过UncaughtExceptionHandler线程异常处理接口来捕获异常并进行处理。 12345678910111213141516171819202122232425262728public class Test { public static void main(String[] args) { Thread t = new Thread(() -&gt; { // 主动抛出异常 throw new RuntimeException(\"sub Thread excute error\"); }, \"subThread\"); // 线程启动前设置异常捕获器 t.setUncaughtExceptionHandler(new DefinedUncaughtExceptionHandler()); t.start(); try { TimeUnit.SECONDS.sleep(1); // 主线程sleep 1s } catch (Exception e) { e.printStackTrace(); } System.out.println(\"main thread over\"); } // 自定义线程异常捕获器 private static class DefinedUncaughtExceptionHandler implements Thread.UncaughtExceptionHandler { @Override public void uncaughtException(Thread t, Throwable e) { System.out.println(t.getName() + \" find Exception：\" + e.getMessage()); } }}","link":"/2020/05/27/5%20%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E5%A4%9A%E7%BA%BF%E7%A8%8B%EF%BC%88%E4%BA%8C%EF%BC%89/"},{"title":"多线程基础（三）","text":"多线程基础（三） 10 AQS和JUC简介10.1 AQS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AbstracQueuedSynchronizer类（简称AQS）是一个用于构建锁和同步器的抽象类，JUC中的很多同步器实现就是以AQS为基础创建而来。它解决了在实现同步器时涉及的大量细节问题，比如等待队列使用FIFO、获取操作和释放操作等。基于AQS构建的同步器不仅极大地减少了实现工作，而且也不必处理竞争问题，它只能在一个时刻发生阻塞，从而降低上下文切换开销，提高吞吐量。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AQS还提供管理同步器中的状态，该状态是一个整型值，可以又具体的实现器赋予状态的含义：比如ReentrantLock中用状态表示线程获取锁的次数，比如Semaphore表示剩余信号量，比如FutureTask表示任务状态等。该状态是volatile的，可以通过getState()、setState()和compareAndSetState()进行操作。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AQS定义了两种资源共享方式，即：Exclusive独占方式，如ReentranLock和Share共享方式如JUC中的Semaphore、CountDownLatch等，不同的同步器实现采用的资源共享方式不同。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AQS内部采用CLH队列实现阻塞队列，CLH（Craig，Landin，and Hagersten）队列是一个虚拟的双向队列，即没有队列实体，而是通过定义内部类Node节点来实现锁的分配。 AQS获取和释放 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当有新的线程需要竞争资源，首先通过获取state的方式竞争，如果成功持有状态则直接调用线程方法；如果持有失败则进入CLH双向队列末尾等待。AQS采用模板方法设计模式，在抽象类中提供 aquire() 方法执行该过程，而首次尝试获得state的 tryAcquire() 方法则交给具体实现的同步器。 123456789101112131415161718192021222324252627public final void acquire(int arg) { if (!tryAcquire(arg) &amp;&amp; // 尝试获得state,获得成功返回true，否则返回false acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) // 没有获得state，则进入阻塞队列末尾并再次获得锁 selfInterrupt(); // 最终没有获得锁，中断当前线程，等待阻塞队列的调度}// 交由具体同步器的实现类完成protected boolean tryAcquire(int arg) { throw new UnsupportedOperationException();}private Node addWaiter(Node mode) { Node node = new Node(mode); for (;;) { Node oldTail = tail; if (oldTail != null) { node.setPrevRelaxed(oldTail); if (compareAndSetTail(oldTail, node)) { // 将当前节点假如CLH队列末尾 oldTail.next = node; return node; } } else { initializeSyncQueue(); } }} 接下来可以看看ReentranLock的非公平锁和Semaphore是如何实现 tryAcquire() 方法的： 12345678910111213141516171819202122232425262728293031323334353637383940414243/* ReentranLock */// Sync extends AbstractQueuedSynchronizerstatic final class NonfairSync extends Sync { private static final long serialVersionUID = 7316153563782823691L; protected final boolean tryAcquire(int acquires) { return nonfairTryAcquire(acquires); }}@ReservedStackAccessfinal boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (compareAndSetState(0, acquires)) { // CAS设置state状态 setExclusiveOwnerThread(current); // 如果成功，则将当前线程设为互斥锁 return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(\"Maximum lock count exceeded\"); setState(nextc); return true; } return false;}/* Semaphore */public boolean tryAcquire() { return sync.nonfairTryAcquireShared(1) &gt;= 0;}final int nonfairTryAcquireShared(int acquires) { for (;;) { int available = getState(); int remaining = available - acquires; if (remaining &lt; 0 || compareAndSetState(available, remaining)) return remaining; }} 自定义同步器的实现 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;自定义实现一个锁，该锁内部维护一个私有的同步器对象，锁的实现是简写，主要体现同步器的实现。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class DefinedLock implements Lock { private final Synchronizer sync; public DefinedLock() { sync = new Synchronizer(); } // lock和unlock属于简写，这里只为强调自定义同步器 // state = 1表示获得，0释放。 @Override public void lock() { sync.acquire(1); } @Override public void unlock() { sync.release(0); } // Lock接口其他实现方法，省略 // 推荐使用内部同步器对象 private class Synchronizer extends AbstractQueuedSynchronizer { @Override protected boolean tryAcquire(int arg) { boolean flag = false; if (compareAndSetState(0, arg)) { // CAS给state赋值 setExclusiveOwnerThread(Thread.currentThread()); // 当前线设置为独占锁 flag = true; } return flag; } @Override protected boolean tryRelease(int arg) { if (isHeldExclusively()) { // 判断当前线程是否是独占锁的线程 setState(arg); setExclusiveOwnerThread(null); } return false; } @Override protected boolean isHeldExclusively() { return getExclusiveOwnerThread() == Thread.currentThread(); } }} 10.2 JUC&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;JUC（java.util.concurrent）指的是jdk5之后官方提供的同步包，其中包含了众多实际开发中用到的概念，包括： 锁Lock及其实现类； AQS及其衍生实现； 原子特性对象如AtomicInteger、AtomicLong等； 并发集合如ConcurrentHashMap等(同步集合)、CopyOnWriteList等(COW集合)； 线程池Executor及其实现类ThreadPoolExecutor、ExecutorService等。 阻塞队列BlockingQueue及其实现类。 可以说JUC是现代多线程并发开发的集大成者，前文已经陆续介绍了JUC的相关组件（锁、原子特性、线程池、AQS、并发集合跳转“源码标签”或”Java源码系列收看”）。接下来最后简要介绍常用的并发器工具类：CountDownLatch、Semaphore、CyclicBarrier 10.2.1 CountDownLatch&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（官方解释）允许一个或多个线程等待直到在其他线程中执行的一组操作完成的同步辅助。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CountDownLatch是一个基于执行时间的同步类，其中通过维护一个全局的 count 变量计数，其操作都具有原子性，主要通过 countDown() 和 await() 两个方法实现。 从字面意思理解，CountDown就是整数自减，直到count=0，Latch就是门闩的意思，意味着count=0时上闩关门。 CountDownLatch是一次性使用的，当count=0时，对象即不可复用，需要重新创建。 使用CountDownLatch的await方法可以保证多个不同步线程最终可以统一回归到一个时间点继续向下执行。 12345678910111213141516171819202122232425262728293031323334353637383940414243public class CountDownLatchDemo { private static final Random random = new Random(); public static void main(String[] args) throws InterruptedException { // 初始计数10 CountDownLatch countDownLatch = new CountDownLatch(10); // 创建10个线程，每个线程执行完毕，计数器自减 for (int i = 1; i &lt;= 10; i++) { new Thread(() -&gt; { try { TimeUnit.SECONDS.sleep(random.nextInt(5)); // 每个线程休眠随机时间秒 System.out.println(Thread.currentThread().getName() + \" execute done\"); countDownLatch.countDown(); } catch (InterruptedException e) { e.printStackTrace(); } }, \"Thread-\" + i).start(); } // 等待count=0，即10个线程执行完毕，此时主线程阻塞 countDownLatch.await(); // 主线程阻塞结束，所有线程都执行完毕，开始继续执行主线程 System.out.println(\"count = \" + countDownLatch.getCount()); System.out.println(\"main thread end\"); }}/* * output:Thread-8 execute doneThread-4 execute doneThread-6 execute doneThread-5 execute doneThread-1 execute doneThread-2 execute doneThread-3 execute doneThread-10 execute doneThread-7 execute doneThread-9 execute donecount = 0main thread end */ 10.2.2 Semaphore&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（官方解释）Semaphore用于限制可以访问某些资源（物理或逻辑的）的线程数目，他维护了一个许可证集合，有多少资源需要限制就维护多少许可证集合。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;简而言之，Semaphore可以控制线程的并发数量，它初始化定义一个信号量上限，通过调用 acquire() 方法获得令牌和 release() 释放令牌。内部通过维护一个整型的 state 状态来表示信号量（即可用令牌个数），acquire()时会判断state，从而确定是获取令牌亦或阻塞等待获取令牌，获取成功state自减。release()时会将state自增，并从阻塞队列全部唤醒等待线程竞争获得令牌，重复acquire过程，这其中都是通过CAS修改state计数。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class SemaphoreDemo { private static final Random random = new Random(); private static final int SEMAPHORE_PERMIT = 3; public static void main(String[] args) { Semaphore semaphore = new Semaphore(SEMAPHORE_PERMIT); final Semaphore tmpSemaphore = semaphore; // 创建10个线程依次获取令牌，执行完毕释放令牌 for (int i = 1; i &lt;= 10; i++) { new Thread(() -&gt; { try { tmpSemaphore.acquire(); // 获取令牌 // 打印当前可用令牌数和阻塞线程个数 System.out.printf(\"current token count: %d, wait queue count: %d\", tmpSemaphore.availablePermits(), tmpSemaphore.getQueueLength()); TimeUnit.SECONDS.sleep(random.nextInt(5)); // 每个线程休眠随机时间秒 System.out.println(Thread.currentThread().getName() + \" execute done\"); } catch (InterruptedException e) { e.printStackTrace(); } finally { tmpSemaphore.release(); // 释放令牌 } }, \"Thread-\" + i).start(); } System.out.println(\"main thread done\"); // 使用完毕，销毁信号量对象 while (semaphore.getQueueLength() == 0 &amp;&amp; semaphore.availablePermits() == SEMAPHORE_PERMIT) { semaphore = null; } }}/* * output:current token count: 0, wait queue count: 0current token count: 0, wait queue count: 0current token count: 0, wait queue count: 0main thread doneThread-5 execute donecurrent token count: 0, wait queue count: 6Thread-2 execute donecurrent token count: 0, wait queue count: 5Thread-4 execute doneThread-3 execute donecurrent token count: 0, wait queue count: 4current token count: 0, wait queue count: 3Thread-10 execute donecurrent token count: 0, wait queue count: 2Thread-8 execute donecurrent token count: 0, wait queue count: 1Thread-7 execute doneThread-9 execute donecurrent token count: 1, wait queue count: 0Thread-6 execute doneThread-1 execute done */ Semaphore和CountDownLatch都属于信号量类型工具，并且初始化时都规定了信号量上限h，两者的区别在于： Semaphore内部维护state表示可用令牌个数，从0开始增长到h；CountDownLatch内部维护计数器count，从h递减到0结束。 Semaphore通过acquire获得令牌state-1，通过release释放令牌state+1，state是可变化的；CountDownLatch通过countDown实现count递减直到为0； Semaphore可复用，CountDownLatch不可复用，再次使用需要重新创建对象。 CountDownLatch可以通过阻塞保证多个线程最终回归同一时间点并继续向下执行，Semaphore不可以。 10.2.3 CyclicBarrier&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（官方解释）允许一组线程全部等待彼此达到共同屏障点的同步辅助。 循环阻塞在涉及固定大小的线程方的程序中很有用，这些线程必须偶尔等待彼此。 屏障被称为循环 ，因为它可以在等待的线程被释放之后重新使用。 简而言之，就是会让所有线程都等待完成后才会继续下一步行动。线程通过调用 await() 方法表示当前线程已经到达“集合点”位置，进入阻塞，直到所有线程都到达“结合点”位置后，继续各自执行。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class CyclicBarrierDemo { private static final Random random = new Random(); public static void main(String[] args) { // CyclicBarrier初始化集合数量，并允许设置集合后的动作 CyclicBarrier cyclicBarrier = new CyclicBarrier(3, () -&gt; { System.out.println(\"arrive the safe point together\"); }); // 创建3个线程，每个线程依次执行，通过调用await()阻塞等待其他线程集合后方可继续执行 for (int i = 1; i &lt;= 3; i++) { new Thread(() -&gt; { try { // 第一次执行和等待集合 TimeUnit.SECONDS.sleep(random.nextInt(5)); System.out.println(Thread.currentThread().getName() + \" arrival\"); cyclicBarrier.await(); // 等待其他线程到达集合点 // 第二次执行和等待集合 TimeUnit.SECONDS.sleep(random.nextInt(5)); System.out.println(Thread.currentThread().getName() + \" next arrival\"); cyclicBarrier.await(); // 最后一次执行和等待集合 TimeUnit.SECONDS.sleep(random.nextInt(5)); System.out.println(Thread.currentThread().getName() + \" finished\"); cyclicBarrier.await(); } catch (InterruptedException | BrokenBarrierException e) { e.printStackTrace(); } }, \"Thread-\" + i).start(); } }}/* * output:Thread-3 arrival Thread-1 arrivalThread-2 arrivalarrive the safe point togetherThread-2 next arrivalThread-3 next arrivalThread-1 next arrivalarrive the safe point togetherThread-2 finishedThread-3 finishedThread-1 finishedarrive the safe point together */ CyclicBarrier 与 CountDownLatch 区别： CountDownLatch 是一次性的，CyclicBarrier 是可循环利用的。 CountDownLatch 参与的线程的职责是不一样的，各自执行结束之后，最终回归初始线程（主线程）。CyclicBarrier 参与的线程职责是一样的。 11 阻塞队列&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BlockingQueue即阻塞队列，基于ReentrantLock实现线程安全。他是“生产者-消费者”模型的主要调度容器。最为基础接口它实现类如下图所示： ArrayBlockingQueue：是一个用数组实现的有界阻塞队列，此队列按照先进先出（FIFO）的原则对元素进行排序。支持公平锁和非公平锁。 LinkedBlockingQueue：一个由链表结构组成的有界队列，此队列的长度为Integer.MAX_VALUE。 PriorityBlockingQueue： 一个支持线程优先级排序的无界队列，默认自然序进行排序，也可以自定义实现compareTo()方法来指定元素排序规则，不能保证同优先级元素的顺序。 DelayQueue： 基于PriorityBlockingQueue实现延迟获取的无界队列，在创建元素时，可以指定多久才能从队列中获取当前元素。只有延时期满后才能从队列中获取元素。 SynchronousQueue： 不存储元素的阻塞队列，每一个put操作必须等待take操作，否则不能添加元素。支持公平锁和非公平锁。是一种单线程模型的阻塞队列。 LinkedBlockingDeque： 一个由链表结构组成的双向阻塞队列。队列头部和尾部都可以添加和移除元素，多线程并发时，可以将锁的竞争最多降到一半。 阻塞队列的方法 异常反馈 null反馈 阻塞 超时 压入 add() offer() put() offer() 弹出 remove() poll() take() poll() 队首元素 element peek 生产者消费者简单实现 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;生产者-消费者是经典的同步模型，包含：多生产多消费、多生产单消费、单生产多消费和单生产单消费。以下以单生产多消费介绍。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public class ProducerAndConsumerDemo { private static final BlockingQueue&lt;String&gt; queue = new LinkedBlockingQueue&lt;&gt;(); private static final int INVENTORY = 50; public static void main(String[] args) { new Thread(new Producer()).start(); new Thread(new Consumer(), \"Consumer1\").start(); new Thread(new Consumer(), \"Consumer2\").start(); } // 生产者对象 private static class Producer implements Runnable { @Override public void run() { int count = 1; while (queue.size() &lt; INVENTORY) { try { TimeUnit.MILLISECONDS.sleep(500); String goods = \"goods-\" + count++; queue.put(goods); System.out.printf(\"produce %s \\n\", goods); } catch (InterruptedException e) { e.printStackTrace(); } } System.out.println(\"# Produce over #\"); } } // 消费者对象 private static class Consumer implements Runnable { @Override public void run() { while (true) { try { TimeUnit.SECONDS.sleep(2); System.out.printf(\"%s consume %s \\n\", Thread.currentThread().getName(), queue.take()); } catch (InterruptedException e) { e.printStackTrace(); } } } }}/* * outputproduce goods-1 produce goods-2 produce goods-3 Consumer1 consume goods-1 Consumer2 consume goods-2 produce goods-4 produce goods-5 produce goods-6 produce goods-7 Consumer1 consume goods-3 Consumer2 consume goods-4 produce goods-8 produce goods-9 produce goods-10 produce goods-11 Consumer1 consume goods-5 Consumer2 consume goods-6... */ 12 协程和纤程&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Java中的线程是一种系统级线程，它运行在用户态（User），从用户态通过JVM调起内核态的线程运行，这其中每个线程的创建，以及在用户态和内核态的状态切换开销都是会消耗可观资源的，为了节省资源开销Java采用线程池的方式管理有限的线程资源。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;协程（Coroutine），是一种轻量级的线程(lightweight thread)，它只存在于用户态，一个线程可以包含多个协程，目前在GO语言中广泛使用，通过go关键字就可以创建一个协程，它的工作原理是：协程负责执行线程中的一段代码片段，当需要切换协程时，会将当前协程执行的上下文进行保存，同时让出CPU资源，恢复被调用协程的上下文继续执行。与线程最大的区别在于：协程只工作在用户态，减少了和内核态切换的开销；线程由系统调度器执行，抢占式调度策略难以控制线程的有序调用，协程可以完全依赖开发者控制调用。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Windows平台运行或者由微软命名的协程称为纤程（Fiber），从2018年开始，由OpenJDK主导的Loom项目旨在从JVM层面实现协程，但作为程序语言的“老者”，考虑到众多程序的稳定，Java想要在协程上做出突破是比较困难的，但作为未来并发编程的主导者，协程的上岸只是时间问题（Java的很多问题都是时间问题）。目前Java生态中三方库形式的协程支持可以看Quasar 项目，JVM层面的协程实现可以关注AJDK（Alibaba JDK）、作为JVM语言的佼佼者，Kotlin也通过编译器和类库提供支持。 作为Java宣传的保留项目，Oracle这一次喊出了：“Code like sync，Work like async。” 总结： 使用协程的优点： 完全由应用程序控制。 运行在用户态，减少与内核态转换的系统开销。 基于上下文切换协程，转换速度快。 因为协程调度可控，所以更符合人类语义且避免了线程安全。 协程的缺点： 因为在线程中创建和运行，无法充分利用多核CPU能力，即CPU密集型。 协程的阻塞必将导致所属线程的阻塞。","link":"/2020/05/27/5%20%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E5%A4%9A%E7%BA%BF%E7%A8%8B%EF%BC%88%E4%B8%89%EF%BC%89/"},{"title":"Java源码——CopyOnWriteArrayList","text":"CopyOnWriteArrayList &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Copy-On-Write（COW）技术是Linux底层采用的策略，称为写时复制技术，它的核心思想是针对读多写少的场景，多线程读时不上锁，只有当写时会对数据进行拷贝并加锁，在产生的副本上修改，并最终合入源数据，在JUC中，CopyOnWriteArrayList是针对ArrayList的COW版本，CopyOnWriteArraySet是针对Set的COW版本，以前者为例，在读多写少的场景时，比如订单地址、缓存系统等，读取列表是支持多线程同时访问的，并不会产生读阻塞（与读写锁的区别），当修改列表数据时会创建当前集合的副本，加锁并修改，最终将数据集成到集合中。由此可以看出： CopyOnWriteArrayList在多线程读操作时，与ArrayList一致，不会产生读阻塞。 修改时创建原集合大小的副本，会占用更多的系统内存，如果并发写过大时就有可能产生频繁的Major GC，甚至出现OOM。此时需要考虑替换其他集合。 存在并发写操作时，读操作仍旧只能读取旧数据。 成员变量 123456public class CopyOnWriteArrayList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable { final transient Object lock = new Object(); // 内置监视器，使用synchronized private transient volatile Object[] array; //数组容器} get 方法 12345678// 通过下标正常读取数组中的数据public E get(int index) { return elementAt(getArray(), index);}static &lt;E&gt; E elementAt(Object[] a, int index) { return (E) a[index];} add 、set 和 remove 方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869// 不指定索引新增public boolean add(E e) { synchronized (lock) { // 写入时加锁 Object[] es = getArray(); int len = es.length; es = Arrays.copyOf(es, len + 1); // 创建当前数组的副本并修改元素 es[len] = e; setArray(es); return true; }}// 指定索引新增，原理相同public void add(int index, E element) { synchronized (lock) { Object[] es = getArray(); int len = es.length; if (index &gt; len || index &lt; 0) throw new IndexOutOfBoundsException(outOfBounds(index, len)); Object[] newElements; int numMoved = len - index; if (numMoved == 0) newElements = Arrays.copyOf(es, len + 1); else { newElements = new Object[len + 1]; System.arraycopy(es, 0, newElements, 0, index); System.arraycopy(es, index, newElements, index + 1, numMoved); } newElements[index] = element; setArray(newElements); }}// set方法，同理public E set(int index, E element) { synchronized (lock) { // 修改前加锁 Object[] es = getArray(); E oldValue = elementAt(es, index); if (oldValue != element) { es = es.clone(); // 创建副本 es[index] = element; setArray(es); } return oldValue; }}// 删除方法同理public E remove(int index) { synchronized (lock) { Object[] es = getArray(); int len = es.length; E oldValue = elementAt(es, index); int numMoved = len - index - 1; Object[] newElements; if (numMoved == 0) newElements = Arrays.copyOf(es, len - 1); else { newElements = new Object[len - 1]; System.arraycopy(es, 0, newElements, 0, index); System.arraycopy(es, index + 1, newElements, index, numMoved); } setArray(newElements); return oldValue; }} COWIterator 迭代器 我们知道每一个集合都拥有迭代器，CopyOnWriteArrayList采用COWIterator迭代器，它是通过使用当前数组的快照实现，也就是说对并发写不敏感。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364static final class COWIterator&lt;E&gt; implements ListIterator&lt;E&gt; { private final Object[] snapshot; // 当前列表的快照对象 private int cursor; //迭代器的游标 COWIterator(Object[] es, int initialCursor) { cursor = initialCursor; snapshot = es; } public boolean hasNext() { return cursor &lt; snapshot.length; } public boolean hasPrevious() { return cursor &gt; 0; } @SuppressWarnings(\"unchecked\") public E next() { if (! hasNext()) throw new NoSuchElementException(); return (E) snapshot[cursor++]; } @SuppressWarnings(\"unchecked\") public E previous() { if (! hasPrevious()) throw new NoSuchElementException(); return (E) snapshot[--cursor]; } public int nextIndex() { return cursor; } public int previousIndex() { return cursor - 1; } // 不支持remove 、 set 、 add方法 public void remove() { throw new UnsupportedOperationException(); } public void set(E e) { throw new UnsupportedOperationException(); } public void add(E e) { throw new UnsupportedOperationException(); } // lambda方法的实现 @Override public void forEachRemaining(Consumer&lt;? super E&gt; action) { Objects.requireNonNull(action); final int size = snapshot.length; int i = cursor; cursor = size; for (; i &lt; size; i++) action.accept(elementAt(snapshot, i)); } }","link":"/2020/05/11/4%20%E6%BA%90%E7%A0%81/Java%E6%BA%90%E7%A0%81%E2%80%94%E2%80%94CopyOnWriteArrayList/"},{"title":"设计模式（一）","text":"设计模式（一） 1 设计模式概述和设计原则&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;设计模式最早由建筑行业提出，随着从业经验的不断累积和迭代，如何按照既定的原则和目标建造出安全、稳定、美观的大楼，这就是设计模式。一砖一瓦一楼阁，平地间筑起高楼。软件工程同样有自身的一套前人总结的设计经验，代表了最佳的实践，是软件开发过程中面临的一般问题的解决方案。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GoF(Erich Gamma、Richard Helm、Ralph Johnson 和 John Vlissides) 四人合著了一本经典书籍——《Design Pattern》，该书大获成功，书中介绍了23种设计模式，实际软件工程演变到现金已经远远超过23种，但这23种设计模式仍然可以普遍应用于实际开发过程中，也是设计模式学习的基础。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;设计模式按照使用场景可以划分为：创建型设计模式、结构型设计模式和行为型设计模式： 创建型设计模式：（5种） 工厂模式、抽象工厂模式、单例模式、原型模式、建造者模式 结构型设计模式：（7种） 适配器模式、装饰模式、代理模式、外观模式、桥接模式、组合模式、亨元模式 行为型设计模式：（11种） 策略模式、模版模式、观察者模式、迭代器模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;设计模式是解决问题的手段，而指导设计模式的思想称为设计原则：单一职责原则、接口隔离原则、依赖倒置原则、里氏替换原则、迪米特法则、开闭原则、组织/聚合原则，共7种（组合/聚合是比较新的设计原则） 单一职责原则：作为高级面向对象语言，Java的单一职责性原则很重要，作为类，一个类只负责完成一个功能领域中的职责；作为方法，一个方法只实现单一功能。单一职责原则是高内聚、低耦合的核心思想。 接口隔离原则：作为抽象层级的顶级设计，接口必须具备隔离原则，即尽可能的创建功能单一的接口，而不创建”大而全“的总接口，这样可以保证实现类只重写相关的方法，而不用重写不相关的方法。 在该设计中Bird接口可以满足Parrot和Eagle，可是Ostrich鸵鸟也属于鸟类，但是鸵鸟并不会飞 (fly()方法)，但是鸵鸟不得不重写 fly 方法，所以需要对Bird接口重新设计。 依赖倒置原则：抽象不应该依赖于细节，细节应当依赖于抽象。换言之，要针对接口编程，而不是针对实现编程。在对象关联关系中尽量引用层次高的抽象层类而不要用具体类 123456789interface Shape{}class Circle implements Shape{}class Rectangle implements Shape{}class Triangle implements Shape{}// 引用类型使用Shape而非具体实例类型Shape shape1 = new Circle();Shape shape2 = new Rectangle();Shape shape3 = new Triangle(); 里氏替换原则：所有引用基类（父类）的地方必须能透明地使用其子类的对象，换而言之基类调用的地方换成子类也应该可以正常调用，所以可以认为里氏替换原则是规定子类尽量避免重写父类方法，如果不得不重写父类方法可以采用关联关系，而非继承关系。 迪米特法则：也称为”最少知道原则“或”最少知识原则“，设计开发时应当尽可能少地与其他实体发生相互作用，换而言之，一个类A与另一个类B存在关联关系，A调用B的方法，B应该只提供简单的对外接口而屏蔽内部实现的复杂性。迪米特法则可降低系统的耦合度，使类与类之间保持松散的耦合关系。 开闭原则：程序应当对扩展开放，对修改关闭。即应尽量在不修改原有代码的情况下进行扩展。抽象化设计是开闭原则的关键。 组合/聚合原则：如果直接继承基类，因为继承将基类的实现细节暴露给子类从而破环基类的封装性；如果基类的实现发生改变，则子类的实现也不得不发生改变。所以程序的设计应当避免使用继承关系而使用组合/聚合的方式替换，也因此可以避免里氏替换原则的问题。 2 关联关系及其UML表示&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;前文提到的组合/聚合就是对象之间关联关系的其中之二，在面向对象中，关联关系是表示对象与对象之间的关系，它包括：泛化（继承）、实现、关联、依赖、组合、聚合六种。 泛化（继承）和实现： 在UML中继承关系通常称为泛化关系，表示父类与子类之间的继承关系；实现，顾名思义就是类与接口之间的关系。 123public class Rectangle extends Shape{} // 继承关系public class Human implements Mammal{} // 实现关系 关联和依赖： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关联：可以表示为单项关联和双向关联，对于类A和B，B作为A的成员变量使用，那么A和B属于单项关联关系；对于类A和B，B作为A的成员变量，A作为B的成员变量，那么A和B属于双向关联关系。 1234567891011121314class Customer { private List&lt;Product&gt; products; private Account account;}class Product { private int id; private int count;}class Account { private Customer customer; private double balance;} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;依赖：对于类A和B，B的对象作为A的方法入参或局部变量使用，那么A和B属于依赖关系。 12345678910class Coffee {}class CoffeeFactory { public void produceCoffee(Coffee coffee) {} //或者 public void produceCoffee() { Coffee coffee = new Coffee(); }} 组合和聚合 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;组合：对于类A和B，B作为A的成员变量使用，并且自身对象内部创建B对象实例使用，也就是说A对象负责B对象的生命周期。 车辆是最终需要的对象，而车辆对象离不开轮胎对象、发动机对象、变速箱对象和方向盘对象。所以它们是组合关系。 1234567891011121314151617181920212223242526272829class Tyre {}class Engine {}class Steering {}class GearBox {}class Automobile { private Tyre tyre = new Tyre(); private Engine engine = new Engine(); private Steering steering = new Steering(); private GearBox gearBox = new GearBox(); // 或者采用其他方法创建 private Tyre tyre; private Engine engine; private Steering steering; private GearBox gearBox; public Automobile() { init(); } private void init() { tyre = new Tyre(); engine = new Engine(); steering = new Steering(); gearBox = new GearBox(); }} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;聚合：在表现形式上聚合和关联关系一致，对于类A和B，B作为A的成员变量使用，并且通过set等方法或构造器赋值，通过外部传入供A使用。 1234567891011121314151617181920212223242526272829class Screen{}class Keyboard{}class Mouse{}class Computer { private Screen screen; private Keyboard keyboard; private Mouse mouse; // 构造器传入 public Computer(Screen screen, Keyboard keyboard, Mouse mouse) { this.screen = screen; this.keyboard = keyboard; this.mouse = mouse; } // 或者set方法传入 public void setScreen(Screen screen){ this.screen = screen; } public void setKeyboard(Keyboard keyboard) { this.keyboard = keyboard; } public void setMouse(Mouse mouse) { this.mouse = mouse; }} 组合和聚合除了外部对象创建方式不同之外，还体现在对象主体与组成对象的强依赖关系：对于汽车对象必须要有轮胎、发动机等对象来构成一台完整的车；对于电脑来说，键盘、显示器和鼠标并不是必须的，缺少其中之一并不影响电脑的构成，打个比方，如果将图示内容的聚合对象改为CPU、内存、硬盘等，那么就构成了组成关系。 总的来说，组合的耦合性高于聚合。","link":"/2020/06/05/9%20%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%88%E4%B8%80%EF%BC%89/"},{"title":"设计模式（二）","text":"设计模式（二） 3 创建型设计模式 工厂模式、抽象工厂模式、单例模式、原型模式、建造者模式 3.1 静态工厂方法（Static Factory）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;严格来说，静态工厂方法并不属于设计模式，因为其严重破坏了OCP原则。静态工厂方法常用if…else if或者switch来完成。 题引：通过静态工厂创建不同风格的咖啡。 1234567891011121314151617181920212223242526272829303132333435public interface ICoffee {}class Americano implements ICoffee{}class Espresso implements ICoffee{}class Latte implements ICoffee{}class Cappucino implements ICoffee{}enum CoffeeType { Americano, Espresso, Latte, Cappucino}public class CoffeeStaticFactory { public static ICoffee produceCoffee(CoffeeType coffeeType) { ICoffee coffee; switch(coffeeType) { case Americano: coffee = new Americano(); break; case Espresso: coffee = new Espresso(); break; case Latte: coffee = new Latte(); break; case Cappucino: coffee = new Cappucino(); break; default: coffee = null; } return coffee; }} 3.2 工厂模式（Factory）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在工厂模式中，我们在创建对象时不会对客户端暴露创建逻辑，并且是通过使用一个共同的接口来指向新创建的对象。扩展性高，如果想增加一个产品，只要扩展一个工厂类即可。 题引：东方人喝不惯黑咖啡（Americano和Espresso）而喜欢喝加奶咖啡（Cappucino和Latte），所以需要根据地域需求来整改方案，通过黑咖啡工厂专门创建黑咖啡，当地黑咖啡需求低则降低产能，同理，当地加奶咖啡需求高则提高加奶工厂的产能； 123456789101112131415161718192021222324252627public class BlackCoffeeFactory { public ICoffee produceCoffee(CoffeeType coffeeType) { if (isNotBlackCoffee(CoffeeType coffeeType)) { throw new IllegalArguments(\"not black coffee\"); } return CoffeeStaticFactory.produceCoffee(coffeeType); } private boolean isNotBlackCoffee(CoffeeType coffeeType) { return coffeeType.equals(CoffeeType.Americano) || coffeeType.equals(CoffeeType.Espresso); }}public class MilkCoffeeFactory { public ICoffee produceCoffee(CoffeeType coffeeType) { if (isNotMilkCoffee(CoffeeType coffeeType)) { throw new IllegalArguments(\"not milk coffee\"); } return CoffeeStaticFactory.produceCoffee(coffeeType); } private boolean isNotMilkCoffee(CoffeeType coffeeType) { return coffeeType.equals(CoffeeType.Latte) || coffeeType.equals(CoffeeType.Cappucino); }} 应用场景：在任何需要创建复杂对象的地方，都可以使用工厂方法模式。特别是对象有统一接口，或者创建的对象比较复杂，属于重对象比如线程池、数据连接池等。 3.3 抽象工厂模式（Abstract Factory）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;抽象工厂模式围绕超级工厂创建其他工厂，属于工厂类的抽象，或者说工厂的工厂。 题引：对于饮料类产品，中国人除了喝咖啡以为最爱喝的应该是茶了，所以需要一个创建茶的工厂来提供茶，同时咖啡工厂继续运营。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485public interface ITea{}class GreenTea implements ITea{}class RedTea implements ITea{}class OolongTea implements ITea{}enum TeaType { GreenTea, RedTea, OolongTea}public class TeaStaticFactory { public static ITea produceCoffee(TeaType teaType) { ITea tea; switch(teaType) { case GreenTea: tea = new GreenTea(); break; case RedTea: tea = new RedTea(); break; case OolongTea: tea = new OolongTea(); break; default: tea = null; } return tea; }}// 抽象工厂public abstract AbstractDrinkFactory { ICoffee produceCoffee(CoffeeType coffeeType); ITea produceTea(TeaType teaType);}public class TeaFactory extends AbstractDrinkFactory{ @Override public ICoffee produceCoffee(CoffeeType coffeeType) { throw new IllegalArguments(\"not support produce coffee\"); } @Override public ITea produceTea(TeaType teaType) { return TeaStaticFactory.produceTea(teaType); }}public class BlackCoffeeFactory extends AbstractDrinkFactory { @Override public ITea produceTea(TeaType teaType) { throw new IllegalArguments(\"not support produce tea\"); } @Override public ICoffee produceCoffee(CoffeeType coffeeType) { if (!isBlackCoffee(CoffeeType coffeeType)) { throw IllegalArguments(\"not black coffee\"); } return CoffeeStaticFactory.produceCoffee(coffeeType); } private boolean isBlackCoffee(CoffeeType coffeeType) { return coffeeType.equals(CoffeeType.Americano) || coffeeType.equals(CoffeeType.Espresso); }}public class MilkCoffeeFactory extends AbstractDrinkFactory { @Override public ITea produceTea(TeaType teaType) { throw new IllegalArguments(\"not support produce tea\"); } @Override public ICoffee produceCoffee(CoffeeType coffeeType) { if (!isMilkCoffee(CoffeeType coffeeType)) { throw IllegalArguments(\"not milk coffee\"); } return CoffeeStaticFactory.produceCoffee(coffeeType); } private boolean isMilkCoffee(CoffeeType coffeeType) { return coffeeType.equals(CoffeeType.Latte) || coffeeType.equals(CoffeeType.Cappucino); }} 3.4 原型模式（Prototype）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;原型模式是用于创建重复的对象，同时又能保证性能。说白了就是对象的克隆，创建一个与目标对象“一模一样“的另一个对象（哈希值不同）。 题引：通过原型模式创建不同咖啡的副本。 1234567891011121314151617181920212223242526272829303132333435public interface ICoffee {}class Americano implements ICoffee, Clonable { private String coffeeBean; // 咖啡豆品种 private String coffeeBakeDuration; // 咖啡豆烘焙时间 private String extractTech; // 提取工艺 public Americano(String coffeeBean, String coffeeBakeDuration, String extractTech) { this.coffeeBean = coffeeBean; this.coffeeBakeDuration = coffeeBakeDuration; this.extractTech = extractTech; } @Override public Object clone() { return super.clone(); }}// 其他咖啡类型一致，省略class CoffeePrototype { publc static generatePrototype(ICoffee coffee) { ICoffee prototype = null; try { prototype = (ICoffee) coffee.clone(); } catch (CloneNotSupportedException e) { // 克隆异常处理 } return prototype; }}// 伪代码调用ICoffee coffee = new Americano(\"cuba coffee bean\", \"10 hours\", \"cold brew\");ICoffee coffeePrototype = CoffeePrototype.generatePrototype(coffee);coffee == coffeePrototype; // false 在使用原型模式时，要注意深浅克隆，当对象成员变量使用基本数据类型时，可以直接使用clone方法，因为clone方法默认是浅克隆，如例子中的Americano；当成员变量含有非基本数据类型的对象，那么需要将对象进行深克隆，即对非基本数据类型的对象进一步递归clone实现。 深克隆举例，使用设计模式（一）中关联关系的例子 12345678910111213141516171819202122232425262728293031323334353637383940414243class Customer implements Clonable { private int id; private List&lt;Product&gt; products; private Account account; // getter and setter // 成员变量非基本数据类型，需要采用深克隆 @Override public Object clone() { Customer customer = (Customer) super.clone(); Account account = this.account.clone(); List&lt;Product&gt; products = new ArrayList&lt;&gt;(); // 递归列表深克隆Producet对象 for (Product p : this.products) { Product prototype = (Product) p.clone(); products.add(prototype); } customer.setProducts(products); customer.setAccount(account); return customer; }}class Product implements Clonable { private int id; private int count; // 基本数据类型直接使用浅克隆 @Override public Object clone() { return super.clone(); }}class Account implements Clonable { private double balance; // 基本数据类型直接使用浅克隆 @Override public Object clone() { return super.clone(); }} 3.5 建造者模式（Builder）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于一个复杂对象，特别是构造器入参多的大对象，可以采用构建着模式逐步录入参数从而构建出一个完整的对象。建造者模式一般分为经典的”原型创建“和”链式创建“。在《effective java》一书中，bloch推荐使用链式写法，更直观且更容易理解。 原型创建的原理是把一个复杂对象拆分成多个部分分别创建，最终组装成一个整体完整对象。一般原型创建分为四个步骤： Product：最终被建造的对象。 Builder：被建造的对象需要遵从的统一标准接口。 Director：建造Product的统一对象。 BuilderImpl：具体的接口实现者。 题引：通过建造者模式创建具有不同性格、体型、样貌等特征的人 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990class Human { private String feature; // 样貌 private String size; // 体型 private String sex; // 性别 private String intelligence; // 智力 // getter and setter}// 造人的标准规范public interface IHumanBuilder { void buildFeature(); void buildSize(); void buildSex(); void buildIntelligence(); Human buildHuman();}// 创建一个身材矮小但聪明的男人class HumanOneBuilderImpl implements IHumanBuilder { private Human human; public HumanOneBuilderImpl(Human human) { this.human = human; } void buildFeature() { human.setFeature(\"random\"); } void buildSize() { human.setSize(\"small\"); } void buildSex() { human.setSex(\"male\"); } void buildIntelligence() { human.setIntelligence(\"smart\"); } Human buildHuman() { return human; }}// 创建一个体型高硕，美丽的女人class HumanTwoBuilderImpl implements IHumanBuilder { private Human human; public HumanOneBuilderImpl(Human human) { this.human = human; } void buildFeature() { human.setFeature(\"beauty\"); } void buildSize() { human.setSize(\"tall\"); } void buildSex() { human.setSex(\"female\"); } void buildIntelligence() { human.setIntelligence(\"random\"); } Human buildHuman() { return human; }}// 对象的组装过程是有先后顺序的，该类就是统一的按照一定顺序规则构建对象的指挥者class Director { public Human createHuman(IHumanBuilder builder) { builder.buildFeature(); builder.buildSize(); builder.buildSex(); builder.buildIntelligence(); return builder.buildHuman(); }}// 测试调用，伪代码Director director = new Director();HumanOneBuilderImpl humanBuilder1 = HumanOneBuilderImpl(new Human());HumanTwoBuilderImpl humanBuilder2 = HumanTwoBuilderImpl(new Human());Human human1 = director.createHuman(humanBuilder1);Human human2 = director.createHuman(humanBuilder2); &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;经典的建造者模式是程式化的，在理解和使用上并不直观，如果对象足够复杂，每个Builder接口的实现类也会复杂。所以就引申出了”链式建造者模式“。 链式建造者模式 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;链式建造者模式通常在复杂对象内部创建一个专属的Builder内部类，对于共性特征可以不必从客户端输入，而只关注可变的参数。我个人觉得，最最最大的优点在于，目前IT行业公司都有各自的 ”编码规范“，基本上规范内容都会要求方法参数个数应该低于N个（我司编码规范要求方法入参必须保证不大于5个），所以在创建对象时如果不是使用不优雅的setter方法而是采用构造器创建，那入参问题必然会打破编码要求。所以，链式建造者可以满足，不违反编码规范的前提下创建复杂对象 [doge] 题引，通过链式建造者模式，创建具有不同特征的人。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788// 对上一例子中的Human进行改造public class Human { private String feature; // 样貌 private String size; // 体型 private String sex; // 性别 private String intelligence; // 智力 private String color; // 肤色 private String nation; // 国籍 private String language; // 语言 private Date birthDate; // 出生日期 // 私有化构造器，只能通过构造器创建对象 private Human(Builder builder) { feature = builder.feature; size = builder.size; sex = builder.sex; intelligence = builder.intelligence; color = builder.color; nation = builder.nation; language = builder.language; birthDate = builder.birthDate; } // getter and setter // 内部构造器对象 public static class Builder { private String feature; private String size; private String sex; private String intelligence; private String color; private String nation; private String language; private Date birthDate; public Builder feature(String feature) { this.feature = feature; return this; } public Builder size(String size) { this.size = size; return this; } public Builder sex(String sex) { this.sex = sex; return this; } public Builder intelligence(String intelligence) { this.intelligence = intelligence; return this; } public Builder color(String color) { this.color = color; return this; } public Builder nation(String nation) { this.nation = nation; return this; } public Builder language(String language) { this.language = language; return this; } public Builder birthDate(Date birthDate) { this.birthDate = birthDate; return this; } // 最终构建完整对象 public Human build() { return new Human(this); } }}// 使用，伪代码Human.Builder builder = new Human.Builder();Human human = builder.feature(\"beauty\").size(\"average\").sex(\"female\").intelligence(\"genius\") .color(\"yellow\").nation(\"China\").language(\"Chinese\").birthDate(new Date()) .build(); 12345678910111213141516171819202122232425262728293031323334353637383940414243// 假如我们要创建中国人，其实color、nation和language已经确定，并且Date与创建时间有关，所以这些参数可以被Builder内部创建好。// 创建中国人，Builder修改如下， 伪代码public class Human { // 各种参数和构造器实现，省略... public static class Builder { private String feature; private String size; private String sex; private String intelligence; private String color = \"yellow\"; private String nation = \"China\"; private String language = \"Chinese\"; private Date birthDate = new Date(); public Builder feature(String feature) { this.feature = feature; return this; } public Builder size(String size) { this.size = size; return this; } public Builder sex(String sex) { this.sex = sex; return this; } public Builder intelligence(String intelligence) { this.intelligence = intelligence; return this; } // 最终构建完整对象 public Human build() { return new Human(this); } }}// 使用Human.Builder builder = new Human.Builder();Human chinese = builder.feature(\"beauty\").size(\"average\").sex(\"female\").intelligence(\"genius\").build(); 3.6 单例模式（Singleton）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;单例模式是为了保证在对象任何被调用的时刻，对象状态永远不变，即不会在内存中存在两个相同的对象。单例模式分为多种，以下将从最简单的单例模式入手不逐步找出问题和解决问题，最终推出 “完美单例模式”。 饿汉式单例 123456789public class Singleton { private static final INSTANCE = new Singleton(); private Singleton(){} public static Singleton getInstance() { return INSTANCE; }} 饿汉式单例模式总结如下： 利用常量特性，在类加载结束后就会创建INSTANCE对象，并通过常量方法返回该对象。 类加载后创建，所以不支持懒加载。 类加载后创建，所以线程安全。 懒汉式单例 专门吐槽一下，也不知道最早是谁翻译的“懒汉”、“饿汉”，真的是醉了，懒汉式应该是Lazy的翻译，或许叫延时加载更合适。“饿汉式”还真没找到出处，或许是 “Hungry” 或者 ”Starve“ 吧？ 123456789101112public class Singleton { private static instance = null; private Singleton(){} public static Singleton getInstance() { if (Objects.isNull(instance)) { instance = new Singleton(); } return instance; }} 懒汉式总结： 初始null，在被调用时创建，所以支持懒加载； 使用全局变量instance，所以在getInstance方法时容易产生线程安全问题，所以非线程安全； 反射不安全、反序列化不安全。 懒汉式单例改进——双检锁+防反射攻击+防反序列化攻击 123456789101112131415161718192021222324252627public class Singleton { private static instance = null; private Singleton(){ // 防止反射攻击 if (!Objects.isNull(instance)) { throw new IllegalStatException(\"不支持反射创建对象\"); } } // 双检锁 public static Singleton getInstance() { synchronized(Singleton.class) { // 非空判断前加锁 if (Objects.isNull(instance)) { synchronized(Singleton.class) { // 对象创建前加锁 instance = new Singleton(); } } } return instance; } // 保证反序列化对象为当前实例对象 private Object readResolve() { return instance; }} 总结： 序列化安全、反射安全、延迟加载； 在《多线程》系列中介绍了双检锁的线程安全问题，这里简单提一下，虽然对创建instance的动作采取双检锁加锁，但是instance = new Singleton()动作并不具有原子性，可能存在以下情况：假设A线程和B线程同时访问getInstance()方法，A线程检测到当前instance = null所以同步开始创建第一个实例，当动作执行到instance = new Singleton()时，因为不具有原子性，可能出现这样情况：JVM在内存中创建了Singleton的内存空间，所以instance不为空，但是对象创建过程中需要经过”加载 -&gt; 链接 -&gt; 初始化“，在链接过程中会对instance分配内存空间但是对象本身不存在，此时线程B检测到instance不为null，直接获得没有完全初始化的Singleton实例，导致出现线程安全问题。 完美单例1——懒汉式静态类单例 12345678910111213141516171819202122public class Singleton { private Singleton(){ // 防止反射攻击 if (!Objects.isNull(instance)) { throw new IllegalStatException(\"不支持反射创建对象\"); } } public static Singleton getInstance() { return InstanceHolder.INSTANCE; } // 保证反序列化对象为当前实例对象 private Object readResolve() { return instance; } private static final class InstanceHolder { private static final Singleton INSTANCE = new Singleton(); }} 总结： 利用静态内部类特性，当首次调用静态内部类常量时才进行创建，所以支持懒加载； 利用常量特点，JVM加载类加载器&lt;clinit&gt;时，可以保证线程阻塞单线程执行，所以线程安全； 反射安全、反序列化安全。 完美单例2——枚举单例 123456public enum Singleton { INSTANCE}// 伪代码调用Singleton instance = Singleton.INSTANCE; 总结：枚举单例是最简单的方式，也是《effective java》推荐使用的单例模式，原因在于JVM天然的保证枚举类型的单一性： 枚举对象与静态内部类一样，在首次访问时才会初始化对象，所以支持懒加载; 枚举对象属于常量，所以可以保证线程安全； 枚举类内部重写readObject()方法，所以反序列化安全。 1234567891011121314151617181920public abstract class Enum&lt;E extends Enum&lt;E&gt;&gt; implements Comparable&lt;E&gt;, Serializable { /** * Sole constructor. Programmers cannot invoke this constructor. * It is for use by code emitted by the compiler in response to * enum type declarations. */ protected Enum(String name, int ordinal) { this.name = name; this.ordinal = ordinal; } /** * prevent default deserialization */ private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException { throw new InvalidObjectException(\"can't deserialize enum\"); }} 反射包内的构造器对象Constructor对通过构造器创建对象实例特别约束了枚举类型，如果创建实例是枚举类型将抛出 IllegalArgumentException 异常，所以反射安全。 12345678910111213141516171819public T newInstance(Object ... initargs) throws InstantiationException, IllegalAccessException, IllegalArgumentException, InvocationTargetException { if (!override) { Class&lt;?&gt; caller = Reflection.getCallerClass(); checkAccess(caller, clazz, clazz, modifiers); } // 如果创建实例是枚举类型将抛出 IllegalArgumentException 异常 if ((clazz.getModifiers() &amp; Modifier.ENUM) != 0) throw new IllegalArgumentException(\"Cannot reflectively create enum objects\"); ConstructorAccessor ca = constructorAccessor; // read volatile if (ca == null) { ca = acquireConstructorAccessor(); } @SuppressWarnings(\"unchecked\") T inst = (T) ca.newInstance(initargs); return inst; }","link":"/2020/06/08/9%20%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%88%E4%BA%8C%EF%BC%89-%20%E5%88%9B%E5%BB%BA%E5%9E%8B%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"title":"设计模式（三）","text":"设计模式（三） 4 结构型设计模式 适配器模式、装饰模式、代理模式、外观模式、桥接模式、组合模式、亨元模式 4.1 适配器模式（Adapter）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;适配器模式解决，一个类的接口变换成客户端所期待的另一种接口，从而使原本因接口不匹配而无法在一起工作的两个类能够在一起工作。适配器模式分为：类适配器、对象适配器和缺省适配器三种实现。 题引：中国民用交流电220V，50Hz。美国民用交流电110V，60Hz。国内生产的某款手机充电器标准是输入220V电压输出5V电压，所以如果拿着国产的手机去美国显然是无法使用的，此时就需要一款转换器加装在国产手机充电器上，使能够正常在美国使用。 类适配器 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;类适配器采用继承被适配对象和实现目标接口来完成，如上图，要想国产手机充电器可以在美国使用就需要使用美国标准的110V电压输入和国产充电器5V输出充电使用。 12345678910111213141516171819202122232425262728// 美国充电器标准public interface IAmericaCharger { void inVoltage110(String in); // 110V void outVoltage5(); void charge();}// 中国充电器标准public interface IChinaCharger { void inVoltage220(String in); // 220V void outVoltage5(); void charge();}// 国产某手机充电器public class ChinaCharger implements IChinaCharger { public void inVoltage220(String in) {} public void outVoltage5() {} public void charge() {}}// 国产充电器的适配器，适配美国标准使用public class ChargerAdapter extends ChinaCharger implements IAmericaCharger { public void inVoltage110(String in) { super.inVoltage220(in); // 输入转换，国产充电器可以在美国使用 } // 输出标准outVoltage5和充电标准charge，与国产充电器标准一致，即已经在父类中被调用，无需重写方法。} 根据组合/聚合复用原则，很明显类适配器使用了继承方式而非组合/聚合方式，所以更常用的则是使用聚合方式的对像适配器 对像适配器 对像适配器采用聚合的方式使用，降低了对象之间的耦合性。 12345678910111213141516171819202122// 国产充电器的适配器，适配美国标准使用public class ChargerAdapter implements IAmericaCharger { private IChinaCharger chinaCharger; public ChargerAdapter(IChinaCharger chinaCharger) { this.chinaCharger = chinaCharger; } public void inVoltage110(String in) { super.inVoltage220(in); // 输入转换，国产充电器可以在美国使用 } public void outVoltage5() { super.outVoltage5(); } public void charge() { super.charge(); } } 缺省适配器 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为一个接口提供缺省实现，这样子类型可以从这个缺省实现进行扩展，而不必从原有接口进行扩展。这是为了方便建立一个不平庸的适配器类而提供的一种平庸实现。 “国际充电器组织” 制订了统一的充电标准，然而每个地区或国家的具体实现都不相同，这就要求在“国际标准”和“落地方案”中采用一个中间缺省抽象层，用于一种更大众和均匀的实现，个别国家则可以在此基础上自己指定具体实现方案。 123456789101112131415161718192021222324252627282930313233// 国际充电器标准public interface IInternationalCharger { void inVoltage(String in); void outVoltage(); void charge();}// 缺省适配器抽象类public abstract AbstractChargerAdapter implements IInternationalCharger { public void inVoltage(String in) { // 国际上普遍使用220V，那么缺省适配器就用220V作为普遍实现标准 } public void outVoltage() { // 采用普遍使用的5V输出 } public void charge() {}}// 中国适配器采用国际标准public class ChinaCharger extends AbstractChargerAdapter { // 中国与国际接轨，标准一致，无需重写}// 美国适配器采用国际标准public class AmericaCharger extends AbstractChargerAdapter { // 重写输入标准，其他标准一致无需重写 @Override public void inVoltage(String in) { // 美国重写输入标准采用110V }} 4.2 装饰模式（Decoration）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;装饰模式可以动态的往对象行为（方法）上添加附加行为，灵活应用了子类覆写父类方法的特性，可以提供比继承更有弹性的替代方案。装饰模式中具有以下几种特点: 抽象角色：要包装的原始类，属于抽象类或接口。 具体抽象角色：具体被装饰的对象，属于抽象角色的实现类。 装饰类：继承或实现抽象角色，并使用抽象角色的聚合态作为成员变量调用方法，一般为抽象类。 装饰实现类：动态添加附属行为的类，是装饰类的实现类。 题引：有一杯纯水，通过装饰类往水中动态的添加各种东西，比如加点柠檬，加些冰块或者放点茶叶进去。最终将纯水变为我们需要的饮品。 123456789101112// 抽象角色interface IWater { String getFeature(); // 水里有什么}// 抽象角色实现类，一杯纯纯的水class Water implements IWater { @Override public String getFeature() { return \"pure water\"; }} 接下来定义水的装饰类和装饰实现类，通过子类实现不同的添加功能 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// 抽象装饰类，持有水的接口对象abstract class WaterDecoration implements IWater { IWater water ; public WaterDecoration(IWater water) { this.water = water; } public String getFeature() { return water.getFeature(); }}// 加入柠檬变成柠檬水class LemonWater extends WaterDecoration { public LemonWater(IWater water) { super(water); } @Override public String getFeature() { return super.getFeature() + \" with lemon\"; }}// 加入冰，让它称为一杯冰水class IceWater extends WaterDecoration { public IceWater(IWater water) { super(water); } @Override public String getFeature() { return super.getFeature() + \" with ice\"; }}// 加入茶叶class TeaWater extends WaterDecoration { public TeaWater(IWater water) { super(water); } @Override public String getFeature() { return super.getFeature() + \" with tea\"; }} 下列测试，动态添加佐料进去 123456789// 伪代码IWater water = new Water();water = new LemonWater(water);water = new IceWater(water);water = new TeaWater(water);System.out.println(water.getFeature());// 输出// pure water with lemon with ice with tea 如果只有一个抽象角色的实现类时，可以将装饰类直接继承使用 1234567891011121314151617static class Water implements IWater { @Override public String getFeature() { return \"pure water\"; }}static abstract class WaterDecoration extends Water { Water water ; public WaterDecoration(Water water) { this.water = water; } public String getFeature() { return water.getFeature(); }} 4.3 代理模式（Proxy）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;代理模式被广泛的应用于框架产品比如Spring和Mybatis，代理模式可以分为：静态代理、JDK动态代理和CGLIB代理。 静态代理 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;静态代理通过代理类实现接口或继承抽象类，并与实际被代理对象通过聚合方式产生调用关系。 题引：要举办一场婚礼，除了婚宴以外还需要找婚庆公司订车队、订司仪、婚礼现场布置等等前置工作，还有答谢宾客等后置动作。 1234567891011121314151617181920212223242526272829303132333435interface IWedding { void wedding();}class Wedding implements IWedding { @Override public void wedding() { System.out.println(\"举办婚礼\"); }}class WeddingProxy implements IWedding { IWedding wedding; public WeddingProxy(IWedding wedding) { this.wedding = wedding; } @Override public void wedding() { System.out.println(\"找婚庆公司\"); System.out.println(\"布置现场\"); wedding.wedding(); System.out.println(\"答谢宾客\"); }}// 测试伪代码IWedding wedding = new Wedding();IWedding proxy = new WeddingProxy(wedding);proxy.wedding();// 输出// 找婚庆公司// 布置现场// 举办婚礼// 答谢宾客 静态代理在编译阶段即可确定，需要通过代理类持有被代理对象的实例，并实现代理对象接口或抽象类的方法，从而达到扩展的目的。 JDK动态代理 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;JDK动态代理利用Java反射机制，在运行期创建对对象的代理实现功能扩展，无需显示的创建代理类，只需要创建代理处理器即可完成。JDK代理要求被代理对象必须实现接口，否则无法使用。 12345678910111213141516171819202122232425262728public class WeddingProxyHandler implements InvocationHandler { private Object proxyObject; public WeddingProxyHandler(final Object proxyObject) { this.proxyObject = proxyObject; } // Object proxy参数指代理对象本身 @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(\"找婚庆公司\"); System.out.println(\"布置现场\"); Object result = method.invoke(proxyObject, args); System.out.println(\"答谢宾客\"); return result; }}// 测试伪代码IWedding wedding = new Wedding();ClassLoader classLoader = IWedding.class.getClassLoader();Class[] interfaces = wedding.getClass().getInterfaces();WeddingProxyHandler proxy = new WeddingProxyHandler();IWedding proxy = Proxy.newProxyInstance(classLoader, interfaces, proxy);proxy.wedding();// 输出与静态代理一致 出于Java机制的限制，一个类无法存在多继承关系，所以JDK动态代理始终需要通过接口完成代理而无法针对父类进行代理。 CGLIB代理 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;JDK代理从源码层面采用反射机制完成代理，在类加载阶段因为只是创建实例类所以该阶段执行效率高，而在运行期不断的通过反射完成所以该阶段效率并不高。相反的，CGLIB代理原理是通过ASM技术，采用动态修改字节码，在加载期向字节码写入代理功能，所以类加载阶段效率不如JDK代理，但是一旦创建实例类，在运行期就会被当作普通对象来正常使用，所以在运行期效率远高于JDK代理。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;除此之外，JDK动态代理只能代理实现接口的对象，而CGLIB代理不仅可以实现非接口类的代理，目前也支持实现接口对象的代理。所以在Spring的选择中，如果对象实现接口默认采用动态代理，除此之外都使用CGLIB代理，当然也可以强制要求实现接口的对象也使用CGLIB代理。 123456789101112131415161718192021222324252627class Wedding { @Override public void wedding() { System.out.println(\"举办婚礼\"); }}class WeddingProxy implements MethodInterceptor { @Override public Object intercept(Object o, Method method, Object[] params, MethodProxy proxy) throws Throwable{ System.out.println(\"找婚庆公司\"); System.out.println(\"布置现场\"); Object result = proxy.invokeSuper(o, params); System.out.println(\"答谢宾客\"); return result; }}// 使用，伪代码Enhancer enhancer = new Enhancer();enhancer.setSuperclass(Wedding.class);enhancer.setCallback(new WeddingProxy());Wedding wedding = (Wedding) enhancer.create();wedding.wedding();// 输出结果同上 4.4 外观模式（Facade）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;外观模式相对容易理解，它更像是一种面板，通过面板可以执行很多操作。如今复杂系统会由多个子系统组成，对于用户来说，一个用户面向多个子系统是一件很烦躁的事情，外观模式可以把子系统整合使用户只正对一个面板类，通过面板类可以执行内部分装的各类子系统。符合迪米特原则，但违反了开闭原则。 题引：如今的全自动洗衣机不仅可以洗衣、甩干、侵泡，还可以提供除螨、高温杀菌、定时清洗等诸多功能，用户通过面板按键可以轻松执行对应的功能。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879// Facade门面类public class WashingMachineMenu { private MainFunction mainFunction; private AssistantFunction assistant; private DetergentFunction detergent; public WashingMachineMenu(MainFunction mainFunction, AssistantFunction assistant, DetergentFunction detergent) { this.mainFunction = mainFunction; this.assistant = assistant; this.detergent = detergent; } public void wash() { mainFunction.wash(); } public void dry() { mainFunction.dry(); } public void soak() { mainFunction.soak(); } public void antiAcne() { assistant.antiAcne(); } public void sterilize() { assistant.sterilize(); } public void schedual() { assistant.schedual(); } public void addPowder() { detergent.addPowder(); } public void addLiquid() { detergent.addLiquid(); } public void addSoftener() { detergent.addSoftener(); }}// 各子系统class MainFunction { public void wash() { // } public void dry() { // } public void soak() { // }}class AssistantFunction { public void antiAcne() { // } public void sterilize() { // } public void schedual() { // }}class DetergentFunction { public void addPowder() { // } public void addLiquid() { // } public void addSoftener() { // }} 4.5 桥接模式（Bridge）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;将实现与抽象放在两个不同的层次，使两个层次隔离，修改互不影响。假如在实际应用中，某个类存在多个独立变化的维度，可以通过桥接模式将维度拆分为两层独立结构，互相扩展互不影响。通常需要识别类的两个层次分别定义抽象类和接口，使抽象类聚合接口，分别拓展两个层次抽象的具体实现。 题引：市场上有很多品牌的各类家用电器，比如美的空调、美的洗衣机、美的扫地机器人、海尔空调、海尔洗衣机、海尔扫地机器人、格力空调、格力扫地机器人等等。通过桥接模式实现，并支持扩展品牌和商品的扩展能力。 首先，定义家用电器接口及其实现类。 123456789101112131415161718192021222324252627282930public interface IAppliance { String type();}// 洗衣机class WashingMachine implements IAppliance { private final String type = \"washing machine\"; public String type() { return type; }}// 空调class AirConditioner implements IAppliance { private final String type = \"air conditioner\"; public String type() { return type; }}// 扫地机器人class SweepRobot implements IAppliance { private final String type = \"sweep robot\"; public String type() { return type; }} 接下来定义品牌抽象类及其实现类。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748abstract class AbstractBrand { private IAppliance appliance; public AbstractBrand(IAppliance appliance) { this.appliance = appliance; } public abstract void description();}// 美的class MideaBrand extends AbstractBrand { private final String brandName = \"Midea\"; public MideaBrand(IAppliance appliance) { super(appliance); } public void description() { System.out.printf(\"%s of %s \\n\", appliance.type, brandName); }}// 海尔class HaierBrand extends AbstractBrand { private final String brandName = \"Haier\"; public HaierBrand(IAppliance appliance) { super(appliance); } public void description() { System.out.printf(\"%s of %s \\n\", appliance.type, brandName); }}// 格力class GreeBrand extends AbstractBrand { private final String brandName = \"Gree\"; public GreeBrand(IAppliance appliance) { super(appliance); } public void description() { System.out.printf(\"%s of %s \\n\", appliance.type, brandName); }} 这样就将品牌和家用电器进行了分层，并各自实现具体对象，接下来通过调用组装我们需要的目标商品。 123456789101112131415161718192021222324252627282930313233343536// 调用，伪代码IApplication washingMachine = new WashingMachine();IApplication airConditioner = new AirConditioner();IApplication sweepRobot = new SweepRobot();AbstractBrand mideaWashingMachine = new MideaBrand(washingMachine);AbstractBrand mideaAirConditioner = new MideaBrand(airConditioner);AbstractBrand mideaSweepRobot = new MideaBrand(sweepRobot);AbstractBrand haierWashingMachine = new HaierBrand(washingMachine);AbstractBrand haierAirConditioner = new HaierBrand(airConditioner);AbstractBrand haierSweepRobot = new HaierBrand(sweepRobot);AbstractBrand greeAirConditioner = new GreeBrand(airConditioner);AbstractBrand greeSweepRobot = new GreeBrand(sweepRobot);mideaWashingMachine.description();mideaAirConditioner.description();mideaSweepRobot.description();haierWashingMachine.description();haierAirConditioner.description();haierSweepRobot.description();greeAirConditioner.description();greeSweepRobot.description();// output// washing machine of Midea// air conditioner of Midea// sweep robot of Midea// washing machine of Haier// air conditioner of Haier// sweep robot of Haier// air conditioner of Gree// sweep robot of Gree 由此可以看出，只要定义好了顶层的分层抽象逻辑，具体实现类就很容易进行横向扩展，再新增家用电器或者品牌都变得容易，但需要注意，如果实现类足够多，容易造成”类爆炸”。 4.6 组合模式（Composite ）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假如一个对象拥有自己完整的属性和行为，它同时又是其他对象的组成部分，所以可以使用组合模式定义一个抽象层，该抽象层提供两个对象的行为，就可以通过抽象层对象来创建复杂的组合关系。比如，在OS的文件系统中，一个目录既可以包含多个目录，每个目录又可以包含多个文件。 题引：一个大公司通常在世界各地有自己的分公司，而不管总公司还是分公司都独自成体系，有自己的职能部门。现在有一个公司，它在北京、旧金山和伦敦都有自己的分公司，总公司职能部门包括：人事部、财务部和秘书办；北京分公司有IT部、测试部、人事部；旧金山和伦敦分公司有业务部、人事部和财务部。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394public abstract AbstractOrganize { private String name; // 公司名或部门名 public String getName() { return name; } public void setName() { this.name = name; } // 提供的方法，如果公司和部门都需要实现则定义抽象方法，否则实现方法但不提供内容 public void addDepartment(Department... depts) { throw new UnsupportedOperationException(\"abstrac not support\"); } public void removeDepartment(Department dept) { throw new UnsupportedOperationException(\"abstrac not support\"); } public void addCompany(Company... companies) { throw new UnsupportedOperationException(\"abstrac not support\"); } public boolean removeCompany(Company company) { throw new UnsupportedOperationException(\"abstrac not support\"); } public abstract void print();}class Company extends AbstractOrganize { private List&lt;Department&gt; deptList = new ArrayList&lt;&gt;(); private List&lt;Company&gt; companyList = new ArrayList&lt;&gt;(); public void addDepartment(Department... depts) { for (Department d : depts) { deptList.add(d); } } public boolean removeDepartment(Department dept) { return deptList.remove(dept); } public void addCompany(Company... companies) { for (Company c : companies) { companyList.add(c); } } public boolean removeCompany(Company company) { return companyList.remove(company); } @Override public void print() { deptList.forEach(System.out::println); companyList.forEach(System.out::println); } @Override public String toString() { // toString方法用于打印，省略 }}class Department extends AbstractOrganize{ @Override public void print() { System.out.println(name); }}// 测试，伪代码AbstractOrganize headCompany = new Company();AbstractOrganize pekingCompany = new Company();AbstractOrganize sanFranciscoCompany = new Company();AbstractOrganize londonCompany = new Company();headCompany.addCompany(pekingCompany, sanFranciscoCompany, londonCompany);AbstractOrganize personnelDept = new Department();AbstractOrganize financeDept = new Department();AbstractOrganize secretaryDept = new Department();AbstractOrganize itDept = new Department();AbstractOrganize testDept = new Department();AbstractOrganize businessDept = new Department();headCompany.addDepartment(personnelDept, financeDept, secretaryDept);pekingCompany.addDepartment(itDept, testDept, personnelDept);sanFranciscoCompany.addDepartment(businessDept, financeDept, personnelDept);londonCompany.addDepartment(businessDept, financeDept, personnelDept);headCompany.print(); 4.7 亨元模式（Flyweight）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;亨元模式是指当系统中存在大量的相同对象时，可以通过共享技术，将相同的类共享给使用者，而非创建。比如围棋游戏中，黑白棋就是大量的重复对象，如果每个棋子都要创建对象，则会造成系统资源吃紧甚至OOM发生，所以如果采用共享技术，将一盘棋中的黑子和白子只创建一个对象，在其他落子位置复用对象则不会造成该问题。亨元模式大量的应用于”池“计数，比如线程池、数据连接池、缓存池等等。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;亨元模式的重点，是要区分内部状态和外部状态，所谓内部状态是指被亨元对象类（FlyWeightFactory）持有的内部控制管理对象，是亨元的具体实现类（ConcreteFlyWeight）；而外部状态是指随着外部状态的改变而发生变化的对象，是不可共享亨元类（UnSharedFlyWeight）； 题引：对于一局围棋游戏，共享的是围棋棋子的颜色，而不可共享的是棋子的落子位置。 定义亨元模式接口（FlyWeight -&gt; IChess）及其实现类（ConcreteFlyWeight -&gt; Piece），对于围棋来说，颜色是共享状态，位置是不共享状态。 12345678910111213141516171819202122232425262728// 亨元接口——棋子public interface IChess { void setColor(Color color); Color getColor(); void position(Coordinate);}// 亨元实现类class Piece implements IChess { private Color color; public Piece(Color color) { this.color = color; } @Override public void setColor (Color color) { this.color = color; } @Override public Color getColor() { return color; } public void position(Coordinate c) { System.out.printf(\"%s 落位（%d, %d）\", color.getValue, c.getX(), c.getY()); }} 定义非共享对象(UnsharedFlyWeight -&gt; Coordinate) 1234567891011121314151617class Coordinate { private int x; private int y; public Coordinate(int x, int y) { this.x = x; this.y = y; } public int getX() { return x; } public int getY() { return y; }} 最后定义亨元模式的工厂类（FlyweightFactory -&gt; ChessFactory），由工厂类控制和管理共享对象。 1234567891011121314151617181920212223242526272829public class ChessFactory { private static final Map&lt;Color, IChess&gt; pieces = new HashMap&lt;&gt;(); // 内部控制棋子创建 private ChessFactory(){} // 私有构造器，工厂类是单例的 // 静态内部类实现工厂单例对象 private final class ChessFactoryInstanceHolder { private static final ChessFactory INSTANCE = new ChessFactory(); } // 返回工厂单例 public ChessFactory getInstance() { return ChessFactoryInstanceHolder.INSTANCE; } public IChess getPiece(Color color) { IChess piece = pieces.get(color); if (Objects.isNull(piece)) { piece = new Piece(color); pieces.put(color, piece); } return piece; } public int getSize() { return pieces.size(); } } 123456789101112131415161718// 测试，伪代码// 黑子执子先行，模拟3步之内的操作ChessFactory factory = ChessFacotry.getInstance();IChess pieceBlack = factory.get(Color.BLACK);pieceBlack.position(new Coordinate(5,5)); // step1:黑子先落子棋盘坐标（5,5）位置IChess pieceWhite = facotry.get(Color.WHITE);pieceWhite.position(new Coordinate(15.15)); // step2:白子随后落子（15,15）坐标位置pieceBlack = factory.get(Color.BLACK);pieceBlack.position(new Coordinate(13,4)); // step3:黑子落子（13,4）位置System.out.println(\"当前棋子类型:\" + factory.getSize());// output// 黑子落位（5,5）// 白子落位（15,15）// 黑子落位（13,4）// 当前棋子类型: 2","link":"/2020/06/12/9%20%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%88%E4%B8%89%EF%BC%89-%20%E7%BB%93%E6%9E%84%E5%9E%8B%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"title":"设计模式（四）","text":"设计模式（四） 4 行为型设计模式 策略模式、模版模式、观察者模式、迭代器模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式 4.1 迭代器模式&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;迭代器模式提供顺序访问聚合对象内的各个元素，并且不会暴露内部状态。在JDK的集合中，几乎每个集合都继承了Iterator接口并通过内部类形式提供了迭代器用于顺序访问集合内的各个元素，下面以ArrayList和LinkedList展示迭代器模式 1234567891011121314151617181920212223242526272829303132333435// ArrayList迭代器模式的部分实现// 主要通过索引代表来确定数据在数组中的位置public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable{ // 提供迭代器对象 public Iterator&lt;E&gt; iterator() { return new Itr(); } // 迭代器实现类 private class Itr implements Iterator&lt;E&gt; { int cursor; int lastRet = -1; int expectedModCount = modCount; Itr() {} public boolean hasNext() { return cursor != size; } public E next() { checkForComodification(); int i = cursor; if (i &gt;= size) throw new NoSuchElementException(); Object[] elementData = ArrayList.this.elementData; if (i &gt;= elementData.length) throw new ConcurrentModificationException(); cursor = i + 1; return (E) elementData[lastRet = i]; } }} 1234567891011121314151617181920212223242526272829303132333435363738// LinkedList迭代器部分实现// 主要通过next后继指针来确定链表中数据的位置public class LinkedList&lt;E&gt; extends AbstractSequentialList&lt;E&gt; implements List&lt;E&gt;, Deque&lt;E&gt;, Cloneable, java.io.Serializable{ // 提供迭代器对象 public ListIterator&lt;E&gt; listIterator(int index) { checkPositionIndex(index); return new ListItr(index); } // 迭代器实现类 private class ListItr implements ListIterator&lt;E&gt; { private Node&lt;E&gt; next; private int nextIndex; ListItr(int index) { next = (index == size) ? null : node(index); nextIndex = index; } public boolean hasNext() { return nextIndex &lt; size; } public E next() { checkForComodification(); if (!hasNext()) throw new NoSuchElementException(); lastReturned = next; next = next.next; nextIndex++; return lastReturned.item; } }} 4.2 观察者模式&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当多个对象之间存在一对多的关系，修改一个对象会影响被依赖的对象发生变化，通过观察者模式可以自动通知该对象并更新。观察者模式中的对象需要建立一套触发机制进行自动修改，当对象之间存在循环依赖的话，可能触发循环更新，发生严重的占用系统资源导致崩溃。观察者模式一般通过会定义两个抽象对象进行解耦操作，并由具体实现类完成。可概括为如下： Subject抽象类，通过集合形式持有注册的观察者，并在数据更新后通知所有注册的观察者进行数据更新。 Observer抽象类，通常定义收到Subject通知后执行数据更新操作。 所以，观察者模式通常完成与“订阅-下发”功能相关的业务。 题引：拍卖行进行商品拍卖，每一件商品都有多个竞争者竞价，每次竞价都会被拍卖人员广播给所有竞争者，从而进行新一轮的竞价，依次往复，直到商品被卖出。 首先定义抽象层，分别是针对拍卖者、观察者和竞价接口。 12345678910111213141516171819202122// 定义Subject的拍卖对象public interface IAuction { void registerObserver(IBidderObserver observer) ; // 注册观察者 void removeObserver(IBidderObserver observer) ; // 删除观察者 void notifyObserver(double price); // 广播观察者 void receivePrice(IBidderObserver bidder, double price); // 收到竞价 String win(); // 获胜者 int getSize(); // 竞争人数}// 定义Observer的竞价者观察对象public interface IBidderObserver { void update(double price); int getNumber();}// 额外定义一个竞价者相关的行为接口public interface IBid { void participate(IAuction auction); // 参与 void bid(IAuction auction, double price); // 喊价 void quit(IAuction auction); // 推出竞价} 定义具体的观察者对象，即竞争者。 1234567891011121314151617181920212223242526272829303132333435class Bidder implements IBidderObserver, IBid { private int number; // 编号 private double bidPrice; // 竞价 private double currentPrice; // 当前商品价格 public Bidder(int number) { this.number = number; } @Override public void update(double price) { currentPrice = price; } @Override public void participate(IAuction auction) { auction.registerObserver(this); } @Override public void bid(IAuction auction, double price) { bidPrice = price; auction.receivePrice(this, bidPrice); } @Override public void quit(IAuction auction) { auction.removeObserver(this); } @Override public int getNumber() { return number; }} 接下来定义拍卖者的实现类。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354class Auction implements IAuction { private List&lt;IBidderObserver&gt; bidders = new ArrayList&lt;&gt;(); // 观察者列表 private static IBidderObserver bestBidder; // 当前竞价最高者 private static volatile String name; // 本次拍卖物品名称 private static volatile double price; // 本次拍卖价格 // 初始拍卖物品价格 public Auction(String name, double price) { this.name = name; this.price = price; } @Override public void registerObserver(IBidderObserver observer) { if (!bidders.contains(observer)) { bidders.add(observer); } } @Override public void removeObserver(IBidderObserver observer) { if (bidders.contains(observer)) { bidders.remove(observer); } } @Override public void notifyObserver(double price) { for (IBidderObserver bidder : bidders) { bidder.update(price); } } // 收到价格 public void receivePrice(IBidderObserver bidder, double price) { if (bidders.contains(bidder) &amp;&amp; this.price &lt; price) { this.price = price; bestBidder = bidder; notifyObserver(price); } } // 获胜者 @Override public String win() { return String.format(\"%d 号参与者赢得 %s , 总价: %.2f \", bestBidder.getNumber(), name, price); } @Override public int getSize() { return bidders.size(); }} 测试代码，模拟一轮拍卖。 1234567891011121314151617181920212223242526IBid bidder1 = new Bidder(1); // 1号竞争者IBid bidder2 = new Bidder(2); // 2号竞争者IBid bidder3 = new Bidder(3); // 3号竞争者IBid bidder4 = new Bidder(4); // 4号竞争者// 初始商品名称和价格String name = \"兽首\";double price = 10000.0;IAuction auction = new Auction(name, price);// 竞争者参与竞价bidder1.participate(auction);bidder2.participate(auction);bidder3.participate(auction);bidder4.participate(auction);// 第一轮竞价bidder1.bid(auction, price + 5000.0);bidder2.bid(auction, price + 8000.0);bidder3.quit(auction); // 3号退出本次竞价bidder4.bid(auction, price + 6000.0);System.out.println(\"竞争人数: \" + auction.getSize());System.out.println(auction.win());// output// 竞争人数: 3// 2 号参与者赢得 兽首 , 总价: 18000.00 4.3 模板模式&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过一个抽象类（模板）定义了一个骨架，骨架中实现了部分的功能，还有一部分功能为抽象方法，由具体的继承类来完成，也就是说，利用多态的特性，把更具体的实现延迟到子类中。 题引：在通用的做菜过程中，一般可以分为洗菜、摘菜、烹制、装盘这四个大步骤，无论做什么样式的菜品，其中洗菜和摘菜是通用的，而口味体现在不同的烹制方法上，不同的装盘也会呈现不同的外观。通过模板模式创建糖醋类做法、宫保类做法和红烧类做法。 首先创建模板类，该类定义了四个做菜步骤，其中洗菜和摘菜是统一通用步骤，烹制和装盘需要具体子类实现重写，最后定义做菜标准流程cookDish做出一道别具特色的菜品。 12345678910111213141516171819202122232425262728public abstract class Cuisine { private final String name; // 菜品名称 public Cuisine(String name) { this.name = name; } public final void wash() { System.out.println(\"洗菜\"); } public final void trim() { System.out.println(\"择菜\"); } public abstract void cook(); // 由具体子类实现烹制方式 public abstract void dishup(); // 由具体子类实现装盘 // 定义做菜工艺流程 public void cookDish() { wash(); trim(); cook(); dishup(); System.out.println(name + \" 呈现在眼前。\"); }} 定义具体实现类，包括糖醋类、宫保类、红烧类。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// 宫保类class Kungpao extends Cuisine { public Kungpao(String name) { super(name); } @Override public void cook() { System.out.println(\"宫保烹制\"); } @Override public void dishup() { System.out.println(\"装盘后撒上香葱\"); }}// 糖醋类class SweetSour extends Cuisine { public SweetSour(String name) { super(name); } @Override public void cook() { System.out.println(\"糖醋烹制\"); } @Override public void dishup() { System.out.println(\"装盘后淋上汤汁\"); }}// 红烧类class Braise extends Cuisine { public Braise(String name) { super(name); } @Override public void cook() { System.out.println(\"红烧烹制\"); } @Override public void dishup() { System.out.println(\"装盘后撒上香葱\"); }} 最后伪代码测试。 1234567891011121314151617181920212223242526272829Cuisine kungpao = new Kungpao(\"宫保鸡丁\");kungpao.cookDish();Cuisine sweetSour = new SweetSour(\"糖醋里脊\");sweetSour.cookDish();Cuisine braise = new Braise(\"红烧肉\");braise.cookDish();// output/*洗菜择菜宫保烹制装盘后撒上香葱宫保鸡丁 呈现在眼前。洗菜择菜糖醋烹制装盘后淋上汤汁糖醋里脊 呈现在眼前。洗菜择菜红烧烹制装盘后撒上香葱红烧肉 呈现在眼前。*/ 4.4 策略模式&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;策略模式可以按照不同种类的策略执行同一个动作，并且切换策略不会影响执行，还可以根据不同场景提供不同的行为。策略模式遵循开闭原则，场景的切换可以切换适配的策略，可以通过创建新的策略接入新需求。策略模式通常用于替换圈复杂度高的条件分支。 题引：作为一名人民警察，当路边遇到年岁古稀的老人求助可以温柔和蔼的问询；当遇到袭击市民的暴徒，可以严词威震的呵斥；当遇到警署上级，可以谦逊的回应。遇见不同的人可以采用不同的交流方式。 首先定义一个交流沟通的策略接口，只有一种交流方法。 123public interface ICommunicationStrategy { String communicate();} 定义交流的具体策略类。 1234567891011121314151617181920212223// 温柔的交流class GentleCommunication implements ICommunicationStrategy { @Override public String communicate() { return \"温柔的沟通\"; }}// 充满威严的交流class AugustCommunication implements ICommunicationStrategy { @Override public String communicate() { return \"呵斥\"; }}// 谦逊的交流class ModestCommunication implements ICommunicationStrategy { @Override public String communicate() { return \"谦逊的交流\"; }} 定义具体的交流场景类。 12345678910111213141516class StrategyContext { private ICommunicationStrategy strategy; public StrategyContext(ICommunicationStrategy strategy) { this.strategy = strategy; } // 更换策略 public void changeStrategy(ICommunicationStrategy strategy) { this.strategy = strategy; } public void communicate(String name) { System.out.prinf(\"与 %s %s \\n\", name, strategy.communicate()); }} 1234567891011121314151617// 测试，伪代码ICommunicationStrategy strategy = new GentleCommunication();StrategyContext context = new StrategyContext(strategy);context.communicate(\"年逾古稀的老者\");strategy = new AugustCommunication();context.changeStrategy(strategy); // 修改策略context.communicate(\"暴徒\");strategy = new ModestCommunication();context.changeStrategy(strategy); // 修改策略context.communicate(\"上级领导\");// output// 与 年逾古稀的老者 温柔的沟通// 与 暴徒 呵斥// 与 上级领导 谦逊的交流 策略模式的使用场景类通常需要确切的知道每个具体策略实现类，当业务不断膨胀可能造成策略类的类爆炸问题，所以最优的策略模式可以采用枚举策略模式，灵活的利用枚举对象的特性，通过实现接口来为不同的枚举对象提供更具体的实现策略，更容易切换策略且更容易理解并且不会产生庞大的实现类体积。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public interface ICommunicationStrategy { String communicate();}public enum StrategyType implements ICommunicationStrategy{ GENTLE { @Override public String communicate() { return \"温柔的沟通\"; } }, AUGUST { @Override public String communicate() { return \"呵斥\"; } }, MODEST { @Override public String communicate() { return \"谦逊的交流\"; } }}class StrategyContext { private StrategyType strategy; public StrategyContext(StrategyType strategy) { this.strategy = strategy; } // 更换策略 public void changeStrategy(StrategyType strategy) { this.strategy = strategy; } public void communicate(String name) { System.out.prinf(\"与 %s %s \\n\", name, strategy.communicate()); }}// 测试，伪代码StrategyContext context = new StrategyContext(StrategyType.GENTLE); // 初始策略context.communicate(\"年逾古稀的老者\");context.changeStrategy(StrategyType.AUGUST); // 修改策略context.communicate(\"暴徒\");context.changeStrategy(StrategyType.MODEST); // 修改策略context.communicate(\"上级领导\");// output// 与 年逾古稀的老者 温柔的沟通// 与 暴徒 呵斥// 与 上级领导 谦逊的交流 4.5 责任链模式&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;责任链模式，顾名思义，就是创建一个引用链，当引用链中的某个对象可以处理输入信息时就会执行相应的方法，否则就传递给下一个对象。在开发过程中对于日志系统定义了四个等级INFO、WARN、ERROR、DEBUG，每个日志等级有专门的处理对象，只有符合当前处理对象的日志等级才会被写入日志文件中。 题引： 在XX公司考勤系统中，个人请假时长小于3天则由PM统一处理，如果请假时间大于3天小于15天则由部长审批，如果超过15天则必须由人事总裁审批。 首先定义审批人的抽象类，提供责任链中的继任者，如果当前对象无法审批则交由继任者处理。 1234567891011121314151617public abstract AbstractApprover { private AbstractApprover successor; public AbstractApprover() {} public AbstractApprover(AbstractApprover successor) { this.successor = successor; } // 设置继任者 protected void setSuccessor(AbstractApprover successor) { this.successor = successor; } // 审批 protected abstract void approve(int duration);} 接着定义责任链中具体的处理对象。 123456789101112131415161718192021222324252627282930313233343536373839// PM 处理3天以内的请假class ProjectManager { public ProjectManager(AbstractApprover successor) { super(successor); } protected void approve(int duration) { if (duration &gt; 0 &amp;&amp; duration &lt;= 3) { System.out.println(\"PM confirmed\"); return; } successor.approve(duration); }}// 部长 处理3-15天的请假class Minister { public Minister(AbstractApprover successor) { super(successor); } protected void approve(int duration) { if (duration &gt; 3 &amp;&amp; duration &lt;= 15) { System.out.println(\"Minister confirmed\"); return; } successor.approve(duration); }}// 人事总裁 处理3-15天的请假class CHO { protected void approve(int duration) { if (duration &gt; 15) { System.out.println(\"CHO confirmed\"); return; } }} 1234567891011121314// 测试，伪代码AbstractApprover cho = new CHO();AbstractApprover minister = new Minister(cho);AbstractApprover pm = new ProjectManager(minister);AbstractApprover approveChain = pm; // 责任链从pm开始，cho结束approveChain.approve(2);approveChain.approve(10);approveChain.approve(45);// output// PM confirmed// Minister confirmed// CHO confirmed","link":"/2020/06/20/9%20%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%88%E5%9B%9B%EF%BC%89-%20%E8%A1%8C%E4%B8%BA%E5%9E%8B%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"title":"使用Lombok","text":"使用Lombok &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Lombok是一款简化POJO对象创建，消除通用代码的第三方工具，通过简单的注解方式自动生成代码，配合IDE集成的插件功能可以有效提高开发效率。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@ToString(exclude = {\"sex\"})@EqualsAndHashCode@AllArgsConstructor@NoArgsConstructor@RequiredArgsConstructor(staticName = \"create\")@Accessors(chain = true)public class Human { @Getter(AccessLevel.PACKAGE) @Setter(AccessLevel.PACKAGE) @EqualsAndHashCode.Exclude @NonNull private int id; @Getter @Setter @NonNull private String name; @Getter @Setter private int age; @Getter @Setter private boolean sex;}// 测试public static void main(String[] args) { Human human = new Human(); human.setName(\"123\"); System.out.println(human); Human human1 = new Human(1, \"x\", 1, true); System.out.println(human1); Human human2 = Human.create(2,\"xxx\"); System.out.println(human2); Human human3 = new Human(); human3.setId(3).setName(\"yyy\"); System.out.println(human3);}// output// Human(id=0, name=123, age=0)// Human(id=1, name=x, age=1)// Human(id=2, name=xxx, age=0)// Human(id=3, name=yyy, age=0) @Getter和@Setter： 通过对类变量设置注解可以允许自动创建对应的get和set方法，boolean类型set方法为is开头； 如果需要设置可见性，可以通过 AccessLevel枚举类型设置，如@Setter(AccessLevel.PRIVATE); @ToString: 默认使用带参数的，以逗号分隔的，不打印继承类信息的toString方法。 如果不希望打印某些字段可以屏蔽@ToString(exclude = {&quot;id&quot;,&quot;sex&quot;})，会重写toString()方法; 如果希望打印父类中的信息，可以设置@ToString(callSuper = true)，会调用父类toString()方法; 构造函数： @NoArgsConstructor，添加无参构造函数，如果类中存在final字段且没有初始化，则可以默认设置初始值。@NoArgsConstructor(force = true); @AllArgsConstructor，添加全部参数的构造函数，且不提供无参构造函数。 @RequiredArgsConstructor，可以添加@NonNull指定参数的构造函数，同时该注解支持静态工厂方法创建对象，只需要注解内设置方法名即可，@RequiredArgsConstructor(staticName = &quot;create&quot;); 12Human human2 = Human.create(2,\"xxx\");System.out.println(human2); @EqualsAndHashCode： 支持重写equals()和hashCode()方法，默认情况下，静态变量和transient变量不会参与其中。 可以指定排除某些字段@EqualsAndHashCode(exclude = {&quot;id&quot;}); 默认不会比较父类对象，可以设置是否比较父类对象@EqualsAndHashCode(callSuper = true); @NonNull: 用于指定成员变量时会在set方法时进行非空判断，如果入参为空则抛出NPE异常。 可以用于创建指定参数的对象，参照@RequiredArgsConstructor。 @Accessor: fluent 变量用于控制setter和getter方法名是否包含get和set字符，默认false； chain 变量用于创建链式调用，默认false； prefix 变量用于创建getter和setter时，是否删除指定前缀。 123456789101112@Accessors(fluent = false, chain = true, prefix = \"hi\")public class Human { @Setter private int id; @Setter private int hiName;}// 测试Human human = new Human();human.setId(1).setName(\"xxx\"); @Value和@Data: @Data注解包含@Getter、@Setter、@ToString、@EqualsAndHashCode、@RequiredArgsConstrutor的全部含义，控制粒度过大，不推荐使用。可以通过将构造器设置为私有，通过该注解就可以创建一个静态工厂方法@Data(staticConstructor = &quot;valueOf&quot;); @Value注解与@Data类似，只不过不提供@Setter方法。","link":"/2020/06/20/%E4%B8%89%E6%96%B9%E5%B7%A5%E5%85%B7/%E4%BD%BF%E7%94%A8Lombok/"},{"title":"Spring及源码——SpringMVC","text":"Spring及源码——SpringMVC 2 SpringMVC2.1 Spring容器 和 SpringMVC容器的加载过程&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在Spring创建Web系统应用过程中会启动两个容器，分别是基础容器（Root IOC）用于保存业务 Bean 和 Web容器用于保存Web相关组件Bean，例如处理器、适配器和解析器等。Root IOC 是 Web容器的父容器，通过这样的继承关系使两个容器产生了关联关系，即父容器无法访问子容器的组件，子容器可以访问父容器的组件。 通常我们称web系统服务器为web容器，例如tomcat、apache、jetty、undertow等，为了加以区别： Spring基础容器称为 root ioc； SpringMVC容器称为 web ioc； web容器表示tomcat等系统应用的名称。 具体创建步骤如下： 启动web容器，加载web容器目录内的server.xml配置，创建ServletContext全局对象。 加载工程内web.xml文件，首先加载&lt;context-param&gt;下的Spring根配置文件路径并保存在ServletContext中。 加载 Spring 的 ContextLoaderListener ，如果ServletContext已创建则读取 context-param 内的配置文件路径，并创建 Spring 容器 Root IOC ，创建成功后将该容器注册到ServletContext的属性列表中，关键字 key 为 ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE，在随后的使用过程中可以从ServletContext中读取ROOT IOC。 加载 web.xml 中的 &lt;servlet&gt;，这里既可以加载自定义的 Servlet ，但主要加载 DispatcherServlet ，Spring支持创建多个 DispatcherServlet 用于处理不同的请求，每一个 DispatcherServlet 都会创建各自独占的 Web IOC （Web IOC 配置文件路径通过 &lt;init-param&gt; 设置）， Web IOC 会将 Root IOC 设为父容器，这样在处理 Web 应用过程中就可以获得 Root IOC 内的业务 Bean 信息。Root IOC 是无法访问 Web IOC 内信息的。创建好的 Web IOC 也会添加到 ServletContext 的属性列表中，关键字 key 为 ServletName。 Web IOC 注册到ServletContext中的关键字其实并不仅仅是Servlet Name，具体源码如下。 12345678910111213141516171819// Spring 源码public class FrameworkServlet { public static final String SERVLET_CONTEXT_PREFIX = FrameworkServlet.class.getName() + \".CONTEXT.\"; protected WebApplicationContext initWebApplicationContext() { ... if (this.publishContext) { String attrName = this.getServletContextAttributeName(); this.getServletContext().setAttribute(attrName, wac); } ... } public String getServletContextAttributeName() { return SERVLET_CONTEXT_PREFIX + this.getServletName(); }} 初始化 Web IOC 策略 Bean 对象，其中主要包括：HandlerMapping、HandlerAdapter、ViewResolvers等解析器、映射器和适配器，最终整个装载过程完成。 123456789101112// spring 源码protected void initStrategies(ApplicationContext context) { initMultipartResolver(context); // 上传文件解析 initLocaleResolver(context); // 本地解析 initThemeResolver(context); // 主题解析 initHandlerMappings(context); // url映射器 initHandlerAdapters(context); // 映射器的目标执行对象，通过适配器执行 initHandlerExceptionResolvers(context); // 异常解析 initRequestToViewNameTranslator(context); initViewResolvers(context); // 视图解析器 initFlashMapManager(context);} 2.2 SpringMVC 框架执行过程 客户端发出请求到web容器。 web容器解析url并找到适配的servlet，此处转发给DispatcherServlet处理。 DispatcherServlet 解析 url 并通过 HandlerMapping 找到适配的处理器Handler。 HandlerAdapter 将对应的 Handler封装为适配器，并进行业务处理，处理结束后会以 ModelAndView对象返回。 DispatcherServlet 解析 ModelAndView 中的 View 路径，并通过 ViewResolver 解析获得对应的视图View。 将 ModelAndView 中的数据与对应的视图 View 进行渲染并最终返回给客户端。 2.3 相关注解@Controller 、 @RestController 和 @AsyncRestController 三者都可以放置在类注解，表示当前类是控制层Bean对象，@Controller表示的控制器返回时要求是 ModelAndView 对象，只返回字符串则表示视图关键字，用于下一步通过视图解析器获得对应视图，如果只希望返回给客户端字符串内容需要配合 @ResponseBody 使用； @RestController 简化了该过程，实际上就是@Controller 和 @ResponseBody的组合形式；该两者控制器在处理过程中都是同步阻塞式执行，而 @AsyncRestController 属于异步非阻塞式控制器。 @RequestMapping 和 Rest风格控制器 SpringMVC 的重要责任之一就是通过解析 url 匹配对应的控制器，@RequestMapping 负责执行匹配逻辑。该注解既可以使用在类层，也可以使用在方法层。当使用在类层时表示上层路径匹配项，该类中的方法层匹配都受此约束；当使用在方法层时表示最终的匹配项，通过 HandlerMapping 可以最终找到映射的处理器。 在早先开发过程中，客户端请求无非 Get 和 Post 两种类型处理，而实际上 HttpRequest 还包含 Put、Delete、Patch、Option、Trace 和 Head。Rest 要求应该严格按照 Http 请求类型来处理对应的业务逻辑，比如 Get类型负责查询、Post类型负责新增、Put类型负责更新、Delete类型负责删除。这就要求在使用 @RequestMapping时应该注明对应的请求类型，如@RequestMapping(method = RequestMethod.GET)。 @RequestMapping 支持表达式声明，以下表达式都是合法的且可以通过@PathVariable 获得路径中的占位符： /user/*/add : 匹配/user/xxx/add、/user/yyy/add等; /user/**/add ：匹配/user/xxx/yyy/add、/user/add等； /user/add?? : 匹配/user/addXXX、/user/addYYY等； /user/{id} : 匹配/user/123等； /user/**/{id} : 匹配/user/123、/user/find/123等； /user/{userId}/job/{jobId}/kpi/{kpiId} : 匹配/user/123/job/456/kpi/789等； @PathVariable、@RequestParam、@RequestBody 通常客户端请求传参过程中，简单参数可以直接在控制器入参中进行转换，当采用路径参数时使用@PathVariable获取；当传递多参数时可以使用@RequestParam进行处理；当传递的是json等对象时，可以使用@RequestBody接收并处理。 1234567891011121314@RequestMapping(value = \"/user/{id}\", method = RequestMethod.GET)public void demo1 (@PathVariable int id) { ...}@RequestMapping(\"/user\")public void demo2 (@RequestParam(\"id\") int id, @RequestParam(\"name\") String name) { ...}@RequestMapping(value = \"/user\", method = RequestMethod.POST)public void demo3 (@RequestBody Object input) { ...} 在使用@PathVariable时特别注意，一般情况下匹配项应该指定获取路径变量名，如 @RequestMapping(“\\user\\{id}”) public void demo(@PathVarable(“id”) int id) {…} 通过指定 id 来获得路径变量 id 的值。 不指定路径变量名，就会通过注解标记的参数名当作路径变量名使用。 但我们知道，在反射调用中参数名是无法获得的（因为在字节码 或 JVM常量池中 并不会保存形参的元数据），所以在JAVA8及以上版本中，通过IDE中配置编译命令 javac -parameters来使该功能可用。 简化Rest风格注解 标准的的 REST 注解应该是这样RequestMapping(value = &quot;/user&quot;, method = RequestMethod.POST)，但为了追求更快速和极简的风格可以采用简化版的注解：@GetMapping(“/user”)、 @PostMapping(“/user”)、@PutMapping(“/user”)、@DeleteMapping(“/user”)、@PatchMapping(“/user”)。","link":"/2020/06/15/6%20Spring/Spring%E5%8F%8A%E6%BA%90%E7%A0%81%E2%80%94%E2%80%94SpringMVC/"},{"title":"Spring及源码——IOC容器","text":"Spring及源码——IOC容器 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Spring作为Java Web开发的首选全栈式框架，了解其作用机制和源码不仅利于日常开发熟练使用，还能从最优秀的开源框架中学习编码风格和设计风格。由于Spring涵盖内容甚广，将分章节介绍从基础Spring后台开发、Spring Boot、Spring Cloud等顺序记录。一句话夸Spring：Spring是轻量级的开源框架，以IOC和AOP为核心功能，支持完备的企业级技术，拥有强大的三方整合能力。 特别说明：作为Java世界最熟悉的框架，该系列不会经常从基础概念和如何使用详细介绍，有时可能一语带过，有时甚至都不会去讲，注重实现逻辑和源码是重点。 1 IOC容器&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Spring能够一统轻量级WEB开发领域，IOC是最重要也是最核心的部分，正是由于IOC的存在，使Spring可以在创建对象Bean的过程中进行自由的排列组合，创造更多的奇迹成为了可能。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;IOC（Inversion Of Control）：其中突出两个重点控制和反转，对象之间的耦合关系由Spring托管负责，而实际对象不用关心关联对象如何创建，只需要关注使用即可。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DI（Dependency Injection）：依赖注入，此概念由业界泰斗Matin Fowler提出，在早期它是为了更加直观的解释IOC的理念，随后逐渐演变为与IOC相同概念的词汇。 虽然DI诠释了IOC的核心理念，但在当代开发语境中两者并不等价，DI依然保持对象之间耦合关系的创建形式含义，而IOC不仅包含DI，还包含Bean生命周期、代理、资源装载等概念。 1.1 DI注入方式&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;依赖注入方式包含：setter方法、构造器、工厂方法 三种。注解方式是实现DI依赖注入的有效方式并不能当作依赖类型。 以构造器注入为主，setter注入为辅。 以对象Human注入演示，其中POJO用到了lombok，如果不清楚可以参照标签“第三方工具”查找使用方式。 创建Human类，提供无参构造器、全参数构造器和指定参数构造器，成员变量都实现getter和setter 123456789101112131415161718192021222324252627@ToString@AllArgsConstructor@NoArgsConstructorpublic class Human { @Getter @Setter private int id; @Getter @Setter private String name; @Getter @Setter private int age; @Getter @Setter private boolean sex; public Human(int id, String name) { this.name = name; this.id = id; } public Human(String name, int age) { this.name = name; this.age = age; }} 1.1.1 配置方式注入12345678910111213141516171819202122232425262728293031323334353637&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:p=\"http://www.springframework.org/schema/p\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd\"&gt; &lt;!-- 1.通过setter实现参数注入，要求对象类提供无参构造器 --&gt; &lt;bean id=\"human1\" class=\"me.zhy.model.Human\" p:id=\"1\" p:name=\"xxx\" p:age=\"10\" p:sex=\"true\"/&gt; &lt;!-- 2.采用参数名实现构造器注入，要求对象类提供全参构造器 --&gt; &lt;bean id=\"human2\" class=\"me.zhy.model.Human\"&gt; &lt;constructor-arg name=\"id\" value=\"2\"/&gt; &lt;constructor-arg name=\"name\" value=\"yyy\"/&gt; &lt;constructor-arg name=\"age\" value=\"20\"/&gt; &lt;constructor-arg name=\"sex\" value=\"true\"/&gt; &lt;/bean&gt; &lt;!-- 3.采用索引实现构造器注入，要求对象类提供全参构造器 --&gt; &lt;bean id=\"human3\" class=\"me.zhy.model.Human\"&gt; &lt;constructor-arg index=\"0\" value=\"3\"/&gt; &lt;constructor-arg index=\"1\" value=\"zzz\"/&gt; &lt;constructor-arg index=\"2\" value=\"30\"/&gt; &lt;constructor-arg index=\"3\" value=\"false\"/&gt; &lt;/bean&gt; &lt;!-- 4.额外的两个构造器由于参数类型相同，为了区分，采用联合索引和参数名方式注入--&gt; &lt;bean id=\"human4\" class=\"me.zhy.model.Human\"&gt; &lt;constructor-arg index=\"0\" name=\"name\" value=\"www\"/&gt; &lt;constructor-arg index=\"1\" name=\"age\" value=\"40\"/&gt; &lt;/bean&gt;&lt;/beans&gt; 123456789101112131415161718192021// 测试public static void main(String[] args) { ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); Human human1 = (Human) context.getBean(\"human1\"); System.out.println(human1); Human human2 = (Human) context.getBean(\"human2\"); System.out.println(human2); Human human3 = (Human) context.getBean(\"human3\"); System.out.println(human3); Human human4 = (Human) context.getBean(\"human4\"); System.out.println(human4);}// output:// Human(id=1, name=xxx, age=10, sex=true)// Human(id=2, name=yyy, age=20, sex=true)// Human(id=3, name=zzz, age=30, sex=false)// Human(id=0, name=www, age=40, sex=false) 工厂方法和静态工厂方法 创建工厂类 123456789101112131415161718public class HumanFactory { // 无参工厂方法 public Human create1() { return new Human(); } // 全参工厂方法 public Human create2(int id, String name, int age, boolean sex) { return new Human(id, name, age, sex); } // 静态工厂方法，部分参数 public static Human staticCreate(int id, String name) { return new Human(id, name); }} 配置实现工厂方法注入 123456789101112131415161718&lt;!-- 创建工厂类 --&gt;&lt;bean id=\"factory\" class=\"me.zhy.model.HumanFactory\" /&gt;&lt;!-- 1. 无参工厂方法 --&gt;&lt;bean id=\"human1\" factory-bean=\"factory\" factory-method=\"create1\" /&gt;&lt;!-- 2. 全参工厂方法 --&gt;&lt;bean id=\"human2\" factory-bean=\"factory\" factory-method=\"create2\"&gt; &lt;constructor-arg name=\"id\" value=\"1\"/&gt; &lt;constructor-arg name=\"name\" value=\"xxx\"/&gt; &lt;constructor-arg name=\"age\" value=\"10\"/&gt; &lt;constructor-arg name=\"sex\" value=\"true\"/&gt;&lt;/bean&gt;&lt;!-- 3. 静态工厂方法 --&gt;&lt;bean id=\"human3\" class=\"me.zhy.model.HumanFactory\" factory-method=\"staticCreate\"&gt; &lt;constructor-arg name=\"id\" value=\"2\"/&gt; &lt;constructor-arg name=\"name\" value=\"yyy\"/&gt; &lt;constructor-arg name=\"age\" value=\"20\"/&gt; &lt;constructor-arg name=\"sex\" value=\"false\"/&gt;&lt;/bean&gt; 123456789101112131415// 测试ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(\"applicationContext.xml\");Human human1 = (Human) context.getBean(\"human1\");System.out.println(human1);Human human2 = (Human) context.getBean(\"human2\");System.out.println(human2);Human human3 = (Human) context.getBean(\"human3\");System.out.println(human3);// output:// Human(id=0, name=null, age=0, sex=false)// Human(id=1, name=xxx, age=10, sex=true)// Human(id=2, name=yyy, age=0, sex=false) 1.1.2 注解方式注入配置文件设置注解扫描 1&lt;context:component-scan base-package=\"me.zhy.model\" /&gt; 创建对象并通过注解实现注入 1234567891011121314151617181920212223242526272829303132333435363738@Data@Component // 标记为一般对象public class Department { private int id; private String name;}//@Data@Componentpublic class Location { private String name;}//@Component(\"staff\")@ToStringpublic class Staff { @Getter @Setter private int id; @Getter private Department dept; @Getter private Location location; // 通过Autowired将标记为Component的类通过set方法注入 @Autowired public void setDept(Department dept) { this.dept = dept; } @Autowired public void setLocation(Location location) { this.location = location; }} 通过构造器注入 12345678910111213141516@Component(\"staff\")@ToStringpublic class Staff { @Getter @Setter private int id; @Getter private Department dept; @Getter private Location location; @Autowired public Staff(Department dept, Location location) { this.dept = dept; this.location = location; }} 测试 123456ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(\"applicationContext.xml\");Staff staff = (Staff) context.getBean(\"staff\");System.out.println(staff);// output:// Staff(id=0, dept=Department(id=0, name=null), location=Location(name=null)) 1.1.3 依赖注入注解的区别&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在依赖注入时有多种注解可以选择，按照来源可以划分为如下： Spring注解： @Autowired，表示以类型注入，如果需要设置为以名字注入需要配合使用@Qualifier(value = “xxx”)使用，这两者都是Spring提供。 12345678910111213141516@Component(\"staff\")@ToStringpublic class Staff { @Getter @Setter private int id; @Getter @Setter @Autowired @Qualifier(value = \"department\") private Department dept; @Getter @Autowired @Qualifier(value = \"location\") private Location location;} 这里的 department 和 location 是@Componenent标记 Department 类和 Location 类时的默认名称。 @Inject功能等同于@Autowired，@Named等同于@Qualifier，区别在于，这两者都是由JDK提供。 @Resource默认使用名称匹配，也可以指定由类型匹配，由JDK提供。 1234567// 默认使用名称匹配@Resource(name = \"department\")private Department dept;// 指定类型匹配@Resource(type = Department.class)private Department dept; @Dao表示数据控制层对象，@Service表示服务层对象，@Controller表示控制层对象，@RestController表示Rest风格的控制层对象。 1.1.4 内部类注入&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于内部类在编译为字节码文件后通常表示为OuterClass$InnerClass ，即由外部类名和内部类名组成，所以在注入内部类时class应保持一致。 123456789class OuterClass { private InnerClass inner; public setInner(InnerClass inner) { this.inner = inner; } public class InnerClass {}} 注入内部类 12345&lt;bean id=\"outerClass\" class=\"xxx.xxx.OuterClass\"&gt; &lt;property name=\"inner\"&gt; &lt;bean class=\"xxx.xxx.OuterClass$InnerClass\" /&gt; &lt;/property&gt;&lt;/bean&gt; 1.1.5 循环依赖问题&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在实际开发中可能会碰到循环依赖的问题，所谓循环依赖是指：当多个类互相引用其他类作为自身成员变量，在构造对象时就会产生循环依赖。 依赖注入时，按照注入方式的不同Spring采取了不同解决策略： 构造器注入： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过构造器注入依赖对象时，由于注入对象尚未创建完成，所以在创建对象时会报错：Requested bean is currently in creation: Is there an unresolvable circular reference? 123456789101112131415161718192021222324252627282930313233343536@Component(\"classA\")public class ClassA { private ClassB classB; @Autowired public ClassA (ClassB classB) { this.classB = classB; }}@Component(\"classB\")public class ClassB { private ClassC classC; @Autowired public ClassB (ClassC classC) { this.classC = classC; }}@Component(\"classC\")public class ClassC { private ClassA classA; @Autowired public ClassC (ClassA classA) { this.classA = classA; }}// 测试ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(\"applicationContext.xml\");context.getBean(\"classA\");// 异常信息// Error creating bean with name 'classA': Requested bean is currently in creation: Is there an unresolvable circular reference? 通过构造器注入源码可以查看处理过程。 12345678910111213141516171819202122232425// spring 源码public class ConstructorResolver { public BeanWrapper autowireConstructor(String beanName, RootBeanDefinition mbd, @Nullable Constructor&lt;?&gt;[] chosenCtors, @Nullable Object[] explicitArgs) { ... // 创建构造器入参对象 argsHolder = this.createArgumentArray(beanName, mbd, resolvedValues, bw, paramTypes, paramNames, this.getUserDeclaredConstructor(candidate), autowiring, candidates.length == 1); ... } private ConstructorResolver.ArgumentsHolder createArgumentArray(String beanName, RootBeanDefinition mbd, @Nullable ConstructorArgumentValues resolvedValues, BeanWrapper bw, Class&lt;?&gt;[] paramTypes, @Nullable String[] paramNames, Executable executable, boolean autowiring, boolean fallback) throws UnsatisfiedDependencyException { ... try { // 执行注入参数创建时抛出异常 convertedValue = this.resolveAutowiredArgument(methodParam, beanName, autowiredBeanNames, (TypeConverter)converter, fallback); args.rawArguments[paramIndex] = convertedValue; args.arguments[paramIndex] = convertedValue; args.preparedArguments[paramIndex] = autowiredArgumentMarker; args.resolveNecessary = true; } catch (BeansException var24) { throw new UnsatisfiedDependencyException(mbd.getResourceDescription(), beanName, new InjectionPoint(methodParam), var24); } } Setter注入（Scope=prototype） &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Setter注入在针对单例对象和原型对象时存在处理差异，在原型模式下循环依赖创建对象依然抛出与构造器注入一样的异常信息Error creating bean with name 'classA': Requested bean is currently in creation: Is there an unresolvable circular reference?，由此可见，在多实例模式下，spring无法自主判断并创建依赖对象。 12345678910111213141516171819202122232425262728293031323334@Component(\"classA\")@Scope(\"prototype\")public class ClassA { private ClassB classB; @Autowired public void setClassB(ClassB classB) { this.classB = classB; }}@Component(\"classB\")@Scope(\"prototype\")public class ClassB { private ClassC classC; @Autowired public void setClassC(ClassC classC) { this.classC = classC; }}@Component(\"classC\")@Scope(\"prototype\")public class ClassC { private ClassA classA; @Autowired public void setClassA(ClassA classA) { this.classA = classA; }}// 获取classA对象时抛出异常// `Error creating bean with name 'classA': Requested bean is currently in creation: Is there an unresolvable circular reference?` Setter注入（Scope=singleton） &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在Spring注入策略中，默认使用单例对象，由于单例对象始终保持唯一性，所以可以将单例对象存入内存中，这是Spring解决循环依赖的前提之一；同时Setter注入本身是通过空参构造器创建实例对象后在调用setter方法设置属性，利用这一特性是解决循环依赖的另一个前提。 12345678910111213141516171819202122232425262728293031323334353637@Component(\"classA\")public class ClassA { private ClassB classB; @Autowired public void setClassB(ClassB classB) { this.classB = classB; }}@Component(\"classB\")public class ClassB { private ClassC classC; @Autowired public void setClassC(ClassC classC) { this.classC = classC; }}@Component(\"classC\")public class ClassC { private ClassA classA; @Autowired public void setClassA(ClassA classA) { this.classA = classA; }}// 测试ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(\"applicationContext.xml\");ClassA classA = (ClassA) context.getBean(\"classA\");System.out.println(classA);// output// me.zhy.model.ClassA@1787bc24 可以看到，在单例模式下循环依赖注入被spring解决了，前面已经提到，spring首先通过构造器创建实例对象，并将实例对象存入可对外暴露的缓存中，该缓存称作 earlySingletonObjects ，用于保存通过构造器创建但仍未setter赋值的对象，因此在发生循环依赖时，先通过缓存获得未完全赋值的对象并注入，解决循环依赖后再调用setter赋值。 1234567891011121314151617181920212223242526272829303132// spring源码public class DefaultSingletonBeanRegistry { // 缓存单例对象 private final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap(256); // 单例工厂对象 private final Map&lt;String, ObjectFactory&lt;?&gt;&gt; singletonFactories = new HashMap(16); // 缓存未完全创建的对象，当对象完全创建后，会删除 private final Map&lt;String, Object&gt; earlySingletonObjects = new HashMap(16); ... protected Object getSingleton(String beanName, boolean allowEarlyReference) { Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null &amp;&amp; this.isSingletonCurrentlyInCreation(beanName)) { synchronized(this.singletonObjects) { singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null &amp;&amp; allowEarlyReference) { ObjectFactory&lt;?&gt; singletonFactory = (ObjectFactory)this.singletonFactories.get(beanName); if (singletonFactory != null) { // 通过单例工厂创建单例对象 singletonObject = singletonFactory.getObject(); // 将创建但未setter的对象存入 this.earlySingletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); } } } } return singletonObject; }} 1.2 Bean的生命周期&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; IOC容器可以创建singleton单例对象和prototype多实例对象，对于多实例对象Bean，IOC负责创建之后则完全交给客户端程序；而单例对象Bean，IOC不仅负责创建，还负责单例对象在内存中的缓存已经整个生命周期，在客户端程序调用时通过缓存给予。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Spring装配Bean的过程中在关键步骤通过接口实现，就好像现代前端单页框架（如AngularJS2+ 框架等）在页面渲染过程中提供了 生命周期钩子 ，通过获取这些钩子可以加入我们扩展的功能，达到定制Bean的效果。 Spring装载Bean过程描述： 实例化 Bean。 依赖注入完成 Bean 中所有属性值的赋值。 BeanNameAware 接口， 调用 setBeanName() 方法传入当前 Bean 的 id 。（一般不做修改） BeanFactoryAware 接口，调用 setBeanFactory() 方法传入当前工厂实例的引用。（一般不做修改） ApplicationContextAware 接口，调用 setApplicationContext() 方法传入当前 ApplicationContext 实例的引用。（一般不做修改） BeanPostProcessor接口 称为 后处理器 ，通过调用postProcessBeforeInitialzation() 方法对 Bean 进行加工操作，此处非常重要，Spring 的 AOP、动态代理等功能 就是利用它实现的。 BeanPostProcessor 独立于Bean，通过类似附加插件的形式注册到IOC中，并通过反射为IOC扫描识别，当Spring创建任何Bean时都会产生作用，所以它的影响是 全局 的。 InitializingBean 接口，调用 afterPropertiesSet() 方法。 配置文件中通过 init-method 属性指定了初始化方法，则调用该初始化方法。 BeanPostProcessor接口的postProcessAfterInitializatio()方法，IOC容器再次对Bean进行加工处理。 区别于step 6，通过方法名就可以见名知意，分别是before和after，即在之前和之后进行加工处理。 如果Bean 的作用范围为 scope=”singleton”，则将该 Bean 放入 Spring IoC 的缓存池中，将触发 Spring 对该 Bean 的生命周期管理； 如果Bean 的作用范围为 scope=”prototype”，则将该 Bean 交给调用者，调用者管理该 Bean 的生命周期，Spring 不再管理该 Bean。 如果 Bean 实现了 DisposableBean 接口，则 Spring 会调用 destory() 方法将 Spring 中的 Bean 销毁；如果在配置文件中通过 destory-method 属性指定了 Bean 的销毁方法，则 Spring 将调用该方法对 Bean 进行销毁。 通过代码举例来验证Spring装配Bean的过程：首先需要自定义BeanPostProcessor对象并聚合到容器中，再创建Bean中依次实现BeanNameAware 、BeanFactoryAware 、ApplicationContextAware 、InitializingBean 和 DisposableBean 接口。 首先自定义一个后处理器，实现 postProcessBeforeInitialization() 方法和 postProcessAfterInitialization()方法，分别只针对Human类型的Bean做特殊化处理。 1234567891011121314151617181920212223242526// 将自定义后处理器加入IOC容器会自动生效@Componentpublic class MyBeanPostProcessor implements BeanPostProcessor { @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException { if (\"human\".equals(beanName)) { Human human = (Human) bean; human.setName(\"zhangsan\"); System.out.println(\"=&gt; MyBeanPostProcessor.postProcessBeforeInitialization(): change name zhangsan\"); return human; } return bean; } @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { if (\"human\".equals(beanName)) { Human human = (Human) bean; human.setSex(false); System.out.println(\"=&gt; MyBeanPostProcessor.postProcessAfterInitialization(): change sex female\"); return human; } return bean; }} 接着定义类，实现生命周期钩子接口并打印。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// 自定义类，实现生命周期钩子接口，并假如IOC容器中@Component(\"human\")@Datapublic class Human implements BeanNameAware, BeanFactoryAware, ApplicationContextAware, InitializingBean, DisposableBean { private int id; private String name; private boolean sex; private BeanFactory beanFactory; private String beanName; private ApplicationContext applicationContext; // BeanFactoryAware接口 @Override public void setBeanFactory(BeanFactory beanFactory) throws BeansException { System.out.println(\"=&gt; BeanFactory.setBeanFactory()\"); this.beanFactory = beanFactory; } // BeanNameAware接口 @Override public void setBeanName(String s) { System.out.println(\"=&gt; BeanNameAware.setBeanName()\"); this.beanName = s; } // DisposableBean接口 @Override public void destroy() throws Exception { System.out.println(\"=&gt; DisposableBean.destroy()\"); } // InitializingBean接口 @Override public void afterPropertiesSet() throws Exception { System.out.println(\"=&gt; InitializingBean.afterPropertiesSet()\"); } // ApplicationContextAware接口 @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException { System.out.println(\"=&gt; ApplicationContextAware.setApplicationContext()\"); this.applicationContext = applicationContext; }} 通过IOC容器调用，查看Bean的创建过程。 12345678910111213141516171819// 测试， 伪代码ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(\"applicationContext.xml\");Human human = (Human) context.getBean(\"human\"); // 创建过程context.destroy(); // 销毁容器即可调用DisposableBean接口System.out.println(human);// output// =&gt; BeanNameAware.setBeanName()// =&gt; BeanFactory.setBeanFactory()// =&gt; ApplicationContextAware.setApplicationContext()// =&gt; MyBeanPostProcessor.postProcessBeforeInitialization(): change name zhangsan// =&gt; InitializingBean.afterPropertiesSet()// =&gt; MyBeanPostProcessor.postProcessAfterInitialization(): change sex female// org.springframework.context.support.ClassPathXmlApplicationContext - Closing org.springframework.context.support.ClassPathXmlApplicationContext@2f0a87b3, started on Tue Jul 07 23:13:37 CST 2020// =&gt; DisposableBean.destroy()// Human(id=0, name=zhangsan, sex=false) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;总结：为了减少代码的侵入性，不推荐直接使用 InitializingBean 接口和 DisposableBean 接口，可以通过配置&lt;init-method&gt; 和 &lt;destroy-method&gt; 来达到相同的目的，当然也可以通过 @PostConstruct 和 @PreDestroy方法注解完成相同操作；一般情况下，除非基于Spring框架定制开发，否则不会用到 BeanNameAware, BeanFactoryAware, ApplicationContextAware, InitializingBean, DisposableBean 这5个接口；而BeanPostProcessor 则完全不同，由于它类似于插件形式供Spring使用，并且基于此之上可以完成注入AOP等重要功能，所以它对于Bean的定制化扩展很重要。","link":"/2020/06/15/6%20Spring/Spring%E5%8F%8A%E6%BA%90%E7%A0%81%E2%80%94%E2%80%94IOC/"},{"title":"Shiro","text":"Shiro &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Apache Shiro是一个功能强大且易于使用的Java安全框架，为开发人员提供了一个直观而全面的解决方案，用于身份验证、授权、加密和会话管理。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;实际上，它实现了管理应用程序安全性的所有方面，同时尽可能避免出现问题。它建立在完善的接口驱动设计和面向对象的原则之上，可以在任何你想象得到的地方实现自定义行为。但是，对于所有事情来说，默认情况下都是合理的，这与应用程序安全性是一样的。 1 QuickStart&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该小节通过官方给出的 QuickStart 项目来创建一个简单的应用，并通过分析源码来对 Shiro 形成一个整体印象，具体步骤如下： 创建 maven 工程，导入如下依赖： 12345678910111213141516171819202122&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-core&lt;/artifactId&gt; &lt;version&gt;1.5.3&lt;/version&gt;&lt;/dependency&gt;&lt;!-- configure logging --&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;jcl-over-slf4j&lt;/artifactId&gt; &lt;version&gt;1.7.30&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-simple&lt;/artifactId&gt; &lt;version&gt;1.7.30&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;commons-logging&lt;/groupId&gt; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt;&lt;/dependency&gt; 在 resources 下创建日志文件 log4j.properties 和 shiro 配置文件 shiro.ini。 12345678910111213141516171819202122232425262728293031323334# [shiro.ini]# -----------------------------------------------------------------------------# Users and their assigned roles## Each line conforms to the format defined in the# org.apache.shiro.realm.text.TextConfigurationRealm#setUserDefinitions JavaDoc# -----------------------------------------------------------------------------[users]# user 'root' with password 'secret' and the 'admin' roleroot = secret, admin# user 'guest' with the password 'guest' and the 'guest' roleguest = guest, guest# user 'presidentskroob' with password '12345' (\"That's the same combination on# my luggage!!!\" ;)), and role 'president'presidentskroob = 12345, president# user 'darkhelmet' with password 'ludicrousspeed' and roles 'darklord' and 'schwartz'darkhelmet = ludicrousspeed, darklord, schwartz# user 'lonestarr' with password 'vespa' and roles 'goodguy' and 'schwartz'lonestarr = vespa, goodguy, schwartz# -----------------------------------------------------------------------------# Roles with assigned permissions## Each line conforms to the format defined in the# org.apache.shiro.realm.text.TextConfigurationRealm#setRoleDefinitions JavaDoc# -----------------------------------------------------------------------------[roles]# 'admin' role has all permissions, indicated by the wildcard '*'admin = *# The 'schwartz' role can do anything (*) with any lightsaber:schwartz = lightsaber:*# The 'goodguy' role is allowed to 'drive' (action) the winnebago (type) with# license plate 'eagle5' (instance specific id)goodguy = winnebago:drive:eagle5 12345678910111213141516171819# [log4j.properties]log4j.rootLogger=INFO, stdoutlog4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m %n# General Apache librarieslog4j.logger.org.apache=WARN# Springlog4j.logger.org.springframework=WARN# Default Shiro logginglog4j.logger.org.apache.shiro=INFO# Disable verbose logginglog4j.logger.org.apache.shiro.util.ThreadContext=WARNlog4j.logger.org.apache.shiro.cache.ehcache.EhCache=WARN 创建一个测试类 QuickStart，通过main函数调用。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class Quickstart { private static final transient Logger log = LoggerFactory.getLogger(Quickstart.class); public static void main(String[] args) { // 通过加载 shiro.ini 创建 Shiro SecurityManager对象 Factory&lt;SecurityManager&gt; factory = new IniSecurityManagerFactory(\"classpath:shiro.ini\"); SecurityManager securityManager = factory.getInstance(); SecurityUtils.setSecurityManager(securityManager); // shiro 环境就设置好了，接下来看看能做些什么 // 1 获得当前执行用户 Subject currentUser = SecurityUtils.getSubject(); // 2 可以使用 shiro Session 对象做一些事情，比如赋值和获取值 Session session = currentUser.getSession(); session.setAttribute(\"someKey\", \"aValue\"); String value = (String) session.getAttribute(\"someKey\"); if (value.equals(\"aValue\")) { log.info(\"Retrieved the correct value! [\" + value + \"]\"); } // 3 对当前用户授权 if (!currentUser.isAuthenticated()) { UsernamePasswordToken token = new UsernamePasswordToken(\"lonestarr\", \"vespa\"); token.setRememberMe(true); try { currentUser.login(token); } catch (UnknownAccountException uae) { log.info(\"There is no user with username of \" + token.getPrincipal()); } catch (IncorrectCredentialsException ice) { log.info(\"Password for account \" + token.getPrincipal() + \" was incorrect!\"); } catch (LockedAccountException lae) { log.info(\"The account for username \" + token.getPrincipal() + \" is locked. \" + \"Please contact your administrator to unlock it.\"); } catch (AuthenticationException ae) { //unexpected condition? error? } } // 打印 log.info(\"User [\" + currentUser.getPrincipal() + \"] logged in successfully.\"); // 测试当前用户的角色 if (currentUser.hasRole(\"schwartz\")) { log.info(\"May the Schwartz be with you!\"); } else { log.info(\"Hello, mere mortal.\"); } // 测试当前用户的权限 if (currentUser.isPermitted(\"lightsaber:wield\")) { log.info(\"You may use a lightsaber ring. Use it wisely.\"); } else { log.info(\"Sorry, lightsaber rings are for schwartz masters only.\"); } if (currentUser.isPermitted(\"winnebago:drive:eagle5\")) { log.info(\"You are permitted to 'drive' the winnebago with license plate (id) 'eagle5'. \" + \"Here are the keys - have fun!\"); } else { log.info(\"Sorry, you aren't allowed to drive the 'eagle5' winnebago!\"); } // 退出 currentUser.logout(); System.exit(0); }} 简单来说，通过 shiro.ini 加载创建了 SecurityManager 对象，通过该对象可以获取当前的执行用户 Subject 对象，当前用户的管理内容包括：会话管理、授权和验证。在授权阶段通过用户名和密码就可以创建一个令牌 token，之后可以验证当前用户的权限信息。 2 Shiro 概述&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Shiro 所谓的“应用程序安全性的四个基石”为目标-身份验证，授权，会话管理和密码术： 身份验证：有时称为“登录”，这是证明用户就是他们所说的身份的行为。 授权：访问控制的过程，即确定“谁”有权访问“什么”。 会话管理：即使在非Web或EJB应用程序中，也管理用户特定的会话。 密码：使用密码算法保持数据安全，同时仍易于使用。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在不同的应用程序环境中，还具有其他功能来支持和加强这些问题，尤其是： Web支持：Shiro的Web支持API可帮助轻松保护Web应用程序。 缓存：缓存是Apache Shiro API的第一层公民，可确保安全操作保持快速和高效。 并发性：Apache Shiro的并发功能支持多线程应用程序。 测试：测试支持可帮助您编写单元测试和集成测试，并确保您的代码将按预期进行保护。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Shiro 包含三个核心组件：SecurityManager、Subject、Realms。 SecurityManager：它是 Shiro 框架的核心管理器，属于门面对象，提供安全管理的各种服务。 Subject：“当前操作用户”，它既可以指当前操作用户，也可以指正在执行的第三方进程等。 Realme：它封装了数据源，向 Shiro 提供相关数据，也就是说，Shiro 会从 Realme 中查找用户权限信息。 3 SecurityManager&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SecurityManager 是 Shiro 体系结构的核心，该对象协调其内部安全组件，这些内部安全组件一起形成对象图。一旦为应用程序配置了SecurityManager及其内部对象图，通常就不理会它，几乎所有的时间都花在SubjectAPI 上。在Shiro的默认SecurityManager实现中，安全操作管理包含： Authentication，认证 Authorization，授权 Session Management，会话管理 Cache Management，缓存管理 Realm coordination，领域协调 Event propagation，时间传播 123456789public interface SecurityManager extends Authenticator, Authorizer, SessionManager { Subject login(Subject subject, AuthenticationToken authenticationToken) throws AuthenticationException; void logout(Subject subject); Subject createSubject(SubjectContext context);} 其实现类包括 AuthenticatingSecurityManager、AuthorizingSecurityManager、CachingSecurityManager、DefaultSecurityManager、RealmSecurityManager、SessionsSecurityManager，下面以 DefaultSecurityManager 为例进行源码分析: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697public class DefaultSecurityManager extends SessionsSecurityManager { // 通过内部集合的方式管理素有的授权和验证操作 private Collection&lt;Realm&gt; realms; // 创建所属 Realme的管理器 public DefaultSecurityManager(Realm singleRealm) { this(); setRealm(singleRealm); } /* *登录方法 *如果登录成功将创建 Subject 实例来替代验证账户的id，并绑定到应用中 */ public Subject login(Subject subject, AuthenticationToken token) throws AuthenticationException { AuthenticationInfo info; // 验证失败将以异常的形式抛出 try { info = authenticate(token); } catch (AuthenticationException ae) { try { onFailedLogin(token, ae, subject); } catch (Exception e) { if (log.isInfoEnabled()) { log.info(\"onFailedLogin method threw an \" + \"exception. Logging and propagating original AuthenticationException.\", e); } } throw ae; } // 验证成功，创建 Subject 实例对象 Subject loggedIn = createSubject(token, info, subject); onSuccessfulLogin(token, info, loggedIn); return loggedIn; } /** * 注销方法 * 一旦注销成功将删除本次 Subject 对象 */ public void logout(Subject subject) { if (subject == null) { throw new IllegalArgumentException(\"Subject method argument cannot be null.\"); } beforeLogout(subject); PrincipalCollection principals = subject.getPrincipals(); if (principals != null &amp;&amp; !principals.isEmpty()) { if (log.isDebugEnabled()) { log.debug(\"Logging out subject with primary principal {}\", principals.getPrimaryPrincipal()); } Authenticator authc = getAuthenticator(); if (authc instanceof LogoutAware) { ((LogoutAware) authc).onLogout(principals); // 先从认证器中删除principals } } try { delete(subject); // 删除 subject 对象 } catch (Exception e) { if (log.isDebugEnabled()) { String msg = \"Unable to cleanly unbind Subject. Ignoring (logging out).\"; log.debug(msg, e); } } finally { try { stopSession(subject); // 关闭session } catch (Exception e) { if (log.isDebugEnabled()) { String msg = \"Unable to cleanly stop Session for Subject [\" + subject.getPrincipal() + \"] \" + \"Ignoring (logging out).\"; log.debug(msg, e); } } } }}//public abstract class RealmSecurityManager extends CachingSecurityManager { /// 通过内部集合的方式管理素有的授权和验证操作 private Collection&lt;Realm&gt; realms; public void setRealms(Collection&lt;Realm&gt; realms) { if (realms == null) { throw new IllegalArgumentException(\"Realms collection argument cannot be null.\"); } if (realms.isEmpty()) { throw new IllegalArgumentException(\"Realms collection argument cannot be empty.\"); } this.realms = realms; afterRealmsSet(); }} 4 Subject&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Subject 称为当前“用户”，这个用户可以使执行用户，也可以是任意第三方进程，通过Subject可以获得当前用户和 Session。 123Subject subject = SecurityUtils.getSubject(); Session session = subject.getSession();session.setAttribute( \"someKey\", \"aValue\" ); 首先看一下 getSubject() 源码，作为静态类获得当前用户，Shiro 通过线程与“用户”绑定，通过线程上下文就可以获得当前线程对应的用户。 123456789101112131415public static Subject getSubject() { // 从线程上下文获得 Subject subject = ThreadContext.getSubject(); // 如果用户为空，则创建用户并绑定到ThreadContext中 if (subject == null) { subject = (new Subject.Builder()).buildSubject(); ThreadContext.bind(subject); } return subject;}public abstract class ThreadContext { // 线程上下文使用 ThreadLocal 绑定当前线程和用户 private static final ThreadLocal&lt;Map&lt;Object, Object&gt;&gt; resources = new InheritableThreadLocalMap&lt;Map&lt;Object, Object&gt;&gt;();} 在 Session 中，Shiro 默认使用 DelegatingSession，它的方法名与 HttpServletSession 保持一致，当我们开启 Web 服务后，Shiro 的 Session 会自动实现 HttpServletSession，该对象也是 Shiro 中封装的。 1234public class HttpServletSession implements Session { // 通过组合的方式，将HttpSession封装在对象内，这样在前端使用时也可以识别 private HttpSession httpSession = null;} Subject 最重要的功能就是判断当前用户的权限和角色，这样就可以在 Realme 中进行相应的 授权和验证工作。 1234567891011121314151617181920212223// 登录try { subject.login(token); // 没有抛异常则登录成功} catch ( UnknownAccountException uae ) { System.out.println(\"用户名不存在\");} catch ( IncorrectCredentialsException ice ) { System.out.println(\"密码错误\");} catch ( LockedAccountException lae ) { System.out.println(\"用户被锁定，不能登录\");} catch ( AuthenticationException ae ) { System.out.println(\"严重的错误\");}// 注销subject.logout();// 获得当前用户String currentUser = subject.getPrincipal().toString();// 判断用户是否是拥有角色boolean isRole = subject.hasRole( \"admin\" );// 是否拥有权限boolean isPer = subject.isPermitted(\"user:add\"); 5 Realme（保留） 多Realme的使用 6 SpringBoot 集成 Shiro 完整示例 导入 shiro 依赖: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-spring&lt;/artifactId&gt; &lt;version&gt;1.4.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.23&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.1.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt;&lt;/dependency&gt; 整合mybatis的代码，修改数据库，添加 password 字段和 permission 字段，分辨用于管理用户密码和权限。 同时修改 User 类、UserMapper接口和 UserMapper.xml。 1234567891011121314151617@NoArgsConstructor@AllArgsConstructor@ToStringpublic class User { @Getter @Setter private Integer id; @Getter @Setter private String name; @Getter @Setter private String password; @Getter @Setter private String permission;} 12345678910@Mapper@Repositorypublic interface UserMapper { List&lt;User&gt; getUsers(); User getUserByID(Integer id); User getUser(String username);} 123456789101112131415&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"me.zhy.integrate.entity.mapper.UserMapper\"&gt; &lt;select id=\"getUsers\" resultType=\"User\"&gt; select * from tb_user; &lt;/select&gt; &lt;select id=\"getUserByID\" resultType=\"User\" parameterType=\"int\"&gt; select * from tb_user where id = #{id}; &lt;/select&gt; &lt;select id=\"getUser\" resultType=\"User\" parameterType=\"String\"&gt; select * from tb_user where `name` = #{username}; &lt;/select&gt;&lt;/mapper&gt; 创建服务类，用于从数据库获取真实数据 1234567891011121314@Servicepublic class UserService { private UserMapper userMapper; public User getUser(String username) { return userMapper.getUser(username); } @Autowired public void setUserMapper(UserMapper userMapper) { this.userMapper = userMapper; }} 创建自定义 Realme，用于实现授权和验证的具体实现，数据部分采用数据库中的真实 tb_user 数据： 123456789101112131415161718192021222324252627282930313233343536public class UserShiroRealme extends AuthorizingRealm { private UserService userService; // 授权 @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principalCollection) { SimpleAuthorizationInfo info = new SimpleAuthorizationInfo(); Subject subject = SecurityUtils.getSubject(); // 获得当前用户 User currentUser = (User) subject.getPrincipal(); info.addStringPermission(currentUser.getPermission()); // 添加当前用户的权限 return info; } // 验证 @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken authenticationToken) throws AuthenticationException { // 获得 controller 封装的前台用户名和密码 token UsernamePasswordToken token = (UsernamePasswordToken) authenticationToken; String username = token.getUsername(); User user = userService.getUser(username); // 从数据库读取数据 if (Objects.isNull(user)) { // 对象为空，验证失败 return null; } String password = user.getPassword(); // user 将在授权中继续使用， password 将被 shiro 自行密文验证 return new SimpleAuthenticationInfo(user, password, \"\"); } @Autowired public void setUserService(UserService userService) { this.userService = userService; }} 创建 Shiro 配置类，向 IOC 容器注册，包含核心组件 Realme、SecurityManager 和 过滤器链 ShiroFilterFactoryBean，其中 SecurityManager 使用与 Web 相关的 DefaultWebSecurityManager： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package me.zhy.integrate.config.shiro;import me.zhy.integrate.entity.domain.User;import me.zhy.integrate.service.UserService;import org.apache.shiro.SecurityUtils;import org.apache.shiro.authc.*;import org.apache.shiro.authz.AuthorizationInfo;import org.apache.shiro.authz.SimpleAuthorizationInfo;import org.apache.shiro.realm.AuthorizingRealm;import org.apache.shiro.session.Session;import org.apache.shiro.subject.PrincipalCollection;import org.apache.shiro.subject.Subject;import org.springframework.beans.factory.annotation.Autowired;import java.util.Objects;public class UserShiroRealme extends AuthorizingRealm { private UserService userService; // 授权 @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principalCollection) { SimpleAuthorizationInfo info = new SimpleAuthorizationInfo(); Subject subject = SecurityUtils.getSubject(); // 获得当前用户 User currentUser = (User) subject.getPrincipal(); info.addStringPermission(currentUser.getPermission()); // 添加当前用户的权限 return info; } // 验证 @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken authenticationToken) throws AuthenticationException { // 获得 controller 封装的前台用户名和密码 token UsernamePasswordToken token = (UsernamePasswordToken) authenticationToken; String username = token.getUsername(); User user = userService.getUser(username); // 从数据库读取数据 if (Objects.isNull(user)) { // 对象为空，验证失败 return null; } // 验证成功，将对象放入shiro.Session中 Subject subject = SecurityUtils.getSubject(); Session session = subject.getSession(); session.setAttribute(\"user\", user); String password = user.getPassword(); // user 将在授权中继续使用， password 将被 shiro 自行密文验证 return new SimpleAuthenticationInfo(user, password, \"\"); } @Autowired public void setUserService(UserService userService) { this.userService = userService; }} 在 step 3 中过滤器链通过 Map&lt;String, String&gt; 方式设置，其中 key表示拦截的 url 路径，value表示过滤器，Shiro 在 DefaultFilter 枚举类中提供了多种内置过滤器，这些组件分为两类：认证过滤器和授权过滤器，在文档中介绍如下： 认证过滤器： anon ：org.apache.shiro.web.filter.authc.AnonymousFilter，可以匿名使用 authc ：org.apache.shiro.web.filter.authc.FormAuthenticationFilter，需要认证(登录)才能使用 authcBasic ：org.apache.shiro.web.filter.authc.BasicHttpAuthenticationFilter，httpBasic认证 user ：org.apache.shiro.web.filter.authz.UserFilter，示必须存在用户，当登入时不做检查 授权过滤器： ssl ：org.apache.shiro.web.filter.authz.SslFilter，https请求使用 port ：org.apache.shiro.web.filter.authz.PortFilter，参数绑定端口号使用 rest ：org.apache.shiro.web.filter.authz.HttpMethodPermissionFilter，参数绑定rest行为使用，如get perms ：org.apache.shiro.web.filter.authz.PermissionAuthorizationFilter，权限类别，包含多个参数 roles ：org.apache.shiro.web.filter.authz.RolesAuthorizationFilter，角色类别，包含多个参数 启动 SpringBoot 验证，zhangsan 用户具有 user:* 权限可以全部访问；lisi 用户具有 user:page1 权限，只能访问 page1 ，page2将提示授权失败；wangwu 用户同理。","link":"/2020/04/01/10%20%E4%B8%AD%E9%97%B4%E4%BB%B6/Shiro/"},{"title":"Spring及源码——SpringBoot（二）","text":"Spring及源码——SpringBoot（二） 6 SpringBoot如何实现无 web.xml 启动&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在 Servlet 发展史上，每个 web 容器都是通过加载 web.xml 来启动，在 web.xml 中定义了若干参数配置、监听器、过滤器以及Servlet，通过容器实例化后将对象保存在 ServletContext 中供全局使用。在 tomcat8 版本中开始全面采用全新的 Servlet 3.1 规范，规范中包含 以代码方式配置 web 容器。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过查阅 Spring 示例文档，其中提到了无参数配置，并给出了如下的例子： 1234567891011121314151617181920public class MyWebApplicationInitializer implements WebApplicationInitializer { @Override public void onStartup(ServletContext servletCxt) { // Load Spring web application configuration // 创建 ioc 容器 AnnotationConfigWebApplicationContext ac = new AnnotationConfigWebApplicationContext(); ac.register(AppConfig.class); ac.refresh(); // Create and register the DispatcherServlet //基于java代码的方式初始化DispatcherServlet DispatcherServlet servlet = new DispatcherServlet(ac); // 动态创建 Servlet 并向 ServletContext 属性列表中注册对象。 ServletRegistration.Dynamic registration = servletCxt.addServlet(\"app\", servlet); registration.setLoadOnStartup(1); registration.addMapping(\"/app/*\"); }} 这个例子虽然简单，但可以看出来 springboot 是如何在不配置 web.xml 情况下向 ServletContext 注册 Servlet 对象。在 springboot 应用实际启动中是通过 DispatcherServletAutoConfiguration 自动装配来实现的。 7 SpringBoot 在 web 领域的应用7.1 加载静态文件&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在 WebMvcAutoConfiguration 自动配置类中，定义了资源加载的过程，其中首先会从配置中读取静态资源加载位置 spring.mvc.static-path-pattern= ，也是最常用的方法，其次在没有配置路径时先通过 /webjars/** 加载静态资源 123456789101112131415161718192021222324public class WebMvcAutoConfiguration { public void addResourceHandlers(ResourceHandlerRegistry registry) { // 1. 配置了静态资源路径 if (!this.resourceProperties.isAddMappings()) { logger.debug(\"Default resource handling disabled\"); } else { Duration cachePeriod = this.resourceProperties.getCache().getPeriod(); CacheControl cacheControl = this.resourceProperties.getCache().getCachecontrol().toHttpCacheControl(); // 2. 从 /webjars/** 下加载静态资源 if (!registry.hasMappingForPattern(\"/webjars/**\")) { this.customizeResourceHandlerRegistration(registry.addResourceHandler(new String[]{\"/webjars/**\"}).addResourceLocations(new String[]{\"classpath:/META-INF/resources/webjars/\"}).setCachePeriod(this.getSeconds(cachePeriod)).setCacheControl(cacheControl)); } // 3. 从mvc参数配置类中加载路径 String staticPathPattern = this.mvcProperties.getStaticPathPattern(); if (!registry.hasMappingForPattern(staticPathPattern)) { this.customizeResourceHandlerRegistration(registry.addResourceHandler(new String[]{staticPathPattern}).addResourceLocations(WebMvcAutoConfiguration.getResourceLocations(this.resourceProperties.getStaticLocations())).setCachePeriod(this.getSeconds(cachePeriod)).setCacheControl(cacheControl)); } } } // ...} webjars webjars支持以 maven 依赖的方式引入静态文件，以该方法引入的静态文件路径需要满足：classpath:/META-INF/resources/webjars/ 的约束。 123456&lt;!-- 通过 maven 依赖引入静态文件 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.webjars&lt;/groupId&gt; &lt;artifactId&gt;jquery&lt;/artifactId&gt; &lt;version&gt;3.4.1&lt;/version&gt;&lt;/dependency&gt; 在路径 classpath:/META-INF/resources/webjars/jquery/3.4.1/jquery.js 就会在工程中存在。 配置类路径加载 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在配置类路径中以如下路径优先级加载：”classpath:/META-INF/resources/“ &gt; “classpath:/resources/“ &gt; “classpath:/static/“ &gt; “classpath:/public/“。 12345public class ResourceProperties { private static final String[] CLASSPATH_RESOURCE_LOCATIONS = new String[]{\"classpath:/META-INF/resources/\", \"classpath:/resources/\", \"classpath:/static/\", \"classpath:/public/\"}; // ...} 7.2 欢迎页的定制&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 在 WebMvcAutoConfiguration 自动配置类中可以定义欢迎页的定制，加载路径与上一例子中的路径加载保持一致。 12345678910111213141516@Beanpublic WelcomePageHandlerMapping welcomePageHandlerMapping(ApplicationContext applicationContext, FormattingConversionService mvcConversionService, ResourceUrlProvider mvcResourceUrlProvider) { WelcomePageHandlerMapping welcomePageHandlerMapping = new WelcomePageHandlerMapping(new TemplateAvailabilityProviders(applicationContext), applicationContext, this.getWelcomePage(), this.mvcProperties.getStaticPathPattern()); welcomePageHandlerMapping.setInterceptors(this.getInterceptors(mvcConversionService, mvcResourceUrlProvider)); return welcomePageHandlerMapping;}private Optional&lt;Resource&gt; getWelcomePage() { String[] locations = WebMvcAutoConfiguration.getResourceLocations(this.resourceProperties.getStaticLocations()); return Arrays.stream(locations).map(this::getIndexHtml).filter(this::isReadable).findFirst();}// 首页名称private Resource getIndexHtml(String location) { return this.resourceLoader.getResource(location + \"index.html\");} 7.3 Thymeleaf模板引擎&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;开源的模板引擎有很多，包括：JSP、Velocity、Freemarker、Thymeleaf等，SpringBoot 官方推荐使用 Thymeleaf 引擎。模板引擎使用都大同小异，具体使用规则可以参照：https://www.thymeleaf.org/ 在 springboot 中 thymeleaf 都在 classpath: templates/ 下使用，可以通过依赖引入： 123456789&lt;dependency&gt; &lt;groupId&gt;org.thymeleaf&lt;/groupId&gt; &lt;artifactId&gt;thymeleaf-spring5&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.thymeleaf.extras&lt;/groupId&gt; &lt;artifactId&gt;thymeleaf-extras-java8time&lt;/artifactId&gt;&lt;/dependency&gt; 7.4 SpringMVC的扩展&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;WebMvcConfigurer 是 重要的配置类接口，该接口内部方法都不是抽象方法，而是空方法，通过该接口可以扩展我们需要的功能，比如我们现在需要向 SpringMVC 中添加自定义的视图解析器，就可以自定义实现，如下： 1234567891011121314151617181920212223import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.web.servlet.View;import org.springframework.web.servlet.ViewResolver;import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;import java.util.Locale;@Configurationpublic class MyWebMvcConfig implements WebMvcConfigurer { @Bean public ViewResolver myViewResolver() { return new MyViewResolver(); } private static class MyViewResolver implements ViewResolver { @Override public View resolveViewName(String s, Locale locale) throws Exception { return null; } }} 通过标记该类为配置类并通过 @Bean 向 IOC 中注册，就可以实现自定义的 SpringMVC 功能扩展，同理，如果扩展其他功能只要继续注册即可。 甚至，可以通过 @EnbleWebMvc 接管整个 WebMvc 的控制，但需要主要使用该注解则 springboot 的功能将全部失效。 在实际开发过程中，如果不是单体应用，基本上都已经实现前后端分离，SpringBoot 只负责开发微服务功能并对外提供接口，本节介绍的内容全部都由前端框架控制。 7.5 执行异步功能当后台同步执行耗时任务时，前台请求会出于等待状态，这是无法忍受的，SpringBoot 支持通过@EnableAsync 开始异步任务支持，并在业务类中标记@Async开启异步执行。 创建耗时任务和请求。 1234567891011121314151617181920212223242526@Servicepublic class AsyncService { public void asyncMethod() { try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); } }}@RestControllerpublic class AsyncController { @Autowired private AsyncService asyncService; @GetMapping(\"/async\") public String asyncTest() { asyncService.asyncMethod(); return \"test\"; }}// 此时请求 /async 会长时间等待3秒 开启异步支持。 12345678910111213141516171819202122232425// SpringBoot 主类开启异步支持@EnableAsync@SpringBootApplicationpublic class App { public static void main(String[] args) { SpringApplication.run(App.class, args); }}@Servicepublic class AsyncService { @Async public void asyncMethod() { try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); } }}// 当再次请求 /async 时，会即使返回接口结果，耗时任务将异步执行 7.6 执行定时器任务&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SpringBoot集成了定时器任务，通过@EnableSchdualing开启定时器开关，在定时任务方法上使用@Scheduled执行，同样采用 Cron 表达式定义执行时间。 12345678910111213141516171819@EnableScheduling@SpringBootApplicationpublic class App { public static void main(String[] args) { SpringApplication.run(App.class, args); }}@Servicepublic class TimerTask { // Cron： 秒 分 时 日 月 周 @Scheduled(cron = \"0 * * * * ?\") public void execute() { System.out.println(\"执行任务\"); }}","link":"/2020/07/01/6%20Spring/Spring%E5%8F%8A%E6%BA%90%E7%A0%81%E2%80%94%E2%80%94SpringBoot%EF%BC%88%E4%BA%8C%EF%BC%89/"},{"title":"使用Swagger","text":"使用Swagger &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Swagger 是一个规范和完整的框架，用于生成、描述、调用和可视化 RESTful 风格的 Web 服务。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当前 WEB 项目以前后端分离为主，后端主要提供 REST API 给前端，前端框架负责数据绑定、路由等，所以 Swagger 不仅可以保证前后端开发的一致性和及时性，还能有效提高开发效率。 1 Swagger 导入12345678910&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.6.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.6.1&lt;/version&gt;&lt;/dependency&gt; 创建 Swagger 配置类相 IOC 容器注册。 123456789101112131415161718192021222324@Configuration@EnableSwagger2public class Swagger { @Bean public Docket docket(){ return new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo()) .select() .apis(RequestHandlerSelectors .basePackage(xxx.xxx.controller\")) .paths(PathSelectors.any()) .build(); } public ApiInfo apiInfo(){ return new ApiInfoBuilder() .title(\"swagger title\") .description(\"swagger desc\") .termsOfServiceUrl(\"\") .version(\"1.0\") .build(); }} 2 Swagger 注解&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Swagger 注解用于 Controller 层，系统运行后会自动扫描注解并最终通过 web-ui 展示。 @Api : 用在类上，说明该类的主要作用。 @ApiOperation：用在方法上，给API增加方法说明； @ApiImplicitParams : 用在方法上，包含一组参数说明； @ApiImplicitParam：用来注解来给方法入参增加说明。 @ApiResponses：用于表示一组响应。 @ApiResponse：用在@ApiResponses中，一般用于表达一个错误的响应信息 @ApiModel：用在返回对象类上，描述一个Model的信息（一般用在请求参数无法使用@ApiImplicitParam注解进行描述的时候） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@RestController@RequestMapping(\"/emp\")@Api(value = \"用户管理类\")public class EmployeeController { @Autowired private EmployeeReposiroty employeeReposiroty; @PostMapping(value = \"/employee\") @ApiOperation(value = \"新增用户\", notes = \"返回新增对象\") @ApiImplicitParam(paramType = \"query\", name = \"employee\", value = \"用户\", required = true) @ApiResponse(code = 400, message = \"参数错误\", response = String.class) public String insert(Employee employee){ // ... } @DeleteMapping(value = \"/employee/{id}\") @ApiOperation(value = \"删除用户\",notes = \"id删除用户\") @ApiImplicitParam(paramType = \"path\",name = \"id\",value = \"用户id\",required = true,dataType = \"Integer\") @ApiResponse(code = 400,message = \"参数错误\",response = String.class) public String delete(@PathVariable(\"id\")Integer id){ // ... } @PutMapping(value = \"/employee/{id}\") @ApiOperation(value = \"修改信息\",notes = \"id修改用户\") public String update(Employee employee){ // ... } @GetMapping(value = \"/employee/query\") @ApiOperation(value = \"查询用户\",notes = \"升序查询用户\") public List&lt;Employee&gt; findAll(){ // ... } @GetMapping(value = \"/employee/query/page\") @ApiOperation(value = \"分页查询\",notes = \"\") @ApiImplicitParams({ @ApiImplicitParam(paramType = \"query\",name = \"sort\",value = \"排序:asc|desc\",dataType = \"String\",required = true), @ApiImplicitParam(paramType = \"query\",name = \"pagenumber\",value = \"第几页\",dataType = \"Integer\",required = true), @ApiImplicitParam(paramType = \"query\",name = \"pageSize\",value = \"分页数\",dataType = \"Integer\",required = true) }) public List&lt;Employee&gt; findAllByPage(String sort,Integer pagenumber,Integer pageSize){ // .... } } 3 Swagger 界面和操作&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过地址 http://localhost:8080/swagger-ui.html 访问 swagger ui页面。 在页面端模拟数据访问接口","link":"/2020/06/20/%E4%B8%89%E6%96%B9%E5%B7%A5%E5%85%B7/%E4%BD%BF%E7%94%A8Swagger/"},{"title":"日志系统","text":"日志系统 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现代日志系统主要通过 SLF4J 日志门面框架 (Simple Logging Facade) 集成各实际使用日志框架，如 log4j、log4j2、logback等。由于 log4j2 是 log4j 的升级版本，所以主要介绍 log4j2 和 logback 的详细配置。 1 SLF4J&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在早期日志系统的演化过程中，日志都是强依赖于某个特定的框架，如果遇到系统升级或者更换日志框架就会因为侵入式使用而备受煎熬，SLF4J是一款日志门面框架，它不提供特定的日志功能，只是整合系统具体使用的日志框架，比如开发中用SLF4J定义日志系统，具体框架可以使Log4J或者Logback，二者更换依赖也不会影响现有系统的使用。 SLF4J does not rely on any special class loader machinery. In fact, each SLF4J binding is hardwired at compile time to use one and only one specific logging framework SLF4J不依赖于任何特殊的类装入器机制。实际上，每个SLF4J绑定都是在编译时硬连接的，以使用且仅使用一个特定的日志记录框架 2.1 Log4j2 配置文件详解&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;log4j2 不再支持 prorperties 配置文件，仅支持 xml 和 json 文件。系统按照以下顺序从 classpath 路径下匹配日志配置文件 : log4j2-text.json =&gt; log4j2-text.xml =&gt; log4j2.json =&gt; log4j2.xml。 完整的 xml 配置文件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!-- status用于设置log4j2自身内部的信息输出，可以不设置 --&gt;&lt;configuration&gt; &lt;!--全局参数--&gt; &lt;Properties&gt; &lt;Property name=\"pattern\"&gt;[%d{yyyy-MM-dd HH:mm:ss.SSS}] %-5level %class{36} [line:%L] %M - %msg%xEx%n&lt;/Property&gt; &lt;Property name=\"rootFileName\"&gt;./logs/root.log&lt;/Property&gt; &lt;Property name=\"debugFileName\"&gt;./logs/debug.log&lt;/Property&gt; &lt;Property name=\"druidFileName\"&gt;./logs/druid-sql.log&lt;/Property&gt; &lt;Property name=\"rootFilePattern\"&gt;logs/$${date:yyyy-MM}/root-%d{yyyy-MM-dd}-%i.log.gz&lt;/Property&gt; &lt;Property name=\"druidFilePattern\"&gt;logs/$${date:yyyy-MM}/druid-%d{yyyy-MM-dd}-%i.log.gz&lt;/Property&gt; &lt;Property name=\"maxSize\"&gt;50 MB&lt;/Property&gt; &lt;/Properties&gt; &lt;appenders&gt; &lt;!-- 控制台配置 --&gt; &lt;Console name=\"Console\" target=\"SYSTEM_OUT\"&gt; &lt;ThresholdFilter level=\"DEBUG\" onMatch=\"ACCEPT\" onMismatch=\"DENY\"/&gt; &lt;PatternLayout&gt; &lt;Pattern&gt;${pattern}&lt;/Pattern&gt; &lt;/PatternLayout&gt; &lt;/Console&gt; &lt;!-- 处理INFO级别的日志，并放到logs/info.log文件中，接受 INFO 及以上的日志--&gt; &lt;RollingFile name=\"RootLog\" fileName=\"${rootFileName}\" filePattern=\"${rootFilePattern}\"&gt; &lt;Filters&gt; &lt;ThresholdFilter level=\"INFO\"/&gt; &lt;/Filters&gt; &lt;PatternLayout&gt; &lt;Pattern&gt;${pattern}&lt;/Pattern&gt; &lt;/PatternLayout&gt; &lt;Policies&gt; &lt;SizeBasedTriggeringPolicy size=\"${maxSize}\"/&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;!-- 处理DEBUG级别的日志--&gt; &lt;RollingFile name=\"DebugLog\" fileName=\"${debugFileName}\" filePattern=\"${rootFilePattern}\"&gt; &lt;ThresholdFilter level=\"DEBUG\"/&gt; &lt;PatternLayout&gt; &lt;Pattern&gt;${pattern}&lt;/Pattern&gt; &lt;/PatternLayout&gt; &lt;Policies&gt; &lt;SizeBasedTriggeringPolicy size=\"${maxSize}\"/&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;!-- Druid的日志记录追加器 --&gt; &lt;RollingFile name=\"DruidLog\" fileName=\"${druidFileName}\" filePattern=\"${druidFilePattern}\"&gt; &lt;PatternLayout&gt; &lt;Pattern&gt;${pattern}&lt;/Pattern&gt; &lt;/PatternLayout&gt; &lt;Policies&gt; &lt;SizeBasedTriggeringPolicy size=\"100 MB\"/&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;/appenders&gt; &lt;loggers&gt; &lt;root level=\"DEBUG\"&gt; &lt;appender-ref ref=\"RootLog\"/&gt; &lt;appender-ref ref=\"Console\"/&gt; &lt;appender-ref ref=\"DebugLog\"/&gt; &lt;/root&gt; &lt;!--过滤掉spring和mybatis的一些无用的DEBUG信息--&gt; &lt;logger name=\"org.springframework\" level=\"INFO\" /&gt; &lt;logger name=\"org.mybatis\" level=\"INFO\" /&gt; &lt;!-- 记录druid-sql的记录 --&gt; &lt;logger name=\"druid.sql.Statement\" level=\"DEBUG\" additivity=\"false\"&gt; &lt;appender-ref ref=\"DruidLog\"/&gt; &lt;/logger&gt; &lt;/loggers&gt;&lt;/configuration&gt; 日志级别包括: OFF &gt; FATAL &gt; ERROR &gt; WARN &gt; INFO &gt; DEBUG &gt; TRACE &gt; ALL。 All : 打开所有日志记录。 TRACE: 追踪日志记录。 DEBUG : 调试应用程序是非常有帮助。 INFO : 粗粒度级别上突出强调应用程序的运行过程。 WARN : 输出警告的日志。 ERROR : 输出错误信息日志。 FATAL : 输出每个严重的错误事件将会导致应用程序的退出的日志。 OFF : 关闭所有日志记录。 Configuration有两个属性:status和monitorinterval,有两个子节点:Appenders和Loggers。status用来指定log4j本身的打印日志的级别，monitorinterval用于指定log4j自动重新配置的监测间隔时间，单位是s,最小是5s。 Appenders节点，常见的有三种子节点:Console、RollingFile、File： Console节点用来定义输出到控制台的Appender： name:指定Appender的名字; target:SYSTEM_OUT 或 SYSTEM_ERR,一般只设置默认:SYSTEM_OUT; PatternLayout:输出格式，不设置默认为:%m%n。 File节点用来定义输出到指定位置的文件的Appender： name:指定Appender的名字； fileName:指定输出日志的目的文件带全路径的文件名； PatternLayout:输出格式，不设置默认为:%m%n。 RollingFile节点用来定义超过指定大小自动删除旧的创建新的的Appender： name:指定Appender的名字； fileName:指定输出日志的目的文件带全路径的文件名； PatternLayout:输出格式，不设置默认为:%m%n； filePattern:指定新建日志文件的名称格式； Filters:决定日志事件能否被输出，过滤条件有三个值：ACCEPT、DENY、NEUTRAL。 ThresholdFilter: 如果目标 level 大于等于配置中的 level ,那么该日志进入过滤，由过滤策略决定是保留（OnMatch=”ACCEPT”）还是舍弃（OnMatch=”DENY”）,默认策略是NEUTRAL，即进入下一级过滤器判断，如果当前过滤器是最后一个过滤器，则为ACCEPT；如果不匹配则进入 onMisMatch ,默认 DENY。 123456789# 打个比方&lt;Filters&gt; &lt;ThresholdFilter level=&quot;INFO&quot;/&gt; &lt;ThresholdFilter level=&quot;WARN&quot; onMatch=&quot;DENY&quot; onMismatch=&quot;NEUTRAL&quot;/&gt;&lt;/Filters&gt;# OFF &gt; FATAL &gt; ERROR &gt; WARN &gt; INFO &gt; DEBUG &gt; TRACE &gt; ALL# 该过滤器表示,如果当前日志 level &gt;= INFO，则进入默认匹配 onMatch=&quot;NEUTRAL&quot;，NEUTRAL表示进入下# 一级过滤器；下一级过滤器表示，如果当前日志 level &gt;= WARN 则匹配onMatch=&quot;DENY&quot;拒绝接收，所以该# 过滤器表示只接收 INFO 日志 Policies:指定滚动日志的策略，就是什么时候进行新建日志文件输出日志； TimeBasedTriggeringPolicy:Policies子节点，基于时间的滚动策略，interval属性用来指定多久滚动一次，默认是1 hour。modulate=true用来调整时间：比如现在是早上3am，interval是4，那么第一次滚动是在4am，接着是8am，12am…而不是7am； SizeBasedTriggeringPolicy:Policies子节点，基于指定文件大小的滚动策略，size属性用来定义每个日志文件的大小； DefaultRolloverStrategy:用来指定同一个文件夹下最多有几个日志文件时开始删除最旧的，创建新的(通过max属性)。 Loggers节点，常见的有两种:Root和Logger： Root节点用来指定项目的根日志，如果没有指定Logger，那么就会默认使用该Root日志输出。 Logger节点用来单独指定日志的形式，比如要为指定包下的class指定不同的日志级别等。 3 Log4j2 和 SLF4J&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当前环境： JDK 11 Maven 3.6.3 （国内采用aliyun镜像） 1234567891011121314151617&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-api&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-slf4j-impl&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; 注意，在个别文章中介绍依赖时，还引入了slf4j，这里需要说明的是，如果引入了log4j-slf4j-impl就无需在单独引入slf4j，否则在日志工厂 LoggerFactory 获得具体日志实现框架时会因为存在多个日志框架而产生问题。 12345678910111213141516171819202122// 测试import org.slf4j.Logger;import org.slf4j.LoggerFactory;public class App { private static Logger LOG = LoggerFactory.getLogger(App.class); public static void main(String[] args) { LOG.debug(\"{} say hi\", \"debug\"); LOG.info(\"{} say hi\", \"info\"); LOG.warn(\"{} say hi\", \"warn\"); LOG.error(\"{} say hi\", \"error\"); }}// output:// [17:33:26.532] DEBUG org.example.App [line:14] main - debug say hi// [17:33:26.533] INFO org.example.App [line:15] main - info say hi// [17:33:26.533] WARN org.example.App [line:16] main - warn say hi// [17:33:26.533] ERROR org.example.App [line:17] main - error say hi// ./logs 目录下创建日志文件debug.log、 root.log、 druid-sql.log 4 Logback 配置及应用&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Logback是由 log4j 创始人设计的另一个开源日志框架，它取代 Log4J 的理由如下： 更快的实现：Logback的重写内核，在关键执行路径上性能提升10倍以上; 更小的内存：初始化内存加载更小； 非常自然实现了SLF4j：充分利用 SLF4J 的接口去实现，减少门面框架适配所带来的中间层开销； 由于 Logback 配置与 Log4J2 很相似，所以具体配置就不多介绍，直接给出较为完整的配置： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;configuration debug=\"false\"&gt; &lt;!-- 全局参数 --&gt; &lt;Property name=\"encoding\" value=\"UTF-8\" /&gt; &lt;Property name=\"rootFileName\" value=\"./logs/root.log\" /&gt; &lt;Property name=\"debugFileName\" value=\"./logs/debug.log\" /&gt; &lt;Property name=\"rootFilePattern\" value=\"./logs/$${date:yyyy-MM}/root-%d{yyyy-MM-dd}-%i.log.gz\" /&gt; &lt;Property name=\"debugFilePattern\" value=\"./logs/$${date:yyyy-MM}/debug-%d{yyyy-MM-dd}-%i.log.gz\" /&gt; &lt;Property name=\"maxSize\" value=\"50 MB\" /&gt; &lt;Property name=\"maxHistory\" value=\"30\" /&gt; &lt;Property name=\"pattern\" value=\"[%d{yyyy-MM-dd HH:mm:ss.SSS}] %-5level %class{36} [line:%L] %M - %msg%xEx%n\" /&gt; &lt;!-- %d{yyyy-MM-dd HH:mm:ss.SSS} [%-5level] %logger - %msg%n --&gt; &lt;!-- %d{yyyy-MM-dd HH:mm:ss} %-4relative [%thread] %-5level %logger{35} - %msg %n --&gt; &lt;!-- [ %-5level] [%date{yyyy-MM-dd HH:mm:ss.SSS}] %logger{96} [%line] [%thread]- %msg%n --&gt; &lt;!-- 控制台输出 --&gt; &lt;appender name=\"Console\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt; &lt;encoder class=\"ch.qos.logback.classic.encoder.PatternLayoutEncoder\"&gt; &lt;charset&gt;${encoding}&lt;/charset&gt; &lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符--&gt; &lt;pattern&gt;${pattern}&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- Root日志 --&gt; &lt;appender name=\"RootLog\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;file&gt;${rootFileName}&lt;/file&gt; &lt;filter class=\"ch.qos.logback.classic.filter.ThresholdFilter\"&gt; &lt;level&gt;INFO&lt;/level&gt; &lt;/filter&gt; &lt;encoder class=\"ch.qos.logback.classic.encoder.PatternLayoutEncoder\"&gt; &lt;charset&gt;${encoding}&lt;/charset&gt; &lt;pattern&gt;${pattern}&lt;/pattern&gt; &lt;/encoder&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\"&gt; &lt;FileNamePattern&gt;${rootFilePattern}&lt;/FileNamePattern&gt; &lt;!--日志文件保留天数--&gt; &lt;MaxHistory&gt;${maxHistory}&lt;/MaxHistory&gt; &lt;/rollingPolicy&gt; &lt;!--日志文件最大的大小--&gt; &lt;triggeringPolicy class=\"ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy\"&gt; &lt;MaxFileSize&gt;${maxSize}&lt;/MaxFileSize&gt; &lt;/triggeringPolicy&gt; &lt;/appender&gt; &lt;!-- Debug日志 --&gt; &lt;appender name=\"DebugLog\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;file&gt;${debugFileName}&lt;/file&gt; &lt;filter class=\"ch.qos.logback.classic.filter.ThresholdFilter\"&gt; &lt;level&gt;DEBUG&lt;/level&gt; &lt;/filter&gt; &lt;encoder class=\"ch.qos.logback.classic.encoder.PatternLayoutEncoder\"&gt; &lt;charset&gt;${encoding}&lt;/charset&gt; &lt;pattern&gt;${pattern}&lt;/pattern&gt; &lt;/encoder&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\"&gt; &lt;FileNamePattern&gt;${debugFilePattern}&lt;/FileNamePattern&gt; &lt;!--日志文件保留天数--&gt; &lt;MaxHistory&gt;${maxHistory}&lt;/MaxHistory&gt; &lt;/rollingPolicy&gt; &lt;!--日志文件最大的大小--&gt; &lt;triggeringPolicy class=\"ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy\"&gt; &lt;MaxFileSize&gt;${maxSize}&lt;/MaxFileSize&gt; &lt;/triggeringPolicy&gt; &lt;/appender&gt; &lt;!-- 日志输出级别 --&gt; &lt;root level=\"DEBUG\"&gt; &lt;appender-ref ref=\"Console\" /&gt; &lt;appender-ref ref=\"RootLog\" /&gt; &lt;appender-ref ref=\"DebugLog\" /&gt; &lt;/root&gt;&lt;/configuration&gt; 123456789101112131415161718&lt;!-- pom --&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.25&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-core&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt; &lt;/dependency&gt; 12345678910111213141516171819202122// 测试import org.slf4j.Logger;import org.slf4j.LoggerFactory;public class App { private static Logger LOG = LoggerFactory.getLogger(App.class); public static void main(String[] args) { LOG.debug(\"{} say hi\", \"debug\"); LOG.info(\"{} say hi\", \"info\"); LOG.warn(\"{} say hi\", \"warn\"); LOG.error(\"{} say hi\", \"error\"); }}// output// [2020-07-11 19:02:38.822] DEBUG org.example.App [line:14] main - debug say hi// [2020-07-11 19:02:38.825] INFO org.example.App [line:15] main - info say hi// [2020-07-11 19:02:38.825] WARN org.example.App [line:16] main - warn say hi// [2020-07-11 19:02:38.825] ERROR org.example.App [line:17] main - error say hi// 输出root.log、debug.log文件 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从配置文件中可以看出 Logback 是接受定制化开发的，即自定义类并实现对应的接口，最后在配置文件class中配置，最常用的定制化开发就是针对过滤器 Filter，通过实现ch.qos.logback.core.filter.Filter接口，在 decide() 方法中重写逻辑，就可以实现自定义过滤器，最后通过配置即可使用。 1234567891011121314import ch.qos.logback.classic.spi.ILoggingEvent;import ch.qos.logback.core.filter.Filter;import ch.qos.logback.core.spi.FilterReply;public class MyFilter extends Filter&lt;ILoggingEvent&gt; { @Override public FilterReply decide(ILoggingEvent iLoggingEvent) { String msg = iLoggingEvent.getMessage(); if (Strings.isNullOrEmpty(msg) &amp;&amp; \"MYFILTER\".equals(msg))){ return FilterReply.ACCEPT; } return FilterReply.DENY; }} 123&lt;filter class=\"xxx.xxx.MyFilter\"&gt; &lt;level&gt;MYFILTER&lt;/level&gt;&lt;/filter&gt;","link":"/2020/07/01/%E4%B8%89%E6%96%B9%E5%B7%A5%E5%85%B7/%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F/"},{"title":"Spring及源码——SpringBoot（三）","text":"Spring及源码——SpringBoot（三） 8 SpringBoot 整合 JDBC 导入jdbc相关依赖和数据库驱动。 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 在配置文件中设置数据源。 123456spring: datasource: username: root password: java url: jdbc:mysql://localhost:3306/demo?serverTimezone=UTC&amp;useUnicode=true&amp;characterEncoding=utf-8 driver-class-name: com.mysql.cj.jdbc.Driver 在测试类中测试。 12345678910111213141516171819202122232425@SpringBootTestclass IntegrateJdbcApplicationTests { @Autowired DataSource dataSource; @Autowired JdbcTemplate jdbcTemplate; @Test void contextLoads() throws Exception { String sql1 = \"insert into tb_user(name) values('lisi')\"; String sql2 = \"select * from tb_user\"; jdbcTemplate.execute(sql1); List&lt;Map&lt;String, Object&gt;&gt; ret = jdbcTemplate.queryForList(sql2); System.out.println(ret); } // 可以正常调用数据库 @Test void datasource() { System.out.println(dataSource); } // 输出 HikariDataSource} 从上述数据源输出中可以看到，SpringBoot 默认使用 Hikari 数据连接池，Hikari 号称当前运行效率最高的数据连接池（期待与Druid的全面比较），通过查询自动配置类 DataSourceAutoConfiguration 源码，可以看到目前支持的数据连接池类型。 1234567891011121314151617181920212223242526@Configuration( proxyBeanMethods = false)@ConditionalOnClass({DataSource.class, EmbeddedDatabaseType.class})@ConditionalOnMissingBean( type = {\"io.r2dbc.spi.ConnectionFactory\"})// 数据源的参数配置类@EnableConfigurationProperties({DataSourceProperties.class})@Import({DataSourcePoolMetadataProvidersConfiguration.class, DataSourceInitializationConfiguration.class})public class DataSourceAutoConfiguration { @Configuration( proxyBeanMethods = false ) @Conditional({DataSourceAutoConfiguration.PooledDataSourceCondition.class}) @ConditionalOnMissingBean({DataSource.class, XADataSource.class}) // 支持的数据连接池 @Import({Hikari.class, Tomcat.class, Dbcp2.class, Generic.class, DataSourceJmxConfiguration.class}) protected static class PooledDataSourceConfiguration { protected PooledDataSourceConfiguration() { } } // ...} 9 SpringBoot 整合 Druid 数据连接池&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从上述例子中可以看到 SpringBoot 支持 Hikari、Tomcat、Dbcp2、Generic、DataSourceJmxConfiguration 数据连接池，作为国产优秀开源数据连接池 Druid ，不仅有可观的执行速度，还提供了非常强大的后台监控功能，在技术选型中使用的也非常普遍，所以本节介绍 SpringBoot 整合 Druid 。 导入 Druid 依赖和数据库驱动。 123456789101112131415161718&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.23&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 如果不引入 log4j 依赖，会导致启动Druid数据源时报错 --&gt;&lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt;&lt;/dependency&gt; 如果不导入 log4j ，启动 SpringBoot 可能出现如下错误提示： APPLICATION FAILED TO START Description: Failed to bind properties under ‘spring.datasource’ to javax.sql.DataSource: Property: spring.datasource.filters Value: stat,wall,log4j Origin: class path resource [application.yml]:25:14 Reason: org.apache.log4j.PriorityAction: Update your application’s configuration 配置自定义数据连接池 12345678spring: datasource: username: root password: java url: jdbc:mysql://localhost:3306/demo?serverTimezone=UTC&amp;useUnicode=true&amp;characterEncoding=utf-8 driver-class-name: com.mysql.cj.jdbc.Driver # 自定义数据源 type: com.alibaba.druid.pool.DruidDataSource 配置数据连接池相关信息，最终通过 @ConfigurationProperties 将所有数据源信息装配到 Druid 实例对象中。 12345678910111213141516171819202122232425262728spring: datasource: username: root password: java url: jdbc:mysql://localhost:3306/demo?serverTimezone=UTC&amp;useUnicode=true&amp;characterEncoding=utf-8 driver-class-name: com.mysql.cj.jdbc.Driver # 自定义数据源 type: com.alibaba.druid.pool.DruidDataSource #Spring Boot 默认是不注入这些属性值的，需要自己绑定 #druid 数据源专有配置 initialSize: 5 minIdle: 5 maxActive: 20 maxWait: 60000 timeBetweenEvictionRunsMillis: 60000 minEvictableIdleTimeMillis: 300000 validationQuery: SELECT 1 testWhileIdle: true testOnBorrow: false testOnReturn: false poolPreparedStatements: true #配置监控统计拦截的filters，stat:监控统计、log4j：日志记录、wall：防御sql注入 filters: stat,wall,log4j maxPoolPreparedStatementPerConnectionSize: 20 useGlobalDataSourceStat: true connectionProperties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=500 将配置信息装配到 Druid Data Source 实例对象中。 12345678910@Configurationpublic class DruidDataSourceConfig { // 将 spring.datasource中的配置信息全部配置到 DruidDataSource 中，不再让 SpringBoot 绑定 @ConfigurationProperties(prefix = \"spring.datasource\") @Bean public DataSource druidDataSource() { return new DruidDataSource(); }} 测试数据连接池 12345678910111213@SpringBootTestclass IntegrateJdbcApplicationTests { @Autowired DataSource dataSource; @Test void dataSource() { System.out.println(dataSource.getClass()); }}// output:// class com.alibaba.druid.pool.DruidDataSource 采用向 ServletContext 注册的方式提交 Druid 监控 Servlet，通过 com.alibaba.druid.support.http.ResourceServlet 可以查看配置参数名，并通过Map保存。 123456789101112131415161718192021222324252627282930@Configurationpublic class DruidDataSourceConfig { @ConfigurationProperties(prefix = \"spring.datasource\") @Bean public DataSource druidDataSource() { return new DruidDataSource(); } // 采用向 ServletContext 注册的方式提交 Druid 监控 Servlet @Bean public ServletRegistrationBean druidMonitorInit() { ServletRegistrationBean servlet = new ServletRegistrationBean(new StatViewServlet(), \"/druid/*\"); // com.alibaba.druid.support.http.ResourceServlet 提供配置参数名 Map&lt;String, String&gt; monitorMap = new HashMap&lt;&gt;(); monitorMap.put(\"loginUsername\", \"admin\"); monitorMap.put(\"loginPassword\", \"admin\"); // 允许访问 // monitorMap.put(\"allow\", \"localhost\"); // 只有本机可以访问 monitorMap.put(\"allow\", \"\"); // 为空或null，表示允许所有人访问 // 拒绝访问 // monitorMap.put(\"deny\", \"127.0.0.1\"); // 禁止此ip访问 // 参数初始化 servlet.setInitParameters(monitorMap); return servlet; }} 通过 url 访问 http://localhost:8080/druid/index.html。 10 SpringBoot 整合 MyBatis&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Spring 的 JPA 是 ORM 框架的轻量级封装，默认采用 Hibernate 框架，作为我国 ORM 的主要生产框架，Mybatis 与 SpringBoot 的整合尤为重要。 导入 mybatis 依赖。 12345&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.1.1&lt;/version&gt;&lt;/dependency&gt; 配置文件创建数据源，与 9 小节保持一致，不在复述。除了数据源还可选择其他 Mybatis 配置。 123456# datasource 配置省略# Mybatismybatis: type-aliases-package: me.zhy.integrate.entity.domain # 由于 IDE 问题，最好将mapper文件放在 classpath 下创建 mapper-locations: classpath:mybatis/mapper/*.xml 创建 User 类 和对应的 Mapper 接口。 123456789101112131415161718192021@NoArgsConstructor@AllArgsConstructor@ToStringpublic class User { @Getter @Setter private Integer id; @Getter @Setter private String name;}@Mapper@Repositorypublic interface UserMapper { List&lt;User&gt; getUsers(); User getUser(Integer id);} 配置 Mapper 映射文件 UserMapper.xml（创建在 classpath:mybatis/mapper/UserMapper.xml）。 1234567891011121314&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"me.zhy.integrate.entity.mapper.UserMapper\"&gt; &lt;select id=\"getUsers\" resultType=\"User\"&gt; select * from tb_user; &lt;/select&gt; &lt;select id=\"getUser\" resultType=\"User\" parameterType=\"int\"&gt; select * from tb_user where id = #{id}; &lt;/select&gt;&lt;/mapper&gt; 测试。 123456789101112131415@SpringBootTestclass IntegrateJdbcApplicationTests { @Autowired private UserMapper userMapper; @Test void contextLoads() throws Exception { List&lt;User&gt; users = userMapper.getUsers(); System.out.println(users); }}// output:// [User(id=1, name=zhangsan), User(id=2, name=lisi), User(id=3, name=lisi)] 11 SpringBoot 整合 安全框架&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在 Web 安全框架领域最常用的分别是 SpringSecurity 和 Shiro，本节将分别介绍 SpringBoot 对安全框架的整合以及简单应用。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在 Web 安全领域，最重要的两件事是：认证和授权，安全框架也主要围绕这主题展开设计。 11.1 SpringBoot 整合 SpringSecurity&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SpringSecurity 核心采用一组过滤器链，加持在客户端请求进入接口 API 的过程中。核心组件包括 ： WebSecurityConfigurerAdapter，用于自定义 Security 策略； AuthenticationManagerBuilder，用于认证策略； @EnableWebSecurity：开启安全模式。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先，简单创建几个页面用于测试，包含：index.html，login.html，page1/1.html，page1/2.html，page2/1.html，page2/2.html，通过index.html可以跳转至其他页面。 使用 SpringSecurity 的具体步骤如下： 导入spring security 相关依赖。 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt; 创建 Spring Security 的 Java 配置类，该配置会创建一个过滤器，称为 SpringSecurityFilterChain ，该过滤器链负责主要的安全任务包括：保护应用的URLS、验证提交后的用户名和密码、登录界面重定向等。 123456789101112131415@EnableWebSecuritypublic class SecurityConfig extends WebSecurityConfigurerAdapter { // 授权 @Override protected void configure(HttpSecurity http) throws Exception { super.configure(http); } // 认证 @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception { super.configure(auth); }} 由于采用向 ServletContext 动态注册过滤器，可以在不影响正常业务代码的情况下使用，即不具有侵入性。为了更形象的展示 Security 的能力，以下分别对 授权 和 认证 进行测试。 测试1：授权测试 1234567891011121314151617181920212223242526@EnableWebSecuritypublic class SecurityConfig extends WebSecurityConfigurerAdapter { /** * 授权： * 对于访问根目录和index页面权限对所有人放行 * 访问page1下的页面需要用户角色 v1 * 访问page2下的页面需要用户角色 v2 */ @Override protected void configure(HttpSecurity http) throws Exception { http.authorizeRequests() .antMatchers(\"/\", \"/index\").permitAll() .antMatchers(\"/page1/**\").hasRole(\"v1\") .antMatchers(\"/page2/**\").hasRole(\"v2\"); // 无权限将跳转到登录页，如果没有登录页会跳转 security 提供的默认登录页 http.formLogin(); } // 认证 @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception { super.configure(auth); }} 经过测试发现 index 页面正常访问，跳转 page1 或 page2 时被拦截并提示 403，拒绝访问。 测试2：认证测试 1234567891011121314151617181920212223242526272829303132333435363738394041@EnableWebSecuritypublic class SecurityConfig extends WebSecurityConfigurerAdapter { /** * 授权： * 对于访问根目录和index页面权限对所有人放行 * 访问page1下的页面需要用户角色 v1 * 访问page2下的页面需要用户角色 v2 */ @Override protected void configure(HttpSecurity http) throws Exception { http.authorizeRequests() .antMatchers(\"/\", \"/index\").permitAll() .antMatchers(\"/page1/**\").hasRole(\"v1\") .antMatchers(\"/page2/**\").hasRole(\"v2\"); // 无权限将跳转到登录页，如果没有登录页会跳转 security 提供的默认登录页 http.formLogin(); } /** * 认证： * admin 用户 具有权限 v1 、v2 * me 用户 具有权限 v1 * * 采用 security 推荐的 BCrypt 加密方式 */ @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception { // 内存中授权 auth.inMemoryAuthentication().passwordEncoder(new BCryptPasswordEncoder()) .withUser(\"admin\") .password(new BCryptPasswordEncoder().encode(\"admin\")) .roles(\"v1\", \"v2\") .and() .withUser(\"me\") .password(new BCryptPasswordEncoder().encode(\"me\")) .roles(\"v1\"); }} 重启应用后测试，使用spring security 默认提供的登录框，当 admin 用户登录后因为具有 v1 和 v2角色，所有页面都可以访问；当 me 用户登录后因为具有 v1 角色，page1 可以正常访问，page2 会被拒绝访问 403。 还值得注意一点，在使用 @EnableWebSecurity 开启安全保护之后，默认启用 CSRF（跨站请求攻击），会针对 Patch、Post、Put、Delete进行防护。避免 CSRF 攻击最常用的手段就是客户端每次请求时都需要提交 token ，服务端会比较 token 的一致性，当不一致时会拒绝访问。 11.2 SpringBoot 整合 Shiro&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Shiro 是 Apache 下的开源项目，是一个强大且易用的 JAVA 安全框架，安全任务包含验证、授权、加密和会话管理，Shiro 提供了易于理解的 API，可以快速构建。Shiro 包含三个核心组件：SecurityManager、Subject、Realms。 SecurityManager：它是 Shiro 框架的核心管理器，属于门面对象，提供安全管理的各种服务。 Subject：“当前操作用户”，它既可以指当前操作用户，也可以指正在执行的第三方进程等。 Realme：它封装了数据源，向 Shiro 提供相关数据，也就是说，Shiro 会从 Realme 中查找用户权限信息。 导入 shiro 依赖: 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-spring&lt;/artifactId&gt; &lt;version&gt;1.4.1&lt;/version&gt;&lt;/dependency&gt; 使用之前整合mybatis的代码，修改数据库，添加 password 字段和 permission 字段，分辨用于管理用户密码和权限。 同时修改 User 类、UserMapper接口和 UserMapper.xml。 1234567891011121314151617@NoArgsConstructor@AllArgsConstructor@ToStringpublic class User { @Getter @Setter private Integer id; @Getter @Setter private String name; @Getter @Setter private String password; @Getter @Setter private String permission;} 12345678910@Mapper@Repositorypublic interface UserMapper { List&lt;User&gt; getUsers(); User getUserByID(Integer id); User getUser(String username);} 123456789101112131415&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"me.zhy.integrate.entity.mapper.UserMapper\"&gt; &lt;select id=\"getUsers\" resultType=\"User\"&gt; select * from tb_user; &lt;/select&gt; &lt;select id=\"getUserByID\" resultType=\"User\" parameterType=\"int\"&gt; select * from tb_user where id = #{id}; &lt;/select&gt; &lt;select id=\"getUser\" resultType=\"User\" parameterType=\"String\"&gt; select * from tb_user where `name` = #{username}; &lt;/select&gt;&lt;/mapper&gt; 创建服务类，用于从数据库获取真实数据 1234567891011121314@Servicepublic class UserService { private UserMapper userMapper; public User getUser(String username) { return userMapper.getUser(username); } @Autowired public void setUserMapper(UserMapper userMapper) { this.userMapper = userMapper; }} 创建自定义 Realme，用于实现授权和验证的具体实现，数据部分采用数据库中的真实 tb_user 数据： 123456789101112131415161718192021222324252627282930313233343536public class UserShiroRealme extends AuthorizingRealm { private UserService userService; // 授权 @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principalCollection) { SimpleAuthorizationInfo info = new SimpleAuthorizationInfo(); Subject subject = SecurityUtils.getSubject(); // 获得当前用户 User currentUser = (User) subject.getPrincipal(); info.addStringPermission(currentUser.getPermission()); // 添加当前用户的权限 return info; } // 验证 @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken authenticationToken) throws AuthenticationException { // 获得 controller 封装的前台用户名和密码 token UsernamePasswordToken token = (UsernamePasswordToken) authenticationToken; String username = token.getUsername(); User user = userService.getUser(username); // 从数据库读取数据 if (Objects.isNull(user)) { // 对象为空，验证失败 return null; } String password = user.getPassword(); // user 将在授权中继续使用， password 将被 shiro 自行密文验证 return new SimpleAuthenticationInfo(user, password, \"\"); } @Autowired public void setUserService(UserService userService) { this.userService = userService; }} 创建 Shiro 配置类，向 IOC 容器注册，包含核心组件 Realme、SecurityManager 和 过滤器链 ShiroFilterFactoryBean，其中 SecurityManager 使用与 Web 相关的 DefaultWebSecurityManager： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package me.zhy.integrate.config.shiro;import me.zhy.integrate.entity.domain.User;import me.zhy.integrate.service.UserService;import org.apache.shiro.SecurityUtils;import org.apache.shiro.authc.*;import org.apache.shiro.authz.AuthorizationInfo;import org.apache.shiro.authz.SimpleAuthorizationInfo;import org.apache.shiro.realm.AuthorizingRealm;import org.apache.shiro.session.Session;import org.apache.shiro.subject.PrincipalCollection;import org.apache.shiro.subject.Subject;import org.springframework.beans.factory.annotation.Autowired;import java.util.Objects;public class UserShiroRealme extends AuthorizingRealm { private UserService userService; // 授权 @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principalCollection) { SimpleAuthorizationInfo info = new SimpleAuthorizationInfo(); Subject subject = SecurityUtils.getSubject(); // 获得当前用户 User currentUser = (User) subject.getPrincipal(); info.addStringPermission(currentUser.getPermission()); // 添加当前用户的权限 return info; } // 验证 @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken authenticationToken) throws AuthenticationException { // 获得 controller 封装的前台用户名和密码 token UsernamePasswordToken token = (UsernamePasswordToken) authenticationToken; String username = token.getUsername(); User user = userService.getUser(username); // 从数据库读取数据 if (Objects.isNull(user)) { // 对象为空，验证失败 return null; } // 验证成功，将对象放入shiro.Session中 Subject subject = SecurityUtils.getSubject(); Session session = subject.getSession(); session.setAttribute(\"user\", user); String password = user.getPassword(); // user 将在授权中继续使用， password 将被 shiro 自行密文验证 return new SimpleAuthenticationInfo(user, password, \"\"); } @Autowired public void setUserService(UserService userService) { this.userService = userService; }} 在 step 3 中过滤器链通过 Map&lt;String, String&gt; 方式设置，其中 key表示拦截的 url 路径，value表示过滤器，Shiro 在 DefaultFilter 枚举类中提供了多种内置过滤器，这些组件分为两类：认证过滤器和授权过滤器，在文档中介绍如下： 认证过滤器： anon ：org.apache.shiro.web.filter.authc.AnonymousFilter，可以匿名使用 authc ：org.apache.shiro.web.filter.authc.FormAuthenticationFilter，需要认证(登录)才能使用 authcBasic ：org.apache.shiro.web.filter.authc.BasicHttpAuthenticationFilter，httpBasic认证 user ：org.apache.shiro.web.filter.authz.UserFilter，示必须存在用户，当登入时不做检查 授权过滤器： ssl ：org.apache.shiro.web.filter.authz.SslFilter，https请求使用 port ：org.apache.shiro.web.filter.authz.PortFilter，参数绑定端口号使用 rest ：org.apache.shiro.web.filter.authz.HttpMethodPermissionFilter，参数绑定rest行为使用，如get perms ：org.apache.shiro.web.filter.authz.PermissionAuthorizationFilter，权限类别，包含多个参数 roles ：org.apache.shiro.web.filter.authz.RolesAuthorizationFilter，角色类别，包含多个参数 启动 SpringBoot 验证，zhangsan 用户具有 user:* 权限可以全部访问；lisi 用户具有 user:page1 权限，只能访问 page1 ，page2将提示授权失败；wangwu 用户同理。 12 SpringBoot 集成 Swagger&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Swagger 是一个规范和完整的框架，用于生成、描述、调用和可视化 RESTful 风格的 Web 服务。当前 WEB 项目以前后端分离为主，后端主要提供 REST API 给前端，前端框架负责数据绑定、路由等，所以 Swagger 不仅可以保证前后端开发的一致性和及时性，还能有效提高开发效率。 导入 Swagger 依赖。 1234567891011&lt;!-- swagger --&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt; 创建 Swagger 配置类并注册到 IOC 容器中 12345678910111213141516171819202122232425262728293031323334353637@Configuration@EnableSwagger2public class SwaggerConfig { // 创建 Swagger 实体类 @Bean public Docket docket(Environment env) { // prod 环境关闭 swagger，dev 环境开启 // 只接收 dev test 环境，不接收 prod 环境 Profiles profiles = Profiles.of(\"dev\", \"test\"); boolean useSwagger = env.acceptsProfiles(profiles); // SWAGGER_12、SWAGGER_2、SPRING_WEB return new Docket(DocumentationType.SWAGGER_2) .apiInfo(getApiInfo()) .groupName(\"spring boot api release\") .select() // 扫描指定包 .apis(RequestHandlerSelectors.basePackage(\"me.zhy.integrate.controller\")) // url 过滤策略 .paths(PathSelectors.any()) .build() .enable(useSwagger); } // 配置信息 public ApiInfo getApiInfo() { Contact contact = new Contact(\"spring-boot-integrate\", \"\", \"\"); return new ApiInfo(\"Spring Boot Integrate Project\", \"Api Documentation\", \"1.0\", \"\", contact, \"\", \"\", new ArrayList()); }} 对 Controller 层接口进行 Swagger 标记。 12345678910111213141516@RestController@RequestMapping(\"/swagger\")public class SwaggerTestController { @ApiOperation(\"欢迎页\") @GetMapping(\"/hello\") public String hello() { return \"hello\"; } @ApiOperation(\"请求User对象\") @PostMapping(\"/hi\") public User hi() { return new User(); }} 如果 Controller 层的接口有返回的实例对象，那么Swagger就会扫描到并在 UI 中显式，所以在实际开发中可以对DTO创建 Swagger 标记用于说明。 123456789101112131415161718192021222324@ApiModel(\"User实体类\")// lombok 注解@NoArgsConstructor@AllArgsConstructor@ToStringpublic class User { @ApiModelProperty(\"用户id\") // lombok 注解 @Getter @Setter private Integer id; @ApiModelProperty(\"用户名\") @Getter @Setter private String name; @ApiModelProperty(\"用户密码\") @Getter @Setter private String password; @ApiModelProperty(\"用户权限\") @Getter @Setter private String permission;} 启动 SpringBoot ，访问 http://localhost:8080/swagger-ui.html 访问: 在 Swagger 页面模拟数据调用接口，如调用本例中的 post 接口。 13 SpringBoot 集成 Redis&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在 SpringBoot2.x 版本后，原先 java 版的 redis 工程由 Jedis 更改为 lettuce，jedis 采用源码直连 redis ，在多线程环境中使用是不安全的，为了避免线程安全问题，使用 Jedis Pool 连接池；而 lettuce 采用 netty 与 redis 通信，其实例可以再多个线程中共享，不存在线程安全问题。 导入 redis 依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; Redis自动配置类和参数配置类 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Redis自动配置类 RedisAutoConfiguration 只定义了两个模板，即 RedisTemplate 和 StringRedisTemplate，通过 Spring 封装的模版类来简化 Redis 操作。 12345678910111213141516171819202122232425@Configuration(proxyBeanMethods = false)@ConditionalOnClass(RedisOperations.class)@EnableConfigurationProperties(RedisProperties.class)@Import({ LettuceConnectionConfiguration.class, JedisConnectionConfiguration.class })public class RedisAutoConfiguration { @Bean @ConditionalOnMissingBean(name = \"redisTemplate\") public RedisTemplate&lt;Object, Object&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) throws UnknownHostException { RedisTemplate&lt;Object, Object&gt; template = new RedisTemplate&lt;&gt;(); template.setConnectionFactory(redisConnectionFactory); return template; } @Bean @ConditionalOnMissingBean public StringRedisTemplate stringRedisTemplate(RedisConnectionFactory redisConnectionFactory) throws UnknownHostException { StringRedisTemplate template = new StringRedisTemplate(); template.setConnectionFactory(redisConnectionFactory); return template; }} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Redis的参数配置类中定义了 Redis 连接的全部信息，其中使用spring.redis 前缀的配置信息，配置类中host、 port 已经给出了默认值，并且通过组合的方式集成了 Jedis 对象和 Lettuce 对象。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@ConfigurationProperties(prefix = \"spring.redis\")public class RedisProperties { /** * Database index used by the connection factory. */ private int database = 0; /** * Connection URL. Overrides host, port, and password. User is ignored. Example: * redis://user:password@example.com:6379 */ private String url; /** * Redis server host. */ private String host = \"localhost\"; /** * Login password of the redis server. */ private String password; /** * Redis server port. */ private int port = 6379; /** * Whether to enable SSL support. */ private boolean ssl; /** * Connection timeout. */ private Duration timeout; /** * Client name to be set on connections with CLIENT SETNAME. */ private String clientName; private Sentinel sentinel; private Cluster cluster; private final Jedis jedis = new Jedis(); private final Lettuce lettuce = new Lettuce(); 自定义 Redis 配置类 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于 RedisAutoConfiguration 中默认使用的 RedisTemplate 采用 &lt;Object, Object&gt; 形式，而我们常用的是String 类型，与此同时，在 RedisTemplate 中默认采用对象序列化方式是 JDK 序列化方式，可以在配置类中自定义序列化方式。 123456789101112131415161718192021222324252627@Configurationpublic class RedisConfig { @Bean public RedisTemplate&lt;String, Object&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) { RedisTemplate&lt;String, Object&gt; redisTemplate = new RedisTemplate&lt;&gt;(); redisTemplate.setConnectionFactory(redisConnectionFactory); // 配置序列化方式 Jackson2JsonRedisSerializer jacksonSerializer = new Jackson2JsonRedisSerializer(Object.class); ObjectMapper objectMapper = new ObjectMapper(); objectMapper.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); objectMapper.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); jacksonSerializer.setObjectMapper(objectMapper); StringRedisSerializer stringSerializer = new StringRedisSerializer(); // key 使用 String 的序列化方式 redisTemplate.setKeySerializer(stringSerializer); redisTemplate.setHashKeySerializer(stringSerializer); // value 使用 jackson 序列化方式 redisTemplate.setValueSerializer(jacksonSerializer); redisTemplate.setHashValueSerializer(jacksonSerializer); redisTemplate.afterPropertiesSet(); return redisTemplate; }} 企业级开发中，Pojo对象通常需要序列化，在 redis 中持久化对象，所以默认的 JDK 序列化方式并不能满足现状要求，自定义序列化方式很有必要，更进一步配合自定义封装的 redis 工具类使用，效率翻倍 12345678910// 测试@Autowired private RedisTemplate&lt;String, Object&gt; redisTemplate; @Test void contextLoads() throws Exception { User user = new User(1,\"张三\",\"123\",\"无权限\"); redisTemplate.opsForValue().set(\"key1\", user); System.out.println(redisTemplate.opsForValue().get(\"key1\")); }","link":"/2020/07/01/6%20Spring/Spring%E5%8F%8A%E6%BA%90%E7%A0%81%E2%80%94%E2%80%94SpringBoot%EF%BC%88%E4%B8%89%EF%BC%89/"},{"title":"Spring及源码——SpringBoot（一）","text":"Spring及源码——SpringBoot（一） 1 SpringBoot概述&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Spring Boot通过大量的集成和自动配置，简化了 Spring 项目开发样板化的配置过程，专注于核心业务的开发，在快速应用开发领域成为领导者。Springboot具有以下特点： IDE 集成了 Spring Initialize 来快速创建 Springboot 项目结构。 通过 maven 配置 springboot 的启动项（spring-boot-starter）来组装项目依赖（spring-boot-starter-dependencies 是核心依赖）。 通过嵌入式 web 容器，并配合主启动类的 main 函数，做到项目工程以 jar 包形式打包并部署，嵌入式 web 容器包括： spring-boot-starter-tomcat spring-boot-starter-jetty spring-boot-starter-undertow 通过 @SpringBootApplication 标注 spring 项目入口。@SpringBootApplication 融合了 @SpringBootConfiguration、@EnableAutoConfiguration、@ComponentScan注解。 通过 application.properties 或 application.yaml 进行参数化配置，spring 官方推荐使用 yaml 进行配置。 2 SpringBoot 简单应用&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过 spring Initializer 快速创建一个 springboot 工程，在 maven 中引入启动项： 123456789101112&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.2.2.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 自动生成的 springboot 入口类： 1234567@SpringBootApplicationpublic class App { public static void main(String[] args) { SpringApplication.run(App.class, args); }} 手动创建一个 Controller 类 12345678@RestControllerpublic class ControllerDemo { @GetMapping(\"/hi\") public String hi() { return \"hi\"; }} 运行 main 函数成功启动 springboot 引用，最后在浏览器输入http://localhost:8080/hi验证执行成功，一个简单的 Springboot 应用就成功运行。 3 SpringBoot 启动源码3.1 自动配置源码&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该小节仅对 SpringBoot 自动配置进行高层次的展示，通过关键注解和调用类来揭开自动装配的原理。 按照注解调用深度，可以简单的罗列为如下，以缩进表示深度： 1234567891011@SpringBootApplication @SpringBootConfiguration // 配置解析 @ComponentScan // Bean扫描 @EnableAutoConfiguration // 自动装配 @AutoConfigurationPackage @Import({Registrar.class}) @Import({AutoConfigurationImportSelector.class}) getAutoConfigurationEntry(); // 获得自动配置元素 getCandidateConfigurations(); // 获得所有候选配置 getSpringFactoriesLoaderFactoryClass(); // 获得所有标记了@EnableAutoConfiguration的类 loadSpringFactories(); // 根据类加载器判断从资源或系统中加载 spring.factories 配置项，并通过循环将配置项封装为Properties对象供系统使用 对于元注解（@Target、@Retention、@Documented、@Inherited）将在源码中省略。 @SpringBootApplication，包含@SpringBootConfiguration、@EnableAutoConfiguration、@ComponentScan三个主要注解，分别用于实现解析配置、自动装配和组件扫描功能。 123456789101112@SpringBootConfiguration // 配置解析@EnableAutoConfiguration // 自动装配@ComponentScan( // Bean扫描 excludeFilters = {@Filter( type = FilterType.CUSTOM, classes = {TypeExcludeFilter.class}), @Filter( type = FilterType.CUSTOM, classes = {AutoConfigurationExcludeFilter.class})})public @interface SpringBootApplication {} @EnableAutoConfiguration 包含两个注解@AutoConfigurationPackage、@Import({AutoConfigurationImportSelector.class})，分别实现自动配置包和自动导入类 AutoConfigurationImportSelector，其中 @AutoConfigurationPackage 实现了自动导入类 Registrar。 123456@AutoConfigurationPackage@Import({AutoConfigurationImportSelector.class})public @interface EnableAutoConfiguration {}@Import({Registrar.class})public @interface AutoConfigurationPackage {} AutoConfigurationImportSelector 类实现了自动选择导入的配置项： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677public class AutoConfigurationImportSelector { // 获得自动配置项 protected AutoConfigurationImportSelector.AutoConfigurationEntry getAutoConfigurationEntry(AutoConfigurationMetadata autoConfigurationMetadata, AnnotationMetadata annotationMetadata) { if (!this.isEnabled(annotationMetadata)) { return EMPTY_ENTRY; } else { AnnotationAttributes attributes = this.getAttributes(annotationMetadata); // 获取所有候选配置，即标记 EnableAutoConfiguration 注解的类 List&lt;String&gt; configurations = this.getCandidateConfigurations(annotationMetadata, attributes); configurations = this.removeDuplicates(configurations); Set&lt;String&gt; exclusions = this.getExclusions(annotationMetadata, attributes); this.checkExcludedClasses(configurations, exclusions); configurations.removeAll(exclusions); configurations = this.filter(configurations, autoConfigurationMetadata); this.fireAutoConfigurationImportEvents(configurations, exclusions); return new AutoConfigurationImportSelector.AutoConfigurationEntry(configurations, exclusions); } } protected List&lt;String&gt; getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) { // 获得所有标记了 EnableAutoConfiguration 注解的类 List&lt;String&gt; configurations = SpringFactoriesLoader.loadFactoryNames(this.getSpringFactoriesLoaderFactoryClass(), this.getBeanClassLoader()); Assert.notEmpty(configurations, \"No auto configuration classes found in META-INF/spring.factories. If you are using a custom packaging, make sure that file is correct.\"); return configurations; } // 返回标记了 EnableAutoConfiguration 注解的类，其实就是@SpringBootApplication protected Class&lt;?&gt; getSpringFactoriesLoaderFactoryClass() { return EnableAutoConfiguration.class; } public static List&lt;String&gt; loadFactoryNames(Class&lt;?&gt; factoryType, @Nullable ClassLoader classLoader) { String factoryTypeName = factoryType.getName(); return (List)loadSpringFactories(classLoader).getOrDefault(factoryTypeName, Collections.emptyList()); } private static Map&lt;String, List&lt;String&gt;&gt; loadSpringFactories(@Nullable ClassLoader classLoader) { MultiValueMap&lt;String, String&gt; result = (MultiValueMap)cache.get(classLoader); if (result != null) { return result; } else { try { // 根据类加载器判断从资源或系统中获取 spring.factories 配置 Enumeration&lt;URL&gt; urls = classLoader != null ? classLoader.getResources(\"META-INF/spring.factories\") : ClassLoader.getSystemResources(\"META-INF/spring.factories\"); LinkedMultiValueMap result = new LinkedMultiValueMap(); // 遍历所有配置项并封装为 Properties 对象供系统使用，并存入map中返回 while(urls.hasMoreElements()) { URL url = (URL)urls.nextElement(); UrlResource resource = new UrlResource(url); Properties properties = PropertiesLoaderUtils.loadProperties(resource); Iterator var6 = properties.entrySet().iterator(); while(var6.hasNext()) { Entry&lt;?, ?&gt; entry = (Entry)var6.next(); String factoryTypeName = ((String)entry.getKey()).trim(); String[] var9 = StringUtils.commaDelimitedListToStringArray((String)entry.getValue()); int var10 = var9.length; for(int var11 = 0; var11 &lt; var10; ++var11) { String factoryImplementationName = var9[var11]; result.add(factoryTypeName, factoryImplementationName.trim()); } } } cache.put(classLoader, result); return result; } catch (IOException var13) { throw new IllegalArgumentException(\"Unable to load factories from location [META-INF/spring.factories]\", var13); } } }} 该步骤是主要的自动装配过程：首先，从 spring.factories 中读出所有配置项并封装为 Properties 对象，并最终转换为 map 对象，通过标记了 @EnableAutoConfiguration 的类找到所有对应的候选配置项。 spring.factories，位于spring-boot-autoconfigure 包 META-INF目录下 其内容主要包含以 AutoConfiguration结尾的自动配置类 ，部分内容节选如下： 1234567891011121314151617# Auto Configureorg.springframework.boot.autoconfigure.EnableAutoConfiguration=\\org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration,\\org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\\org.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration,\\org.springframework.boot.autoconfigure.batch.BatchAutoConfiguration,\\org.springframework.boot.autoconfigure.cache.CacheAutoConfiguration,\\org.springframework.boot.autoconfigure.cassandra.CassandraAutoConfiguration,\\org.springframework.boot.autoconfigure.cloud.CloudServiceConnectorsAutoConfiguration,\\org.springframework.boot.autoconfigure.context.ConfigurationPropertiesAutoConfiguration,\\org.springframework.boot.autoconfigure.context.MessageSourceAutoConfiguration,\\org.springframework.boot.autoconfigure.context.PropertyPlaceholderAutoConfiguration,\\org.springframework.boot.autoconfigure.couchbase.CouchbaseAutoConfiguration,\\org.springframework.boot.autoconfigure.dao.PersistenceExceptionTranslationAutoConfiguration,\\org.springframework.boot.autoconfigure.web.servlet.DispatcherServletAutoConfiguration,\\# 等等，共计 123 项 在自动配置类中，有一个关键的以 @ConditionalOn 开头的注解，称为 SpringBoot 条件过滤注解，用于判断自动配置类是否可用，而是否可用的的根本是工程中是否导入依赖的启动器，即spring-boot-starter-xxx，如果导入对应的启动器则自动配置类生效，否则无效。 以 WebMvcAutoConfiguration 为例，简单介绍自动配置原理： 12345678910111213141516171819202122232425262728293031323334353637383940414243// 自带配置@Configuration( proxyBeanMethods = false)// 从配置文件中获得所有 spring.http 配置@EnableConfigurationProperties({HttpProperties.class})// 过滤条件1@ConditionalOnWebApplication( type = Type.SERVLET)// 过滤条件2@ConditionalOnClass({Servlet.class, DispatcherServlet.class, WebMvcConfigurer.class})// 过滤条件3@ConditionalOnMissingBean({WebMvcConfigurationSupport.class})// 自动配置顺序，在SpringApplication.run() 方法中会进行排序@AutoConfigureOrder(-2147483638)// 自动配置后执行@AutoConfigureAfter({DispatcherServletAutoConfiguration.class, TaskExecutionAutoConfiguration.class, ValidationAutoConfiguration.class})public class WebMvcAutoConfiguration { @Bean @ConditionalOnBean({View.class}) @ConditionalOnMissingBean public BeanNameViewResolver beanNameViewResolver() { BeanNameViewResolver resolver = new BeanNameViewResolver(); resolver.setOrder(2147483637); return resolver; } @Bean @ConditionalOnBean({ViewResolver.class}) @ConditionalOnMissingBean( name = {\"viewResolver\"}, value = {ContentNegotiatingViewResolver.class} ) public ContentNegotiatingViewResolver viewResolver(BeanFactory beanFactory) { ContentNegotiatingViewResolver resolver = new ContentNegotiatingViewResolver(); resolver.setContentNegotiationManager((ContentNegotiationManager)beanFactory.getBean(ContentNegotiationManager.class)); resolver.setOrder(-2147483648); return resolver; } // ...} 每一个自动配置对象都标记了 @Configuration 表明该类是配置类，内部实现了配置项比如上述例子中以 @Bean 标记的视图、视图解析器等装入 IOC 容器中，供 Spring 使用。 同时如果该自动装配对象要生效必须满足以 @ConditionOnXXX 的过滤条件，约束过滤条件就是工程中是否导入了 springboot 启动项。 总结： 从标记的 @SpringBootApplication 类开始启动，加载环境中（spring-boot-autoconfigure）的 /NETA-INFO/spring.factories 获取自动配置类列表（spring boot 2.2.2 当前包含123项）。 每一个自动配置类都实现了 @Configuration 注解，表示该类是一个配置类，其内部通过 @Bean 注解向IOC 容器中注入依赖对象。 自动配置类生效的约束条件是满足 @ConditionalOnXXX 注解的，判断约束条件的根本是是否导入了相关的启动器，或者自定义方法是否满足，如果不生效则不进行自动配置。也就是说 SpringBoot 通过自动配置类内的默认配置项完成了 Spring 的配置，而自动配置类中需要的参数通过配置文件输入，通过@EnableConfigurationProperties 和 @ConfigurationProperties 完成。 通过 AutoConfigurationImportSelector 类，从标记了 @EnableConfiguration 注解类中（即@SpringBootApplication）筛选出需要使用的自动配置类，并最终向 IOC 容器中注册 Bean 对象实例。 如果我们需要的组件不在自动配置类中，那么就需要通过 JavaConfig 方式（@Configuration 和 @Bean）向IOC容器注册。 3.2 主启动项源码初探&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这一小节围绕 SpringApplication.run() 方法探究，配合源码和执行流程图来说明。 1234567@SpringBootApplicationpublic class App { public static void main(String[] args) { SpringApplication.run(App.class, args); }} SpringApplication 对象的创建，将传入的主类当作主启动项，通过构造器完成参数初始化，这一步主要功能包括：判断是否 web 工程、加载初始化器、加载监听、推断并设置main方法的定义类。 1234567891011121314151617181920212223public SpringApplication(ResourceLoader resourceLoader, Class&lt;?&gt;... primarySources) { this.sources = new LinkedHashSet(); this.bannerMode = Mode.CONSOLE; this.logStartupInfo = true; this.addCommandLineProperties = true; this.addConversionService = true; this.headless = true; this.registerShutdownHook = true; this.additionalProfiles = new HashSet(); this.isCustomEnvironment = false; this.lazyInitialization = false; this.resourceLoader = resourceLoader; Assert.notNull(primarySources, \"PrimarySources must not be null\"); this.primarySources = new LinkedHashSet(Arrays.asList(primarySources)); // 推断是否 web 工程 this.webApplicationType = WebApplicationType.deduceFromClasspath(); // 加载初始化启动器 this.setInitializers(this.getSpringFactoriesInstances(ApplicationContextInitializer.class)); // 设置监听器 this.setListeners(this.getSpringFactoriesInstances(ApplicationListener.class)); // 推断并设置主类 this.mainApplicationClass = this.deduceMainApplicationClass();} run方法的主方法运行，这一部是主方法，主要完成各种参数的初始化并最终创建一个 ConfigurableApplicationContext 类型的上下文对象，完成 IOC 容器的初始化。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public ConfigurableApplicationContext run(String... args) { // 任务监听对象，记录任务执行时间 StopWatch stopWatch = new StopWatch(); stopWatch.start(); ConfigurableApplicationContext context = null; Collection&lt;SpringBootExceptionReporter&gt; exceptionReporters = new ArrayList(); // 设置headless，使 awt 组件可以在无外设情况下正常使用 this.configureHeadlessProperty(); // 设置并启动监听器 SpringApplicationRunListeners listeners = this.getRunListeners(args); listeners.starting(); Collection exceptionReporters; try { ApplicationArguments applicationArguments = new DefaultApplicationArguments(args); // 环境参数准备，properties或yaml配置文件加载 ConfigurableEnvironment environment = this.prepareEnvironment(listeners, applicationArguments); this.configureIgnoreBeanInfo(environment); // 打印banner （控制台logo） Banner printedBanner = this.printBanner(environment); // 创建一个基于注解的 IOC 容器对象，包含：web ioc、standard ioc 和 响应式 ioc context = this.createApplicationContext(); // 创建Spring异常报告器 exceptionReporters = this.getSpringFactoriesInstances(SpringBootExceptionReporter.class, new Class[]{ConfigurableApplicationContext.class}, context); // ioc容器前置处理：向ioc注册配置参数、监听器、日志等信息并刷新 this.prepareContext(context, environment, listeners, applicationArguments, printedBanner); this.refreshContext(context); this.afterRefresh(context, applicationArguments); stopWatch.stop(); // 启动日志创建 if (this.logStartupInfo) { (new StartupInfoLogger(this.mainApplicationClass)).logStarted(this.getApplicationLog(), stopWatch); } // 启动 ioc 容器 listeners.started(context); this.callRunners(context, applicationArguments); } catch (Throwable var10) { this.handleRunFailure(context, var10, exceptionReporters, listeners); throw new IllegalStateException(var10); } try { // 发布可用的 IOC 容器并返回实例对象 listeners.running(context); return context; } catch (Throwable var9) { this.handleRunFailure(context, var9, exceptionReporters, (SpringApplicationRunListeners)null); throw new IllegalStateException(var9); }} 3.3 debug模式查看启动&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在配置文件中，通过 debug = true 开启调试模式，查看自动配置类的创建和过滤过程。 12# application.ymldebug: true 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556============================CONDITIONS EVALUATION REPORT============================// 启动的自动配置类Positive matches:-----------------AopAutoConfiguration matched:- @ConditionalOnProperty (spring.aop.auto=true) matched (OnPropertyCondition)AopAutoConfiguration.ClassProxyingConfiguration matched:- @ConditionalOnMissingClass did not find unwanted class 'org.aspectj.weaver.Advice' (OnClassCondition)- @ConditionalOnProperty (spring.aop.proxy-target-class=true) matched (OnPropertyCondition)DispatcherServletAutoConfiguration matched:- @ConditionalOnClass found required class 'org.springframework.web.servlet.DispatcherServlet' (OnClassCondition)- found 'session' scope (OnWebApplicationCondition)DispatcherServletAutoConfiguration.DispatcherServletConfiguration matched:- @ConditionalOnClass found required class 'javax.servlet.ServletRegistration' (OnClassCondition)- Default DispatcherServlet did not find dispatcher servlet beans (DispatcherServletAutoConfiguration.DefaultDispatcherServletCondition)DispatcherServletAutoConfiguration.DispatcherServletRegistrationConfiguration matched:- @ConditionalOnClass found required class 'javax.servlet.ServletRegistration' (OnClassCondition)- DispatcherServlet Registration did not find servlet registration bean (DispatcherServletAutoConfiguration.DispatcherServletRegistrationCondition)...etc// 未使用，过滤调的自动配置类Negative matches:-----------------ActiveMQAutoConfiguration:Did not match:- @ConditionalOnClass did not find required class 'javax.jms.ConnectionFactory' (OnClassCondition)AopAutoConfiguration.AspectJAutoProxyingConfiguration:Did not match:- @ConditionalOnClass did not find required class 'org.aspectj.weaver.Advice' (OnClassCondition)ArtemisAutoConfiguration:Did not match:- @ConditionalOnClass did not find required class 'javax.jms.ConnectionFactory' (OnClassCondition)BatchAutoConfiguration:Did not match:- @ConditionalOnClass did not find required class 'org.springframework.batch.core.launch.JobLauncher' (OnClassCondition)CacheAutoConfiguration:Did not match:- @ConditionalOnBean (types: org.springframework.cache.interceptor.CacheAspectSupport; SearchStrategy: all) did not find any beans of type org.springframework.cache.interceptor.CacheAspectSupport (OnBeanCondition)Matched:- @ConditionalOnClass found required class 'org.springframework.cache.CacheManager' (OnClassCondition)...etc 4 SpringBoot配置和yaml&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SpringBoot的全量配置如下 https://docs.spring.io/spring-boot/docs/2.2.2.RELEASE/reference/htmlsingle/#appendix ，在springboot的加载配置文件时按照如下顺序：application.yml &gt; application.yaml &gt; application.properties`，前两者都是yaml文件也是官方推荐使用的配置方式。 4.1 yaml的配置注入&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;yaml不仅支持参数配置，还支持对象和容器的配置，在SpringBoot中，可以通过配置 @ConfigurationProperties(prefix = &quot;&quot;) 从yaml中读取对象并绑定到实例对象中。 在yaml中配置对象 123456person: name: zhang age: ${random.int[0,30]} dog: {type: Alaska, name: wangwang} map: {k1: v1, k2: v2} list: [a1, a2, a3] 创建类，通过@ConfigurationProperties(prefix = “person”) 指定yaml中的person绑定到该类中 12345678910@Component@ConfigurationProperties(prefix = \"person\")@Datapublic class Person { private String name; private Integer age; private Dog dog; private List&lt;String&gt; list; private Map&lt;String, String&gt; map;} 测试 1234567891011121314@SpringBootTestclass TestApplicationTests { @Autowired private Person person; @Test void test01() { System.out.println(person); }}// output// Person(name=zhang, age=29, dog=Dog(type=Alaska, name=wangwang), list=[a1, a2, a3], map={k1=v1, k2=v2}) 当然，该注解同样支持properties的对象绑定，如果项目路径下包含多个配置文件可以通过 @PropertySource(value = &quot;application.yaml&quot;)指定。 4.2 多环境配置&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;实际应用开发中经常需要配置多种环境：测试环境、开发环境、生产环境，正对不同的环境需要进行不同的配置，springboot中支持在配置文件中创建多环境。在 SpringBoot 启动过程中，会按照一定的优先级加载配置文件，官方给出的加载顺序如下： Config locations are searched in reverse order. By default, the configured locations are classpath:/,classpath:/config/,file:./,file:./config/. The resulting search order is the following: file:./config/ file:./ classpath:/config/ classpath:/ 也就是说，当存在多个路径下的配置时，优先加载项目路径下的 config 目录内配置，随后是项目下的配置，接着是类路径下的 config 目录内配置，最后才是类路径下的配置。可以通过路径覆盖来达到不同环境配置文件的替换，但这并不优雅，可以采取多配置文件激活的方式配置，如下: 首先在 classpath 下创建多个配置文件: application.yml、application-test.yml、application-dev.yml、application-prod.yml； 通过在 application.yml中配置加载指定配置文件即可，spring.profiles.active=prod，就会去加载application-prod.yml内的配置。 当然，最优雅的方式应该是在同一配置文件配置多个环境并根据环境激活: 123456789101112131415161718192021222324server: port: 8080spring: profiles: active: dev# 使用---拆分多环境---server: port: 8081spring: profiles: dev ---server: port: 8082spring: profiles: test---server: port: 8083spring: profiles: prod 5 自定义启动器 Starter&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Spring官方支持自定义启动器，并按照 COC 要求创建自定义的启动器建议采用 xxx-spring-boot-starter 创建。在日常开发中，有很多独立于业务之外的配置模块，通常的做法是采用 jar 包的形式在其他工程内引入并使用，而在 springboot 中完全可以将这样的模块组成微服务并通过自定义启动器的形式在其他工程中引入。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;创建一个自定义的启动器，完全可以采用官方 spring-boot-autoconfiguration 中的做法，即通过自动装配来完成，具体步骤包括如下： 创建一个满足约定启动器名称的工程，如 my-spring-boot-starter，引入相关依赖。 创建一个普通业务类，该类实现具体业务功能。 创建一个参数配置类，该类通过 @ConfigurationProperties 从配置文件中读取相关配置参数。 创建一个自动装配类，依葫芦画瓢，以 AutoConfiguration 作为类名后缀，并配合 @Configuration、@Bean组成配置类向 ioc 容器中注册，同时使用 @EnableConfigurationProperties 指定该装配类所依赖的参数配置类，最后通过设置 @ConditionalOnXXX 注解进行有效过滤。 在 classpath 下创建 META-INF\\spring.factories，设置自动装配类全路径。 最后，通过 maven 工程打包上传本地仓库供其他工程使用。 创建工程 my-spring-boot-starter，引入相关依赖。 123456789101112131415161718&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.2.2.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建普通业务类，该业务类对外提供相应的功能。 1234567891011121314package me.zhy.starter;import lombok.AllArgsConstructor;import lombok.Data;@Data@AllArgsConstructorpublic class MyService { private MyProperties myProperties; public String foo() { return myProperties.getParam1() + \"#\" + myProperties.getParam2(); }} 创建一个业务配置类，通过 @ConfigurationProperties 从配置文件中读取相关配置参数。 12345678910111213package me.zhy.starter;import lombok.Data;import org.springframework.boot.context.properties.ConfigurationProperties;// 读取所有以 my.config 开头的配置信息@ConfigurationProperties(prefix = \"my.config\")@Datapublic class MyProperties { private String param1; private String param2;} 创建一个自动装配类，该类是实现自动配置的核心。 12345678910111213141516171819202122232425package me.zhy.starter;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.autoconfigure.condition.ConditionalOnWebApplication;import org.springframework.boot.context.properties.EnableConfigurationProperties;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configuration@EnableConfigurationProperties(MyProperties.class)@ConditionalOnWebApplication // 仅限web工程public class MyServiceAutoConfiguration { private MyProperties myProperties; @Bean public MyService instant() { return new MyService(myProperties); } @Autowired public void setMyProperties(MyProperties myProperties) { this.myProperties = myProperties; }} 在 classpath 下创建 META-INF\\spring.factories，设置自动装配类全路径。 1org.springframework.boot.autoconfigure.EnableAutoConfiguration=me.zhy.starter.MyServiceAutoConfiguration 通过 maven 向本地仓库安装工程 1mvn clean install 重新创建工程并引入 step 6 中的启动器工程 12345&lt;dependency&gt; &lt;groupId&gt;me.zhy&lt;/groupId&gt; &lt;artifactId&gt;my-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; 配置 application.properties 文件。 12my.config.param1=param1my.config.param2=param2 创建一个简单的 web 控制器并测试。 1234567891011121314151617import me.zhy.starter.MyService;@RestControllerpublic class myController { @Autowired private MyService myService; @GetMapping(\"/test\") public String test() { return myService.foo(); }}// 启动 springboot ，在浏览器中输入 localhost:8080/test 测试// output: param1 # param2","link":"/2020/07/01/6%20Spring/Spring%E5%8F%8A%E6%BA%90%E7%A0%81%E2%80%94%E2%80%94SpringBoot%EF%BC%88%E4%B8%80%EF%BC%89/"},{"title":"Redis（一）","text":"Redis（一） &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Redis 是一个开源的 K-V 结构缓存系统，还支持数据持久化即可以作为数据库和消息中间件使用。它支持多种数据结构：字符串、散列、列表、集合、有序集合等。Redis内置了 LUA 脚本，LRU 驱动事件，事务和不同级别的磁盘持久化，支持通过 Redis 哨兵（Sentinel）和自动分区（Cluster）提供高可用性。 1 NoSQL&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NoSQL，泛指非关系型的数据库，为了解决大规模数据集合多重数据种类带来的挑战，尤其是大数据应用难题。具有如下优点： 易扩展，NoSQL数据库种类繁多，但是一个共同的特点都是去掉关系数据库的关系型特性； 数据之间无关系，这样就非常容易扩展； 高性能，NoSQL数据库都具有非常高的读写性能，尤其在大数据量下，同样表现优秀； &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;常用 NoSQL 数据库类型包括： 哈希表数据库，表中有一个特定的键和一个指针指向特定的数据；（Redis） 列存储数据库，通常是用来应对分布式存储的海量数据。键仍然存在，但是它们的特点是指向了多个列；(HBase) 文档型数据库，数据模型是版本化的文档，半结构化的文档以特定的格式存储，比如JSON。文档型数据库可以看作是键值数据库的升级版，允许之间嵌套键值，在处理网页等复杂数据时，文档型数据库比传统键值数据库的查询效率更高；（MongoDB） 图形数据库，它是使用灵活的图形模型（不是保存图），并且能够扩展到多个服务器上。NoSQL数据库没有标准的查询语言(SQL)，因此进行数据库查询需要制定数据模型，使用场景比如社交网络。(Neo4j) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NoSQL框架体系NosoL整体框架分为四层，由下至上分为数据持久层(data persistence)、整体分布层(data distribution model)、数据逻辑模型层(data logical model)、和接口层(interface)，层次之间相辅相成，协调工作。 数据持久层：定义了数据的存储形式，主要包括基于内存、基于硬盘、内存和硬盘接口、订制可拔插四种形式。基于内存形式的数据存取速度最快，但可能会造成数据丢失。基于硬盘的数据存储可能保存很久，但存取速度较基于内存形式的慢。内存和硬盘相结合的形式，结合了前两种形式的优点，既保证了速度，又保证了数据不丢失。订制可拔插则保证了数据存取具有较高的灵活性。 数据分布层：定义了数据是如何分布的，相对于关系型数据库，NoSQL可选的机制比较多，主要有三种形式：一是CAP支持，可用于水平扩展。二是多数据中心支持，可以保证在横跨多数据中心是也能够平稳运行。三是动态部署支持，可以在运行着的集群中动态地添加或删除节点。 数据逻辑层：表述了数据的逻辑变现形式，与关系型数据库相比，NoSQL在逻辑表现形式上相当灵活，参考数据库类型介绍。 接口层：为上层应用提供了方便的数据调用接口，提供的选择远多于关系型数据库。接口层提供了五种选择：Rest，Thrift，Map/Reduce，Get/Put，特定语言API，使得应用程序和数据库的交互更加方便。 NoSQL数据库在以下的这几种情况下比较适用： 数据模型比较简单； 需要灵活性更强的IT系统； 对数据库性能要求较高； 不需要高度的数据一致性； 对于给定key，比较容易映射复杂值的环境。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MongoDB 是一个介于关系数据库和非关系数据库之间的产品，是非关系数据库中功能最丰富，最像关系数据库的。它支持的数据结构非常松散，是类似Json的Bjson格式，因此可以存储比较复杂的数据类型。MongoDB最大的特点是支持的查询语言非常强大，其语法有点类似于面向对象的查询语言，几乎可以实现类似关系数据库单表查询的绝大部分功能，还支持为数据建立索引。它的特点是高性能、易部署、易使用、存储数据非常方便。 2 Redis 配置&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Redis （Remote Dictionary Server）的关键字： C语言编写、支持网络、基于内存、可持久化、K-V数据库、开源，提供多API。 数据类型丰富、操作丰富、操作原子性、基于持久化的主从同步。 高性能：读110000次/s，写81000次/s。 2.1 Redis Windows 安装 下载解压包并解压，配置到环境变量 path，在安装目录下输入命令 redis-server redis.windows.conf 安装 windows 服务，redis-server --service-install redis.windows.conf，启动服务 Redis常用的指令卸载服务：redis-server –service-uninstall开启服务：redis-server –service-start停止服务：redis-server –service-stop 通过 set 和 get 测试 12345D:\\software code\\Redis-x64-3.2.100&gt;redis-cli127.0.0.1:6379&gt; set A 123OK127.0.0.1:6379&gt; get A&quot;123&quot; 2.2 Redis Linux 安装 解压 tar 包，执行环境安装 yum install gcc-c++。 执行 make 和 make install 安装。 修改配置文件 redis.conf 中 daemonize 为 yes，支持服务自动开启。 开启服务 redis-server redis.conf。 查看 redis 进程 ps -ef | grep redis。 使用 shutdown 关闭服务。 2.3 Redis 基础信息 redis 默认有 16 个数据库， 使用第 0 个数据库，在集群中不支持切换db 12345678910# 查看数据库大小127.0.0.1:6379&gt; dbsize(integer) 0# 选择数据库127.0.0.1:6379&gt; select 5OK# 清除当前数据库127.0.0.1:6379[5]&gt; flushdb redis 是单线程，它的效率瓶颈是内存和网络决定，而不是CPU使用率 ，多线程下会存在用户态和内核态切换的事件开销。 设置过期时间及查询。 123456789101112131415127.0.0.1:6379&gt; set name zhangsanOK127.0.0.1:6379&gt; keys *1) &quot;name&quot;# 设置过期时间127.0.0.1:6379&gt; expire name 5(integer) 1# 查看过期倒计时127.0.0.1:6379&gt; ttl name(integer) 3127.0.0.1:6379&gt; keys *(empty list or set) 2.4 配置详解&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; redis.conf 是 redis 核心配置文件，其中定义了 redis 通用配置、分布式配置等等。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# 导入其他配置文件include .\\path\\to\\local.conf### 网络配置 ###bind 127.0.0.1port 6379protected-mode yes # 保护模式### 通用配置 ###daemonize yes # 守护进程，默认为 nologlevel notice # 日志等级 debug、verbose、notice、warninglogfile \"\" # 具体日志文件databases 16 # 默认 16 个数据库pidfile /var/run/reids_6379.pid # 以服务方式运行需指定### RDB 配置 ###save 900 1 # 如果 900s 内有至少1个 key 被修改，执行持久化操作save 300 10 # 如果 300s 内有至少10个 key 被修改，执行持久化操作save 60 10000 # 如果 60s 内有至少10000个 key 被修改，执行持久化操作persistence-available [(yes)|no] # 是否持久化，在windows下可以关闭，会重定向到堆分配stop-writes-on-bgsave-error yes # 持久化错误是否继续工作rdbcompression yes # 是否亚索 rdb 文件 （消耗cpu资源）rdbchecksum yes # rdb文件校验dbfilename dump.rdb # rdb 文件名dir ./ # rdb文件保存路径### AOF 配置 ###appendonly no # 默认使用 rdb 持久化，不开启 aof 模式# AOF同步策略# always 每次写操作都会同步# everysec 每秒同步一次# no 不同步appendfsync always appendfilename \"appendonly.aof\" # aof 文件名no-appendfsync-on-rewrite no # 重写期间是否同步auto-aof-rewrite-percentage 100 # 当 aof 达到文件容量的百分比后自动重写auto-aof-rewrite-min-size 64mb # 当 aof 文件大于文件容量后自动重写### 主从复制 ###replicaof &lt;masterip&gt; &lt;masterport&gt; # 设置master节点的地址masterauth &lt;master-password&gt; # 设置连接主机的密码replica-serve-stale-data yesreplica-read-only yes # 设置主机只读不写### 安全 ###requirepass foobared # 设置redis连接密码，默认无密码，通常通过 `config set requirepass 123`设置### 约束 ###maxclients 10000 # 客户端连接最大数，默认无配置maxmemory &lt;bytes&gt; # redis 最大内存容量# volatile-lru：使用 LRU 算法移除过期集合内的 key# allkeys-lru：使用 LRU 算法移除任意 key# volatile-random：随机从过期集合内移除 key# allkeys-random：随机移除任意 key# volatile-ttl：移除接近过期的 key# noeviction：无策略，写操作时报错maxmemory-policy noeviction # 当内存达到最大内存时采用的策略，默认不采取策略只返回错误 3 数据类型详解string 常用命令：set \\ get \\ append \\ strlen \\ getrange \\ setrange \\ incr \\ decr \\ incrby \\ decrby \\ setex \\ setnx \\ mset \\ mget \\ getset 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667### 字符串操作命令 ###127.0.0.1:6379&gt; set key1 value1127.0.0.1:6379&gt; append key1 _appendix127.0.0.1:6379&gt; get key1&quot;value1_appendix&quot;127.0.0.1:6379&gt; strlen key115# 追加key不存在则设置为该值127.0.0.1:6379&gt; append key2 _appendix2127.0.0.1:6379&gt; get key2&quot;_appendix2&quot;# 范围127.0.0.1:6379&gt; set key1 &quot;this is a string&quot;127.0.0.1:6379&gt; getrange key1 0 3&quot;this&quot;127.0.0.1:6379&gt; getrange key1 0 -1&quot;this is a string&quot;# 替换127.0.0.1:6379&gt; setrange key1 7 &quot; not &quot;127.0.0.1:6379&gt; get key1&quot;this is not ring&quot;# 自增和自减127.0.0.1:6379&gt; set count 0127.0.0.1:6379&gt; incr count127.0.0.1:6379&gt; get count&quot;1&quot;127.0.0.1:6379&gt; decr count127.0.0.1:6379&gt; get count&quot;0&quot;# 步长自增127.0.0.1:6379&gt; incrby count 5127.0.0.1:6379&gt; get count&quot;5&quot;# setnx &quot;set if not exist&quot;127.0.0.1:6379&gt; setnx key2 redis127.0.0.1:6379&gt; get key2&quot;redis&quot;127.0.0.1:6379&gt; setnx key2 db127.0.0.1:6379&gt; get key2&quot;redis&quot;# 批量处理127.0.0.1:6379&gt; mset key1 v1 key2 v2 key3 v3127.0.0.1:6379&gt; mget key1 key2 key31) &quot;v1&quot;2) &quot;v2&quot;3) &quot;v3&quot;# 保存对象127.0.0.1:6379&gt; set user1 {name:zhangsan,sex:false}127.0.0.1:6379&gt; get user1&quot;{name:zhangsan,sex:false}&quot;# 组合操作127.0.0.1:6379&gt; getset current 123(nil)127.0.0.1:6379&gt; get current&quot;123&quot;127.0.0.1:6379&gt; getset current 456&quot;123&quot;127.0.0.1:6379&gt; get current&quot;456&quot; list 可以通过 list 模拟很多基础数据类型比如 栈、 队列、双端队列等。 常用命令：lpush / rpush / lpop / rpop / lrange / llen / lrem / lset / lindex / rpoplpush / linsert 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384### list 命令 #### 左入右入127.0.0.1:6379&gt; lpush list1 1 2 3 4 5127.0.0.1:6379&gt; lrange list1 0 -11) &quot;5&quot;2) &quot;4&quot;3) &quot;3&quot;4) &quot;2&quot;5) &quot;1&quot;127.0.0.1:6379&gt; rpush list2 1 2 3 4 5127.0.0.1:6379&gt; lrange list2 0 -11) &quot;1&quot;2) &quot;2&quot;3) &quot;3&quot;4) &quot;4&quot;5) &quot;5&quot;# 左出右出127.0.0.1:6379&gt; lpop list1&quot;5&quot;127.0.0.1:6379&gt; rpop list2&quot;5&quot;# 下标索引127.0.0.1:6379&gt; flushdbOK127.0.0.1:6379&gt; rpush list1 1 2 3127.0.0.1:6379&gt; lindex list1 1&quot;2&quot;# 列表长度127.0.0.1:6379&gt; llen list1(integer) 3# 删除127.0.0.1:6379&gt; lrem list1 1 3127.0.0.1:6379&gt; lrange list1 0 -11) &quot;1&quot;2) &quot;2&quot;127.0.0.1:6379&gt; rpush list2 1 2 3 4 5127.0.0.1:6379&gt; lrange list2 0 -11) &quot;1&quot;2) &quot;2&quot;3) &quot;3&quot;4) &quot;4&quot;5) &quot;5&quot;127.0.0.1:6379&gt; ltrim list2 1 3127.0.0.1:6379&gt; lrange list2 0 -11) &quot;2&quot;2) &quot;3&quot;3) &quot;4&quot;# 弹出末尾元素并添加到新列表中127.0.0.1:6379&gt; rpush list3 1 2 3 4 5127.0.0.1:6379&gt; rpoplpush list3 list4&quot;5&quot;127.0.0.1:6379&gt; lrange list3 0 -11) &quot;1&quot;2) &quot;2&quot;3) &quot;3&quot;4) &quot;4&quot;127.0.0.1:6379&gt; lrange list4 0 -11) &quot;5&quot;# 修改列表元素127.0.0.1:6379&gt; rpush list1 1127.0.0.1:6379&gt; lset list1 0 0OK127.0.0.1:6379&gt; lrange list1 0 -11) &quot;0&quot;# 插入127.0.0.1:6379&gt; rpush list1 1 3 5## 在元素3之前插入2127.0.0.1:6379&gt; linsert list1 BEFORE 3 2## 在元素3之后插入4127.0.0.1:6379&gt; linsert list1 AFTER 3 4127.0.0.1:6379&gt; lrange list1 0 -11) &quot;1&quot;2) &quot;2&quot;3) &quot;3&quot;4) &quot;4&quot;5) &quot;5&quot; set set 内数据不会重复。 常用命令：sadd \\ srem \\ smembers \\ sismember \\ spop \\ srandmember \\ smove \\ scard \\ sdiff \\ sinter \\ sunion 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465### set 命令 #### 新增和查看127.0.0.1:6379&gt; sadd set1 &quot;zhangsan&quot; &quot;lisi&quot; &quot;zhangsan&quot;127.0.0.1:6379&gt; smembers set11) &quot;zhangsan&quot;2) &quot;lisi&quot;# 判断127.0.0.1:6379&gt; sismember set1 zhangsan(integer) 1# 长度127.0.0.1:6379&gt; scard set1(integer) 2# 删除127.0.0.1:6379&gt; srem set1 zhangsan127.0.0.1:6379&gt; smembers set11) &quot;lisi&quot;#随机删除127.0.0.1:6379&gt; spop set1 11) &quot;zhangsan&quot;127.0.0.1:6379&gt; smembers set11) &quot;lisi&quot;# 随机选取 n 个元素127.0.0.1:6379&gt; srandmember set1 11) &quot;zhangsan&quot;127.0.0.1:6379&gt; srandmember set1 11) &quot;lisi&quot;# 移动元素127.0.0.1:6379&gt; sadd set1 zhangsan lisi127.0.0.1:6379&gt; sadd set2 wangwu127.0.0.1:6379&gt; smembers set11) &quot;zhangsan&quot;2) &quot;lisi&quot;127.0.0.1:6379&gt; smembers set21) &quot;wangwu&quot;127.0.0.1:6379&gt; smove set2 set1 wangwu127.0.0.1:6379&gt; smembers set11) &quot;wangwu&quot;2) &quot;zhangsan&quot;3) &quot;lisi&quot;127.0.0.1:6379&gt; smembers set2(empty list or set)# 集合操作127.0.0.1:6379&gt; sadd set1 zhangsan lisi wangwu zhaoliu127.0.0.1:6379&gt; sadd set2 zhangsan wangwu# 差集127.0.0.1:6379&gt; sdiff set1 set21) &quot;lisi&quot;2) &quot;zhaoliu&quot;# 交集127.0.0.1:6379&gt; sinter set1 set21) &quot;wangwu&quot;2) &quot;zhangsan&quot;# 并集127.0.0.1:6379&gt; sunion set1 set21) &quot;lisi&quot;2) &quot;zhaoliu&quot;3) &quot;zhangsan&quot;4) &quot;wangwu&quot; hash 常用命令：hset / hget / hmset / hmget / hgetall / hdel / hlen / hexists / hkeys / hvals / hincr / hdecr / hincrby / hdecrby / hsetnx / 使用场景：常应用于对象数据变更 hash 命令同比字符串，不过多介绍 zset zset 是有序集合，使用功能类比 set ，不过多介绍，只介绍排序类。 常用命令：zadd / zrem / zcard / zrange / zrevrange / zrangebyscore / zrevrangebyscore / zcount 12345678910111213141516171819202122232425262728293031323334353637383940414243# 添加127.0.0.1:6379&gt; zadd ages 18 zhangsan127.0.0.1:6379&gt; zadd ages 19 lisi127.0.0.1:6379&gt; zadd ages 21 wangwu127.0.0.1:6379&gt; zadd ages 17 zhaoliu# 升序排列127.0.0.1:6379&gt; zrange ages 0 -11) &quot;zhaoliu&quot;2) &quot;zhangsan&quot;3) &quot;lisi&quot;4) &quot;wangwu&quot;127.0.0.1:6379&gt; zrange ages 0 -1 WITHSCORES1) &quot;zhaoliu&quot;2) &quot;17&quot;3) &quot;zhangsan&quot;4) &quot;18&quot;5) &quot;lisi&quot;6) &quot;19&quot;7) &quot;wangwu&quot;8) &quot;21&quot;# 降序排列127.0.0.1:6379&gt; zrevrange ages 0 -11) &quot;wangwu&quot;2) &quot;lisi&quot;3) &quot;zhangsan&quot;4) &quot;zhaoliu&quot;# 指定升序排序，inf表示无穷127.0.0.1:6379&gt; zrangebyscore ages -inf +inf1) &quot;zhaoliu&quot;2) &quot;zhangsan&quot;3) &quot;lisi&quot;4) &quot;wangwu&quot;# 指定降序排序127.0.0.1:6379&gt; zrevrangebyscore ages +inf -inf1) &quot;wangwu&quot;2) &quot;lisi&quot;3) &quot;zhangsan&quot;4) &quot;zhaoliu&quot;# 计数127.0.0.1:6379&gt; zcount ages -inf +inf(integer) 4 geospatial 用于地理位置经纬度的保存。 常用命令：geoadd / geopos / geodist / georadius / georadiusbymember 1234567891011121314151617181920212223242526272829303132333435# 添加 key longitude latitude name127.0.0.1:6379&gt; geoadd china:city 116.40 39.90 beijing127.0.0.1:6379&gt; geoadd china:city 121.47 32.23 shanghai127.0.0.1:6379&gt; geoadd china:city 108.95 34.263 xian127.0.0.1:6379&gt; geoadd china:city 114.18 22.27 hongkong# 获取坐标127.0.0.1:6379&gt; geopos china:city xian1) 1) &quot;108.95000249147415&quot; 2) &quot;34.263000754041961&quot; # 两者直线距离127.0.0.1:6379&gt; geodist china:city hongkong xian&quot;1428303.9802&quot;127.0.0.1:6379&gt; geodist china:city hongkong xian km&quot;1428.3040&quot;# 给定坐标（110,30）和半径（500 km），查找范围内元素127.0.0.1:6379&gt; georadius china:city 110.0 30.0 500 km1) &quot;xian&quot;# 以给定本经和元素为中心，超找其他元素127.0.0.1:6379&gt; georadiusbymember china:city xian 2000 km withcoord1) 1) &quot;xian&quot; 2) 1) &quot;108.95000249147415&quot; 2) &quot;34.263000754041961&quot;2) 1) &quot;hongkong&quot; 2) 1) &quot;114.17999893426895&quot; 2) &quot;22.27000054000478&quot;3) 1) &quot;shanghai&quot; 2) 1) &quot;121.47000163793564&quot; 2) &quot;32.229999766261393&quot;4) 1) &quot;beijing&quot; 2) 1) &quot;116.39999896287918&quot; 2) &quot;39.900000091670925&quot; hyperloglog 基数统计算法，重复数只统计一次（基数），在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定 的、并且是很小。 官方提示，会有 0.81% 的错误率。 比如 UV 统计，单人多次点击 UV 量始终保持 1。 常用命令：pfadd / pfcount / pfmerge 12345678910# 添加元素和统计127.0.0.1:6379&gt; pfadd key1 a b c d e f g127.0.0.1:6379&gt; pfadd key2 h i j k l m n127.0.0.1:6379&gt; pfcount key1 key2(integer) 14# 合并127.0.0.1:6379&gt; pfmerge key3 key1 key2127.0.0.1:6379&gt; pfcount key3(integer) 14 bitmaps 位图，位存储，只记录 0 或 1状态。 使用场景：在二义性的统计中，可以使用位存储，例如统计员工一年总打卡数，就可以将每个员工的一年打开保存为 365 位的 0 元素，打卡的天数表示为 1 。 常用命令： 1234567891011121314151617181920# 记录一周的打卡127.0.0.1:6379&gt; setbit sign 1 1127.0.0.1:6379&gt; setbit sign 2 1127.0.0.1:6379&gt; setbit sign 3 1127.0.0.1:6379&gt; setbit sign 4 1127.0.0.1:6379&gt; setbit sign 5 1127.0.0.1:6379&gt; setbit sign 6 0127.0.0.1:6379&gt; setbit sign 7 0# 查看某天127.0.0.1:6379&gt; getbit sign 1(integer) 1127.0.0.1:6379&gt; getbit sign 6(integer) 0# 统计有 1 的值的数量127.0.0.1:6379&gt; bitcount sign(integer) 5127.0.0.1:6379&gt; bitcount sign 0 -1(integer) 5","link":"/2020/04/01/10%20%E4%B8%AD%E9%97%B4%E4%BB%B6/Redis%EF%BC%88%E4%B8%80%EF%BC%89/"},{"title":"Redis（二）","text":"Redis（二） 4 Redis 事务控制&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所谓 Redis 事务，就是指一次执行一组指令，Redis 可以保证原子性和隔离性： 事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。（事实上，Redis 并不能很好的保证事务的原子性，因为不支持回滚和 EXEC 执行错误处理）。 Redis 使用 MULTI、EXEC、DISCARD、WATCH 和 UNWATCH 负责事务的使用，其具体使用周期如下： 使用 MULTI 标记事务开启，之后的命令都将被视为事务内的命令，直到调用 EXEC 或 DISCARD； EXEC 提交事务，Redis 将事务内命令逐一入队并持久化，保证不会受到内存异常或其他指令的影响，按顺序依次执行； DISCARD 丢弃事务，一直到 MULTI 部分的指令都不会执行。 WATCH 监控重要的 key ，相当于对关键key加锁，如果在事务提交前 key 被修改，那么本次事务不会提交。 UNWATCH 用于取消监控命令，即撤销锁。 异常情况 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果发生在 EXEC 执行之前的错误，客户端会检查命令入队所得的返回值：如果命令入队时返回 QUEUED ，那么入队成功；否则，入队失败。服务器会对命令入队失败的情况进行记录，并在客户端调用 EXEC 命令时，拒绝执行并自动放弃这个事务。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在 EXEC 命令执行之后所产生的错误， 并没有对它们进行特别处理： 即使事务中有某些命令在执行时产生了错误， 事务中的其他命令仍然会继续执行 —— Redis 不会停止执行事务中的命令。。 Redis 为何不支持回滚 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Redis 官方勉强的解释如下： The Redis command will only fail because of the wrong syntax (and these problems cannot be found when entering the team), or the command is used on the wrong type of key: that is, from a practical point of view, the failed command It is caused by programming errors, and these errors should be discovered during the development process, and should not appear in the production environmen 从实用性的角度来讲，失败的命令是由编程错误造成的，这些错误应该在开发过程中被发现，而不应该出现在生产环境中。 Because there is no need to support rollback, Redis internals can be kept simple and fast. 因为不需要对回滚进行支持，所以 Redis 的内部可以保持简单且快速。 使用事务 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 开启和提交事务127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; rpush list 1 2 3QUEUED127.0.0.1:6379&gt; lpush list 0 -1QUEUED127.0.0.1:6379&gt; rpoplpush list list_tmpQUEUED127.0.0.1:6379&gt; lrange list 0 -1QUEUED127.0.0.1:6379&gt; lrange list_tmp 0 -1QUEUED127.0.0.1:6379&gt; exec1) (integer) 32) (integer) 53) &quot;3&quot;4) 1) &quot;-1&quot; 2) &quot;0&quot; 3) &quot;1&quot; 4) &quot;2&quot;5) 1) &quot;3&quot;# 放弃事务127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set key1 value1QUEUED127.0.0.1:6379&gt; set key2 value2QUEUED127.0.0.1:6379&gt; discardOK# 事务不支持原子性示例127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; rpush list 1 2 3QUEUED127.0.0.1:6379&gt; lset list 3 0QUEUED127.0.0.1:6379&gt; lrange list 0 -1QUEUED127.0.0.1:6379&gt; exec1) (integer) 32) (error) ERR index out of range3) 1) &quot;1&quot; 2) &quot;2&quot; 3) &quot;3&quot; 使用 WATCH 进行加锁，UNWATCH 解锁 1234567891011121314151617# 在 watch 监控 money 后 ，创建事务标记前修改 money 值127.0.0.1:6379&gt; set money 100OK127.0.0.1:6379&gt; watch moneyOK127.0.0.1:6379&gt; set money 50OK127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set money 10QUEUED127.0.0.1:6379&gt; get moneyQUEUED127.0.0.1:6379&gt; exec(nil)127.0.0.1:6379&gt; unwatchOK Watch 采用乐观锁策略， Watch 对监控对象加锁时会记录 version，每一次的修改都会创建不同的 version，在事务提交前会比较 version 与初始是否一致，如果不一致则证明被修改，放弃本次事务提交 5 Redis 持久化 redis 计划将 RDB 和 AOF 两种持久化模型合并为一种。（长期计划） 5.1 RDB&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Redis Database（RDB）是 Redis 提供的一种通过 快照 技术实现数据持久化的功能，可以通过配置在指定时间间隔内将内存中的数据集快照写入磁盘中的二进制文件，默认使用 dump.rdb 文件。它提供了三种触发机制： 通过配置文件自动触发 （bgsave 指令）: 123456789101112### RDB 配置 #### 采用异步非阻塞的 bgsave 命令save 900 1 # 如果 900s 内有至少1个 key 被修改，执行持久化操作save 300 10 # 如果 300s 内有至少10个 key 被修改，执行持久化操作save 60 10000 # 如果 60s 内有至少10000个 key 被修改，执行持久化操作persistence-available [(yes)|no] # 是否持久化，在windows下可以关闭，会重定向到堆分配stop-writes-on-bgsave-error yes # 持久化错误是否继续工作rdbcompression yes # 是否亚索 rdb 文件 （消耗cpu资源）rdbchecksum yes # rdb文件校验dbfilename dump.rdb # rdb 文件名dir ./ # rdb文件保存路径 通过 save 命令触发，该命令是同步阻塞式的，即执行save命令期间，Redis不能处理其他命令，直到RDB过程完成为止，执行完成时候如果存在老的RDB文件，就会被替换掉。 通过 bgsave 命令触发，该命令是异步非阻塞式的，原理是 redis 主进程通过 fork 创建一个子进程，持久化的任务就交给子进程来完成 RDB 的优点： RDB文件紧凑，全量备份，非常适合用于进行备份和灾后恢复。例如，您可能希望在最近的24小时内每小时存档一次RDB文件，并在30天之内每天保存一次RDB快照。可以轻松还原数据集的不同版本。 RDB持久化时主进程唯一需要做的就是 fork 一个子进程，通过子进程完成持久化，父进程永远不会执行磁盘I / O或类似操作。 RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快（RDB 是数据导入，AOF 是指令回放）。 RDB 的缺点： 在快照开始时子进程共享父进程内存快照数据，但持久化期间父进程修改的数据不会被子进程探知，如果期间发生意外，则这部分数据将会丢失。 在大数据量且 CPU 负载运行时，频繁的 fork 会造成 CPU 的开销。‘ 注意： BGSAVE 执行期间会拒绝 SAVE 和 BGSAVE 命令。 服务启动后会载入 rdb 文件，在此期间主进程处于阻塞状态。 5.2 AOF&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RDB 在持久化开始时总是统计全量数据集并产生快照，这相对会花费一些时间，并且系统异常时数据丢失率高，AOF （Append Only File）可以作为恰当的替代方案，其原理类似于 MYSQL 的 bin.log 文件，每当 Redis 接收到写命令（set、lpush、lpop等） 时都会通过 write 函数将该命令追加到文件中，形成一个指令日志文件。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可以预见到，无论哪种 AOF 同步策略最终都会导致 aof 文件越来越大，redis 提供了一种 重写方案 ——通过 bgrewriteaof 命令将内存中数据重写，其原理类似于 RDB 的快照原理：redis 主进程 fork 子进程，子进程收集内存全量数据集，以命令的形式重写数据到 aof 文件中，旧文件将被替换。 AOF 通过配置提供了三种同步策略： 123456789101112### AOF 配置 ###appendonly no # 默认使用 rdb 持久化，不开启 aof 模式# AOF同步策略# always 每次写操作都会同步# everysec 每秒同步一次# no 不同步appendfsync always appendfilename \"appendonly.aof\" # aof 文件名no-appendfsync-on-rewrite no # 重写期间是否同步auto-aof-rewrite-percentage 100 # 当 aof 达到文件容量的百分比后自动重写auto-aof-rewrite-min-size 64mb # 当 aof 文件大于文件容量后自动重写 在 Redis 灾后恢复时，通过将 aof 全量指令回放来完成数据的恢复。 AOF 的优点： AOF 一般采用每秒同步策略，异常情况时数据丢失率低。 AOF 文件写入性能高，没有任何磁盘寻址开销。 AOF 智能的通过重写功能减少文件体量堆积。 6 Redis 实现订阅-发布&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Redis 可以作为消息中间件实现标准的消息 订阅-发布 功能：在分布式环境中，单节点的 redis 既可以作为接收消息的客户端（Subscriber）也可以作为发布消息的服务器（Publisher），在 sub 和 pub 之间存在消息传递的频道 channel，订阅者面向频道发起订阅并从频道接收下发的消息；同样的，发布者也针对频道下发，又频道负责消息分发。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Redis 通过 PSUBSCRIBE 、PUBSUB、 PUBLISH、PUNSUBSCRIBE、 SUBSCRIBE、 UNSUBSCRIBE 六个指令完成消息的订阅和发布功能 PSUBSCRIBE ：订阅一个或多个匹配的频道； PUBSUB ：查看订阅与发布系统状态； PUBLISH ：将信息发送到指定的频道； PUNSUBSCRIBE ：退订所有匹配的频道； SUBSCRIBE ：订阅指定的一个或多个频道； UNSUBSCRIBE ：退订指定的频道 演示 本地开启三个客户端窗口，其中选定两个为 subscriber，一个为 publisher。 12345678910111213141516# sub1 订阅频道 msg_channel127.0.0.1:6379&gt; subscribe msg_channelReading messages... (press Ctrl-C to quit)1) \"subscribe\"2) \"msg_channel\"3) (integer) 1# sub2 订阅频道 news_channel127.0.0.1:6379&gt; subscribe news_channelReading messages... (press Ctrl-C to quit)1) \"subscribe\"2) \"news_channel\"3) (integer) 1# pub127.0.0.1:6379&gt; 两个订阅者都订阅了 msg_channel 频道并处于消息监听状态，接下来 publisher 准备发送消息。 12345678910111213141516171819202122232425262728293031# pub 向 msg_channel频道和news_channel频道分别下发两条消息消息127.0.0.1:6379&gt; publish msg_channel \"This is first message from three body\"127.0.0.1:6379&gt; publish msg_channel \"dont repeat! dont repeat!! dont repeat!!!\"127.0.0.1:6379&gt; publish news_channel \"Kobe was a pity\"127.0.0.1:6379&gt; publish news_channel \"R.I.P\"# sub1127.0.0.1:6379&gt; subscribe msg_channelReading messages... (press Ctrl-C to quit)1) \"subscribe\"2) \"msg_channel\"3) (integer) 11) \"message\"2) \"msg_channel\"3) \"This is first message from three body\"1) \"message\"2) \"msg_channel\"3) \"dont repeat! dont repeat!! dont repeat!!!\"# sub2127.0.0.1:6379&gt; subscribe news_channelReading messages... (press Ctrl-C to quit)1) \"subscribe\"2) \"news_channel\"3) (integer) 11) \"message\"2) \"news_channel\"3) \"Kobe was a pity\"1) \"message\"2) \"news_channel\"3) \"R.I.P\" 原理：在 redis 发布订阅模块下维护了一组 字典 ，该字典即为频道，里面记录的频道名和订阅者信息，发布者向频道发送消息，该消息又订阅模块接收并通过字典获得所有订阅者并分发。","link":"/2020/04/01/10%20%E4%B8%AD%E9%97%B4%E4%BB%B6/Redis%EF%BC%88%E4%BA%8C%EF%BC%89/"},{"title":"Redis（三）","text":"Redis（三） 7 Redis 主从复制&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;与 RDBMS 含义一致，在分布式 Redis 系统中，存在多个节点，其中会有==少量若干个 Master 节点负责数据写操作，大量 Slave 节点负责数据读操作==，为了保证读写数据的一致性，就需要维护主-从节点的数据复制。默认情况下，Redis 服务都是主节点，并且每个 Slave 节点只能指向一个 Master节点，同时主从复制能且只能是单向传递的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主从复制的主要作用包括： 数据热备，主从复制实现了数据的热备份，增强了灾后恢复的能力；同时当主节点出现故障，从节点可以代替主节点继续提供服务，提升了系统容错率。 负载均衡，在多数业务写少读多的场景下，通过==主写从读==的形式，大大提高了系统吞吐率。 高可用基础，主从复制能力是提供 Redis 哨兵模式的基础。 具体的配置和使用 查看主从复制配置信息 123456789127.0.0.1:6379&gt; info replication# Replicationrole:masterconnected_slaves:0master_repl_offset:0repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:0repl_backlog_histlen:0 修改每个节点的配置文件 1234portpidfilelogfiledbfilename 启动三个服务器 由于 redis 服务默认都是 master ，需要配置其中两个为 slave。 1234### 通过命令设置主从关系# 设置127.0.0.1:6380 和 127.0.0.1:6381 是 127.0.0.1:6379的从节点127.0.0.1:6380&gt; slaveof 127.0.0.1 6379127.0.0.1:6381&gt; slaveof 127.0.0.1 6379 测试主从读写能力 12345678910111213141516171819202122232425262728# master 写，slave 可读127.0.0.1:6379&gt; set key1 val1127.0.0.1:6380&gt; get key1val1127.0.0.1:6381&gt; get key1val1# slave 不可写127.0.0.1:6380&gt; set key2nil127.0.0.1:6381&gt; set key2nil# master 断开连接，slave 依旧可读127.0.0.1:6379&gt; shutdown127.0.0.1:6380&gt; get key1val1127.0.0.1:6381&gt; get key1val1# slave 断开重连，依旧可以读取数据127.0.0.1:6380&gt; shutdown127.0.0.1:6381&gt; get key1val1# 6380 重连成功127.0.0.1:6380&gt; get key1val1#说明：如果是通过命令设置的主从关系只在本次会话中临时有效，如果断开重连就会自动变为 master 而无法获取数据，现实情况中应该通过配置文件设置主从关系，保证永久性有效。 从上述例子中可以看到，在从机断线重连后依然可以获得主机所有的内存数据，称为 全量复制；当主机新增数据后，从机也能实时获得更新后的数据，称为 增量复制。 8 Redis 哨兵模式&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从第 7 节主从模式中可以看到一些弊端，假如分布式环境中是一主多从结构，如果主节点故障退出，环境中将无法接收写操作并更新到从节点，这将造成严重的问题。Redis 提供了哨兵模式（Sentinel）用于解决该类问题。 8.1 哨兵模式原理&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在哨兵模式中，“哨兵”是一个独立的进程，它通过向集群中的各节点发送心跳信息并等待响应，从而监控运行中的服务的异常。然而，单“哨兵”作为进程也有出现问题的风险或者集群数量过大时会出现性能问题，所以通常会设立 N 个哨兵进行监控，并且“哨兵”进程间也会互相监控，这称为“多哨兵模式”。 当分布式环境中一个 master 节点故障并被其中一个哨兵监视到，系统不会立刻进入 failover 状态；因为仅一个哨兵的报告并不能作为最终的决策，称为主观下线； 当多个哨兵也检测到故障，并且这些哨兵数量达到某个设定的阈值，那么就会认为该节点确实出现故障，此时所有哨兵就会进行一次投票，推选其中一位哨兵发起 failover 操作，切换该哨兵监控下的某个 slave 为 master 并通过 发布-订阅 模式将修改消息下发给各哨兵，再由各哨兵修改自己监控下的节点主从信息，这个过程称为客观下线。 哨兵投票采用raft算法。 规则： 所有的哨兵都有被选举权； 每次哨兵选举后，不管成功与失败，都会计数epoch。 在一个epoch内，所有的哨兵都有一次将某哨兵设置为局部领头的机会，并且局部领头一旦设置，在这个epoch内就不能改变。 每个发现主服务器进入客观下线的哨兵都会要求其他哨兵选举自己。 哨兵选举局部领头的原则是先到先得，后来的拒绝。 如果一个哨兵获得的选票大于半数，则成为领头。 8.2 哨兵模式配置&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在 redis 目录下由 sentinel.conf 文件用于配置哨兵信息。单个哨兵配置信息如下： 12345678910111213141516171819202122#配置端口port 26379#以守护进程模式启动daemonize yes#日志文件名logfile \"sentinel_26379.log\"#存放备份文件以及日志等文件的目录dir \"/opt/redis/data\"#监控的IP 端口号 名称 #sentinel通过投票后认为mater宕机的数量，此处为至少2个sentinel monitor mymaster 127.0.0.1 6379 2#心跳策略#30秒ping不通主节点的信息，主观认为master宕机sentinel down-after-milliseconds mymaster 30000#故障转移后重新主从复制，1表示串行，&gt;1并行sentinel parallel-syncs mymaster 1#故障转移开始，三分钟内没有完成，则认为转移失败sentinel failover-timeout mymaster 180000 12# 启动哨兵进程命令root &gt; redis-sentinel sentinel.conf 9 缓存击穿、穿透、雪崩、污染 缓存穿透：缓存和数据库中都没有真实数据，此时大量访问该无效数据造成系统压力增大，如果攻击者使用缓存穿透持续攻击，将造成持久层崩溃。 解决方案 对无效数据访问后可在缓存创建值为null的返回，并设定较短的过期时间，可以有效避免攻击。 业务层添加有效性校验（布隆过滤），拦截较容易识别的风险。 布隆过滤器是一种概率型的数据结构，对所有可能查询的参数以hash形式存储，在业务层中添加校验，如果不符合则丢弃，这样有效降低了对持久层的压力。 缓存击穿：当缓存数据过期，此时QPS达到峰值并对该数据进行大规模并发访问，造成数据库瞬间访问量骤增导致奔溃。击穿的特点是少量数据过期，之后对这些数据高并发访问。 解决方案 甄别热点数据，采用合适的过期策略如LRU或LFU。 在高QPS来临前，设置Redis冷热数据隔离，假如冷数据发生高并发访问，也可以保证从缓存冷数据读取。 业务层创建互斥锁，当缓存数据不存在时，可以保证同一类请求只有一个可以访问数据库，其他请求阻塞，访问成功后刷新缓存，使其他请求通过缓存访问。 缓存雪崩：缓存数据大量过期，此时业务查询增大，同样导致数据库压力骤增容易发生奔溃。雪崩特点是大量数据过期，之后被大量访问，区别于缓存击穿。 解决方案 设置合理的缓存过期策略和过期时间，可以自定义一个过期时间范围，并将缓存单数据过期时间以哈希方式分散到不同时间范围。 数据预热。 停用非热点服务。 限流降级，当缓存失效后，只允许若干线程访问数据库，其他线程进入阻塞队列等待。 缓存污染：系统将不常用的数据从内存移到缓存，造成常用数据的失效，降低了缓存的利用率。缓存容量是弥足珍贵的，容量过大反而容易影响查询效率，所以在有效的空间内保证热点数据很重要。 解决策略 设置合理的过期策略，FIFO、LRU、LFU等。 业务层识别，避免大而全的数据添加进缓存中。 10 Jedis 和 Lettuce&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Jedis 和 Lettuce 都是 Redis 的 Java Api，Jedis 采用源码直连 redis ，在多线程环境中使用是不安全的，为了避免线程安全问题，使用 Jedis Pool 连接池；而 lettuce 采用 netty 与 redis 通信，其实例可以在多个线程中共享，不存在线程安全问题，所以 SpringBoot 新版本在自动配置类中虽然提供了两种对象，但已默认使用 Lettuce。 SpringBoot 集成 Redis 可以参阅本站《Spring及源码——SpringBoot（三）》 13 小节 单机 maven 环境使用 Jedis： 导入依赖 1234567891011&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.62&lt;/version&gt;&lt;/dependency&gt; 连接本地 Redis，首先需要开启本地 redis 服务 Jedis 提供的 api 方法与 redis 指令保持一致，所以不展开介绍，仅测试使用 1234567public static void main( String[] args ) { Jedis jedis = new Jedis(\"127.0.0.1\", 6379); System.out.println(jedis.ping());}// output// PONG// 连接成功 其他功能使用 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475public static void main( String[] args ) { Jedis jedis = new Jedis(\"127.0.0.1\", 6379); // string System.out.println(jedis.flushDB()); System.out.println(jedis.setnx(\"name1\", \"zhangsan\")); System.out.println(jedis.setex(\"name2\", 50, \"lisi\")); System.out.println(jedis.get(\"name1\")); // set System.out.println(jedis.sadd(\"set1\", \"A\", \"B\", \"C\", \"D\", \"E\")); System.out.println(jedis.sadd(\"set2\", \"H\", \"B\", \"I\", \"D\", \"K\")); System.out.println(jedis.sdiff(\"set1\", \"set2\")); System.out.println(jedis.sunion(\"set1\", \"set2\")); // 对象 User user = new User(\"wangwu\", \"123456\"); JSONObject json1 = new JSONObject(); json1.put(\"user\", user); System.out.println(jedis.set(\"user\", json1.toJSONString())); System.out.println(\"== 输出所有 keys ==\"); jedis.keys(\"*\").forEach(System.out::println); System.out.println(\"== ==\"); System.out.println(jedis.get(\"user\")); // 事务 System.out.println(\"==== 事务 ====\"); System.out.println(jedis.flushDB()); System.out.println(jedis.set(\"money\", \"100\")); System.out.println(jedis.watch(\"money\")); Transaction tx = jedis.multi(); try { tx.incrBy(\"money\", 5); tx.decrBy(\"money\", 33); tx.exec(); } catch (Exception e) { tx.discard(); e.printStackTrace(); } System.out.println(jedis.get(\"money\")); System.out.println(\"========\"); System.out.println(jedis.dbSize()); System.out.println(jedis.flushAll());}static class User { private String username; private String password; public User() { } public User(String username, String password) { this.username = username; this.password = password; } public String getUsername() { return username; } public void setUsername(String username) { this.username = username; } public String getPassword() { return password; } public void setPassword(String password) { this.password = password; }}","link":"/2020/04/01/10%20%E4%B8%AD%E9%97%B4%E4%BB%B6/Redis%EF%BC%88%E4%B8%89%EF%BC%89/"}],"tags":[{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"MySQL优化","slug":"MySQL优化","link":"/tags/MySQL%E4%BC%98%E5%8C%96/"},{"name":"数据结构","slug":"数据结构","link":"/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"缓存","slug":"缓存","link":"/tags/%E7%BC%93%E5%AD%98/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"源码","slug":"源码","link":"/tags/%E6%BA%90%E7%A0%81/"},{"name":"多线程","slug":"多线程","link":"/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"AQS","slug":"AQS","link":"/tags/AQS/"},{"name":"JUC","slug":"JUC","link":"/tags/JUC/"},{"name":"线程池","slug":"线程池","link":"/tags/%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{"name":"协程","slug":"协程","link":"/tags/%E5%8D%8F%E7%A8%8B/"},{"name":"纤程","slug":"纤程","link":"/tags/%E7%BA%A4%E7%A8%8B/"},{"name":"COW","slug":"COW","link":"/tags/COW/"},{"name":"设计模式","slug":"设计模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"第三方工具","slug":"第三方工具","link":"/tags/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%B7%A5%E5%85%B7/"},{"name":"Spring","slug":"Spring","link":"/tags/Spring/"},{"name":"中间件","slug":"中间件","link":"/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"Shiro","slug":"Shiro","link":"/tags/Shiro/"},{"name":"SpringBoot","slug":"SpringBoot","link":"/tags/SpringBoot/"},{"name":"swagger","slug":"swagger","link":"/tags/swagger/"},{"name":"Log","slug":"Log","link":"/tags/Log/"},{"name":"Lombok","slug":"Lombok","link":"/tags/Lombok/"},{"name":"Redis","slug":"Redis","link":"/tags/Redis/"}],"categories":[]}