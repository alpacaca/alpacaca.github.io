{"pages":[],"posts":[{"title":"Github + Hexo 搭建博客(二)","text":"3 创建Github SSH连接Github支持https和ssh两种方式管理本地和远程代码的同步，作为个人repo的拥有者，推荐使用ssh方式，因为每次写博客上传远端库时可以省略用户名和密码的验证。 ① 创建ssh key 1$ ssh-keygen -t rsa -C &quot;your_email@example.com&quot; “your_email@example.com”使用github个人账户邮箱，创建成功后连续三次回车，以下分别说明 12Generating public/private rsa key pair.# Enter file in which to save the key (/c/Users/you/.ssh/id_rsa): [Press enter] 第一次要求输入保存公钥和秘钥的文件路径，直接回车视为默认，保存在/c/Users/you/.ssh/id_rsa下 12Enter passphrase (empty for no passphrase): # Enter same passphrase again: 接下来两年次回车是提交密码和密码确认，可以默认无密码。 当然，如果要设置密码需要说明，该密码仅为push本地代码到远端时的密码，而非账户密码，建议省略。 接下来，就会完成创建并保存到本地 1234Your identification has been saved in /c/Users/you/.ssh/id_rsa.# Your public key has been saved in /c/Users/you/.ssh/id_rsa.pub.# The key fingerprint is:# 01:0f:f4:3b:ca:85:d6:17:a1:7d:f0:68:9d:f0:a2:db your_email@example.com ② 绑定ssh key 在保存路径下找到id_rsa.pub，打开并复制其中的内容 登录github，在个人Setting下找到SSH and GPG keys 并将复制的内容添加一个新的SSH Key。 至此，绑定ssh完毕 ③ 测试ssh连接在git bash中输入以下命令 12345$ ssh -T git@github.comThe authenticity of host 'github.com (207.97.227.239)' can't be established.# RSA key fingerprint is 16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48.# Are you sure you want to continue connecting (yes/no)? 显示警告无妨，输入yes并继续，会得到最终测试的结果。 12Hi username! You've successfully authenticated, but GitHub does not# provide shell access. 如果显示successfully authenticated表示ssh授权成功，可以使用； 相反，如果提示access denied表示失败，可以删除已绑定的ssh并重新进行②步骤，注意粘贴内容不含其它字符包括空格和回车。 4 将初始化博客部署至github上一节最终已经在本地初始化了hexo博客并且正常运行，接下来需要部署至github的仓库并通过互联网域名进行访问 ① 首先，需要在根路径安装自动部署插件 1d:\\blog &gt; npm install hexo-deployer-git --save ② 配置部署信息 首先，访问github，并在上节创建的仓库中找到专属SSH连接地址，复制该地址 其次，在本地根路径下打开_config.yml文件，找到deploy对象并进行配置 说明1：由于建议使用ssh方式，这里type和repo都是ssh连接。当然也支持https方式，在②中就需要复制https对应得到地址配置在此处 说明2：github搭载的hexo只支持上传master分支，如果上传其他自定义分支是无法正常显示的。所以branch配置master分支 说明3: yml文件特别需要注意冒号后的一个空格，如果疏忽则会导致配置出错。并且yml文件受到缩进的严格限制来进行归类，这区别于properties文件和json文件。 ③ 自动化部 使用以下命令进行生成静态文件和部署操作。 1d:\\blog &gt; hexo d -g 或者 123d:\\blog &gt; hexo gd:\\blog &gt; hexo d 当显示部署成功后,就可以通过上节中配置的域名https:\\\\yourdomain.github.io进行访问，当然，你也可以访问github仓库查看本次上传的全部内容。 结尾 `OK，你期望的美好正在发生，但是觉得页面太丑？或者我如何发布自己写的博客，这将在下节展开`","link":"/2018/09/26/1%20Github%20+%20Hexo%20%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/2%20Github%20+%20Hexo%20%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2(%E4%BA%8C)/"},{"title":"Github + Hexo 搭建博客(四)","text":"7 Hexo写作和Markdown首先介绍Hexo博客的原理：本地编辑好的文本通过hexo generate命令，编译为静态html文本并发布到远端展示。换句话说，Hexo展示的内容都是Html文本形式的，这就意味着在本地编写的文本支持编译html。 这就引入了Markdown文本编辑 Markdown，是一种可以使用普通文本编辑器编写的标记语言，通过简单的标记语法，它可以使普通文本内容具有一定的格式(无法访问wiki，摘自百度百科解释)。 对于Markdown写作的规范和优势，可以自行百度，这里不多做解释，只需要记住一点，Markdown可以让我们更加关注内容的编写而不用花费更多精力在格式排版上。 既然作为标记语言，那么同样支持W3C规范的html标签。 比如: 一级标题、二级标题、三级标题等对应的Markdown格式为`#一级标题`、`##二级标题`、`###三级标题` 也可以使用Html标签`一级标题`、`二级标题`、`三级标题` Markdown文本编辑工具由于我个人在此之前长期使用印象笔记记录内容，搭配使用的是(马克飞象)[https://maxiang.io]Markdown编辑工具，良好的支持本地活在线编辑并和印象同步内容，然而马克飞象并不是当做其他md文本编辑的好工具。 在使用了一众md编辑工具后，最终还是选择安利Visual Studio Code。VS Code本身并不是md工具，只是微软做出来与WebStorm抗衡的前端IDE，然而它实在是太优秀了，并且很好的支持Markdown的编辑工作，甚至hexo根目录下的yml文本、前端各种文本都可以编辑和展示，软件反应灵敏且安装容量极小，这就让我爱不释手。 注意： 不同的md编辑工具可能某些语法并不相同，比如Latex公式、表格甚至链接和图片等。但总的md标记不变，只是在不同工具上渲染出的效果不一样. 这就好比不同的浏览器，内核不同，渲染出的网页效果也有一定的出入。具体最终渲染出的效果什么样还要遵循Hexo的md规范。 写作新建写作Ok，假设你已经掌握了md的基本语法，我们接下来可以正式进入编写博客了。 1d:\\blog &gt; hexo new myblog 使用该命令创建一个新的文章，myblog会保存在_posts文件中，该文件是博客存放和最终编译静态标记文本的路径。当然你也可以使用如下命令创建本次草稿，并最终移动到_posts下发布到远端（个人不推荐，原因是麻烦且没必要） 12345d:\\blog &gt; hexo new draft myblog #创建草稿myblog并在draft文件下保存# 编写草稿内容，完成后执行d:\\blog &gt; hexo publish [scaffold] #将草稿移动到_posts下，也可指定scaffold模板 写作模板说明1234567---title: Github + Hexo 搭建博客(四)date: 2018-9-30tags: hexotoc: hexo---# 内容 目前我采用的写作头文件是这样： title: 表示文章的标题，也是在博客首页最上方醒目显示的地方。 date： 文章写作时间 tags: 本所所属的标签，标签可以是单个标签，也可以是标签序列，如tags: [github, hexo, blog],需要在根目录的_config.yml中开启标签支持 toc： 文章目录。 申明头部之后，就可以在下方开始使用md书写内容了。 博客内图片Markdown内的图片常规引入方式为使用标记![text](imgurl)，然而在hexo中并不适用，这也是标记不一致的地方，由于Hexo要生成静态页面，需要在同级目录中创建保存静态文件的文件夹。具体方式如下：在根目录_config.yml中配置 1post_asset_folder: true 在使用命令d:/blog &gt; hexo new myblog后会在同级目录中创建同名文件夹，将需要引入文章的图片保存在该文件夹中并命名，比如test.jpg。在md文本中使用如下命令引入 1{% asset_img test.jpg Hexo %} 创建博客评论功能之——Gitment登录Gitment进行注册，注册成功后会获得client_id和client_secret,进入主题下的_congif.yml配置如下： 12345gitment_owner: alpaca #你的 GitHub IDgitment_repo: 'https://alpacaca.github.io/' #存储评论的 repogitment_oauth: client_id: 'xxx' #client ID client_secret: 'xxx' #client secret 配置成功后文章底部就会支持Gitment评论功能。 注意： Gitment目前只能登录git账号后才能评论 切勿滥用！可以看[这里](https://imsun.net/posts/gitment-introduction/) 结尾 `Github + Hexo搭建博客，暂且告一段落。当然，有深度定制Hexo需求甚至自建风格都可以参照官方文档进行开发，或者可以通过修改主题内的ejs文件和css文件。`","link":"/2018/10/02/1%20Github%20+%20Hexo%20%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/4%20Github%20+%20Hexo%20%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2(%E5%9B%9B)/"},{"title":"Github + Hexo 搭建博客(一)","text":"1 申请Github的page域名访问Github官网新建账号，并创建个人仓库。 在个人仓库操作框下进行设置settings，要满足三级域名要求必须进行如下操作。 Repository name 必须保证三级域名与账号一致，且以io顶级域名结束，否则访问路径会出现在路径分隔符后。例如：正常域名:alpacaca.github.io；错误域名www.github.com/alpacaca/xxxblog 如果正确创建，在GitHub Pages中正确显示已经发布的域名地址 可以在个人repo的master分支中创建readme.md用作说明或提示类信息（不要求）。 下载Github工具，安装带有Github Bash的命令提示符。 2 配置环境并下载HexoHexo下载安装依赖npm，npm是nodejs包管理器，nodejs安装依赖python环境 下载python3版本并安装，在环境变量path中添加python解释器的路径。添加项应是安装路径下的python3.exe 下载stable稳定版nodejs并安装，需要安装npm包管理器。使用以下命令查看npm 和node的版本是否正确。 12user &gt; npm -vuser &gt; node -v （由于万里长城的原因，npm默认中央仓库可能无法正常访问或者加载速度过慢）这时，需要通过以下命令查看metrics-registry参数 1user &gt; npm config list taobao提供国内npm仓库镜像https://registry.npm.taobao.org/，可以通过以下命令设置。 1user &gt; npm config set registry http://registry.npm.taobao.org/ 此时，nodejs和npm的准备工作已经完成。 通过npm安装hexo 12345#(此处为注释，下同)全局安装hexo组件user &gt; npm install hexo -g #安装完毕查看hexo版本user &gt; hexo -v 可以通过cmd进入硬盘任意位置 或者 在任意位置通过地址栏输入cmd打开命令提示符，初始化hexo组件、安装并初次运行 12345678910111213# 初始化hexo组件d:\\blog &gt; hexo init# 安装依赖组件d:\\blog &gt; npm install# 当全部完成后，生成静态文件d:\\blog &gt; hexo g# 本地浏览d:\\blog &gt; hexo s -o# hexo默认端口4000，如果此处端口被占用，可以使用下列命令，xxx为自定义端口d:\\blog &gt; hexo s -o -p xxx 至此，当在浏览器中出现下图，表示hexo已经在本地可以完美运行。接下来需要发布blog到Github，并在互联网上进行访问。 结尾`本地初始化博客已经跑起来了，如何部署到远端通过互联网域名方式访问，将在下节展开`","link":"/2018/09/19/1%20Github%20+%20Hexo%20%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/1%20Github%20+%20Hexo%20%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2(%E4%B8%80)/"},{"title":"Github + Hexo 搭建博客(三)","text":"5 Hexo全局配置初次浏览全局配置文件，在本地根目录下找到_config.yml文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: zhy's blogsubtitle: Stay Hungry , Stay Foolishdescription:keywords: zhy blogauthor: zhylanguage: zh-Hanstimezone:# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: http://yoursite.comroot: /permalink: :year/:month/:day/:title/permalink_defaults:# Directorysource_dir: sourcepublic_dir: publictag_dir: tagsarchive_dir: archivescategory_dir: categoriescode_dir: downloads/codei18n_dir: :langskip_render:# Writingnew_post_name: :title.md # File name of new postsdefault_layout: posttitlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0render_drafts: falsepost_asset_folder: truerelative_link: falsefuture: truehighlight: enable: true line_number: true auto_detect: false tab_replace: # Home page setting# path: Root path for your blogs index page. (default = '')# per_page: Posts displayed per page. (0 = disable pagination)# order_by: Posts order. (Order by date descending by default)index_generator: path: '' per_page: 10 order_by: -date # Category &amp; Tagdefault_category: uncategorizedcategory_map:tag_map:# Date / Time format## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DDtime_format: HH:mm:ss# Pagination## Set per_page to 0 to disable paginationper_page: 10pagination_dir: page# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: yilia# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: git@github.com:alpacaca/alpacaca.github.io.git branch: master 注意：yml文件相比于properties配置文件，更加简洁。类似于python代码，主从配属严格受到缩进的影响，特别特别需要强调的是，配置名后的冒号和参数之间是一定有空格的！ 接下来分别认识一下配置文件。 站点和url配置123456789101112131415# Sitetitle: zhy's blogsubtitle: Stay Hungry , Stay Foolishdescription:keywords: zhy blogauthor: zhylanguage: zh-Hanstimezone:# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: http://yoursite.comroot: /permalink: :year/:month/:day/:title/permalink_defaults title ， 是浏览器标签显示的名字 subtitle ， 子标题 keywords ， 主要用于SEO，可以采用列表表示[key1,key2],下同 author, 作者 language: 初次加载根据pc环境选择，可以手动修改 timezone： 时区，默认同系统 说明 如果你部署的博客地址并非三级以内的域名，而是地址分割符后的，比如`http://github.com/alpacaca`，那么需要配置url信息，如下 url： 默认不变，如果是地址分割符后地址需要配置http://yoursite.com/child root: /child/ permalink: 永久链接 目录123456789# Directorysource_dir: sourcepublic_dir: publictag_dir: tagsarchive_dir: archivescategory_dir: categoriescode_dir: downloads/codei18n_dir: :langskip_render: source_dir 资源文件夹，这个文件夹用来存放内容。 public_dir 公共文件夹，这个文件夹用于存放生成的站点文件。 tag_dir 标签文件夹 archive_dir 归档文件夹 category_dir 分类文件夹 code_dir Include code 文件夹 i18n_dir 国际化（i18n）文件夹 skip_render 跳过指定文件的渲染，可使用 glob 表达式来匹配路径。 写作123456789101112131415# Writingnew_post_name: :title.md # File name of new postsdefault_layout: posttitlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0render_drafts: falsepost_asset_folder: truerelative_link: falsefuture: truehighlight: enable: true line_number: true auto_detect: false tab_replace: new_post_name 新文章的文件名称 default_layout 预设布局 auto_spacing 在中文和英文之间加入空格 titlecase 把标题转换为 external_link 在新标签中打开链接 filename_case 把文件名称转换为 (1) 小写或 (2) 大写 render_drafts 显示草稿 post_asset_folder 启动 Asset 文件夹 relative_link 把链接改为与根目录的相对位址 future 显示未来的文章 highlight 代码块的设置 分页、主题和部署12345678910111213141516# Pagination## Set per_page to 0 to disable paginationper_page: 10pagination_dir: page# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: yilia# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: git@github.com:alpacaca/alpacaca.github.io.git branch: master per_page 每页显示博客数 pagination_dir 分页目录 theme 主题，将在下节介绍 deploy 部署 关于deploy当安装了hexo部署插件后，可以通过配置deploy自动进行部署，我选择的是以git方式发布到repo中的master分支 最终，使用hexo d -g发布到远程仓库 6 主题的选择和配置如果不喜欢Hexo的默认主题（没有人会喜欢[doge]），官方还提供了大量的主题类型，可以访问这里选择自己认为ok的主题 比如：中意首页展示的[Aria主题](https://sh.alynx.xyz/)，那么你可以在最下方footer标签范围找到该主题对应托管在github上的[资源](https://github.com/AlynxZhou/hikaru-theme-aria/)clone该项目到本地根目录下的themes文件夹下。 首先，需要在根目录下的/_config.yml中配置theme指定aria 1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: aria 之后进入主题内的置/themes/aria/_config.yml文件中进行自定义配置 关于主题这里就不展开了，因为不同的主题都有各自不同的配置要求，具体说明可以在打开的github资源首页READEME.md下找到 当配置完之后可以本地查看修改后的状态，运行 1d:\\blog &gt; hexo s -o 结尾 `经过定制化的个人博客成功运行，你可以动手写文章啦！什么？你还不知道怎么写？那么我们下一节介绍具体介绍。` 目前博客的书写均采用MarkDown进行编辑，如果你已经掌握可以跳过下节","link":"/2018/09/30/1%20Github%20+%20Hexo%20%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/3%20Github%20+%20Hexo%20%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2(%E4%B8%89)/"},{"title":"MySQL优化系列（一）","text":"MySQL优化系列（一） 0 总述按照Java当前框架式的开发模式，我总结单系统Mysql优化主要分为三类： 代码级优化。 数据库级优化。 框架级优化。 代码级优化主要包含： 尽量使用联接查询代替子查询（嵌套查询）。 避免使用关键字LIKE或正则表达式匹配，会导致全表扫描。 DELETE删除大量数据后，使用Optimize table tb_xxx释放残余空间。 避免在where中使用函数或表达式。 避免使用使索引失效的操作。 避免使用select *，会导致全表查询。 使用相对较高的性能分页，比如where id &gt; 100 limit 20 性能优于limit 100, 20。 Join时使驱动表为小数据量表。 数据库级优化主要包含： 建立索引，并使用explain调试接近最优查询。 合理使用存储过程。 选择适合业务特点的数据库引擎（事务、锁机制）。 掌握使用慢查询日志。 mysql buffer和cache。 物理资源使用分析（cpu使用率和io阻塞）。 框架级优化主要指多级缓存策略，其中包含： 关系型数据库中，ORM框架支持的一二级缓存。 非关系型数据库中，redis数据库缓存。 1 准备1.1 数据准备&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在展开研究Mysql数据库优化前，我们先创建一组数据，用于分析所有涉及的操作行为。四张数据表的数据字典如下：（主键id均为整型自增，非特别说明都使用varchar类型） tb_course : id，name（课程名），is_required（是否必修课 boolean），credit（学分 int） tb_teacher : id，name（教师名），age（年龄），course_id（所授课程id） tb_student : id，name（学生名），age（年龄 int） tb_student_course : id，student_id（学生id），course_id（课程id），is_pass（是否及格 boolean），score（得分 int） &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;创建数据库脚本代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788CREATE DATABASE `optdemo` CHARACTER SET 'utf8';USE `optdemo`;#创建课程表，包括id，名称，是否必修课，学分CREATE table `tb_course`( id int primary key auto_increment, name varchar(20), is_required boolean, credit int)ENGINE = InnoDB CHARACTER SET = utf8;#创建老师表，包括id，名称，年龄，教授课程CREATE table `tb_teacher`( id int primary key auto_increment, name varchar(20), age int, course_id int)ENGINE = InnoDB CHARACTER SET = utf8;#创建学生表，包括id，名称，年龄，所学课程CREATE table `tb_student`( id int primary key auto_increment, name varchar(20), age int)ENGINE = InnoDB CHARACTER SET = utf8;#创建学生--课程关系表，包括id，学生id，课程id，是否通过CREATE table `tb_student_course`( id int primary key auto_increment, student_id int not null, course_id int not null, is_pass boolean, score int)ENGINE = InnoDB CHARACTER SET = utf8;#初始化表INSERT INTO `tb_course` (name, is_required, credit) VALUES ('C', true, 4), ('JAVA', true, 4), ('数据结构', true, 3), ('操作系统', true, 4), ('C#', false, 2), ('Python', false, 3), ('人工智能', true, 4), ('TCP/IP', false, 2), ('计算机网络', true, 3),('Linux', false, 2);INSERT INTO `tb_teacher` (name, age, course_id)VALUES('老张', 45, 1),('老王', 46, 2),('老李', 48, 3),('老赵', 35, 4),('老吴', 55, 5),('老孙', 45, 6),('老刘', 40, 7),('老贾', 39, 8),('老钱', 32, 9),('老周', 36, 10);INSERT INTO `tb_student` (name, age)VALUES('小张', 18),('小王', 18),('小李', 18),('小赵', 18),('小吴', 18),('小孙', 18),('小刘', 18),('小贾', 18),('小钱', 18),('小周', 18);INSERT INTO `tb_student_course` (student_id, course_id, is_pass, score)VALUES(1,1,true, 80), (1,2,true, 90), (1,3,true, 92), (1,4,true, 93), (1,7,true,87), (1,9,true, 89), (1,10,true, 78), (2,1,true, 99), (2,2,true, 98), (2,3,true, 100), (2,4,true,100), (2,7,true,95), (2,8,true,91), (2,9,false,55),(3,1,true, 76), (3,2,true,68), (3,3,true,82), (3,5,true,77), (3,7,true,71), (3,9,false,59),(4,1,true,100), (4,2,true,100), (4,3,true,100), (4,4,true,100), (4,7,true,100), (4,9,true,100),(5,1,false, 42), (5,2,false,39), (5,3,false,33), (5,4,false,21), (5,5,true,60), (5,7,false,0), (5,9,false,58),(6,1,false,82), (6,2,true,78), (6,3,true,78), (6,4,false,49), (6,6,true,63), (6,7,false,47), (6,9,true,82),(7,1,true,98), (7,2,true,96), (7,3,true,92), (7,4,true,91), (7,5,true,96), (7,6,true,99), (7,7,true,93), (7,8,true,99), (7,9,true,94), (7,10,true,98),(8,1,false,32), (8,2,false,33), (8,3,false,42), (8,4,false,43), (8,7,false,52), (8,9,false,53),(9,1,false,36), (9,2,false,39), (9,3,false,56), (9,4,false,55), (9,7,false,55), (9,9,false,51),(10,1,true,86), (10,2,true,88), (10,3,true,92), (10,4,true,100), (10,5,true,100), (10,6,true,100), (10,7,false,100), (10,8,true,100), (10,9,true,100), (10,10,true,100); 1.2 Mysql逻辑分层结构&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先确定一个概念，通常开发中操作的是Mysql Client客户端，与Client相连并且处理数据和存储数据的称为Mysql Server或者Database Manager System（DBMS），Client职责就是输入并发送给Server处理，所以这里我们只讨论Server的逻辑分层结构。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从上图大体可以看出来，按逻辑共分为四层，概括一下包含：连接层、服务层、数据驱动层（引擎层）和数据层。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;① 连接层被设计为连接池形式，主要负责与Client的通信，同时提供授权和安全等策略； &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;② 服务层是Mysql Server的核心结构，主要包括：管理服务、SQL接口、SQL解析器、SQL优化器和缓存，具体介绍请看下一段。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;③ 数据驱动层（引擎层）是由Mysql提供的插件式数据引擎套件，按照业务实际需要选择合适的引擎，将会很大程度上提高算力。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;④ 数据层主要是物理层次的数据及相关信息的存储，例如慢查询日志等。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在服务层中，管理服务主要负责备份、恢复、数据迁移、集群等操作；SQL接口主要负责最熟悉的DML、DDL、存储过程、视图和触发器等处理；SQL解析器主要负责语法解析；SQL优化器主要负责重写系统判定不够优化的语句，也就是说，当我们通过Client提交一个SQL之后，优化器可能会对我们的SQL等效重写并继续向下执行，重写后的语句是Server认为足够优化的语句。SQL缓存将在之后章节具体介绍。 由于SQL优化器的存在，我们写的SQL并不一定是Server执行的SQL。 1.3 DQL执行顺序与SQL优化器 DQL : Data Query Language 数据查询语言，特指Select及相关的(group by etc.)操作统称。 DML : Data Manipulation Language 数据操作语言，包含INSERT、DELETE、UPDATE。 DDL : Data Defination Language 数据定义语言，包含CREATE、 DROP等。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在数据库DQL中，存在一条线性的关键字匹配和处理流程，这里所说的关键字就是与数据库操作有关的系统关键字。首先看下列关键字序列： 123456789101112131415SELECT DISTINCT &lt; select_list &gt;FROM &lt; left_table &gt; &lt; join_type &gt; JOIN &lt; right_table &gt; ON &lt; join_condition &gt;WHERE &lt; where_condition &gt;GROUP BY &lt; group_by_list &gt;HAVING &lt; having_condition &gt;ORDER BY &lt; order_by_condition &gt;LIMIT &lt; limit_number &gt; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这是一条日常开发中涉及较多关键字的SQL，也是SQL解析器认为合法的表示流程，但实际上，SQL解析器解析之后是按照如下顺序执行： 12345678910FROM &lt;left_table&gt;ON &lt;join_condition&gt;&lt;join_type&gt; JOIN &lt;right_table&gt;WHERE &lt;where_condition&gt;GROUP BY &lt;group_by_list&gt;HAVING &lt;having_condition&gt;SELECT DISTINCT &lt;select_list&gt;ORDER BY &lt;order_by_condition&gt;LIMIT &lt;limit_number&gt; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;掌握Mysql执行过程很重要，因为在后续优化过程中，经常需要确定驱动表以及多表查询时的执行顺序影响效率问题。 简化表示：FROM…ON…JOIN…WHERE…GROUP BY… HAVING…SELECT…DISTINCT…ORDER BY…LIMIT 简单来说，就是按顺序找到表，再从表中抽数据，再组织数据 2 代码级优化&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;代码级优化是指，针对手写或者ORM框架生成的原生SQL语句进行优化，优化目的是产生运行效率更高的执行语句。多数情况下，我们针对DQL进行优化。 2.1 使用联接代替子查询&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;子查询是指，使用SELECT查询出目标结果集，然后将该结果集当作其他查询的过滤条件使用。换句话说就是一个SELECT查询中的WHERE条件嵌套另一个或多个SELECT语句，这种写法不仅将多个逻辑联系起来更符合自然逻辑，而且避免了执行中的死锁和事务安全问题，但是，在执行过程中，Mysql会对子查询创建临时表，这就增加了IO消耗。 例：查询选修了Python课程的学生信息。 123456SELECT * FROM `tb_student`WHERE id IN ( SELECT student_id FROM `tb_student_course` WHERE course_id IN ( SELECT id FROM `tb_course` WHERE name = 'Python')); 声明：为了方便，使用SELECT *写法，实际业务处理中应使用具体字段SELECT id, name, age写法。 上述例子中，使用了三层SELECT语句组成的完整查询，逻辑清晰但实际运行效率并不高，下面改成JOIN写法。 12345SELECT t1.* FROM `tb_student` t1 JOIN `tb_student_course` t2 ON t1.id = t2.student_id JOIN `tb_course` t3ON t2.course_id = t3.idWHERE t3.name = 'Python'; 或者 123SELECT t1.* FROM `tb_student` t1, `tb_student_course` t2, `tb_course` t3 WHERE t1.id = t2.student_id AND t2.course_id = t3.id AND t3.name = 'Python'; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在排除SQL优化器和缓存条件（重复执行，效率会提高）时，从执行时间来看，后两者效率明显比前者效率高。 2.2 删除操作2.2.1 DELETE、DROP和TRUNCATE的区别 DELETE属于DML，执行操作时，每次从目标表中删除一行数据，并且删除行为作为日志保存以便进行”恢复“操作；而DROP和TRUNCATE属于DDL，其操作不能回滚。 DELETE时会执行相关的触发器，并且执行之后需要显示的commit才能完成删除动作[1]；而DROP和TRUNCATE会隐式commit，且不会执行触发器。 DROP删除表中所有数据，并释放表空间；TRUNCATE删除表中所有数据，重置高水位（high watermark）[2]；DELETE逐行删除，高水位保持不变。 [1] : 在Mysql中，默认DML开启了自动提交，所以执行DML之后无需commit，但并不代表它是隐式执行，可以通过SHOW VARIABLES LIKE 'autocommit'查看。 [2] : 举个例子，例如表中id主键自增且当前id=10，当DELETE 全表之后再新增，id为11；而TRUNCATE和DROP之后再新增，id=1。 2.2.2 DELETE大数据量优化&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DELETE删除大数据量后，不仅产生了许多日志空间，且所删除的空间并未被释放，此时需要使用OPTIMIZE TABLE [数据库]来释放。 123DELETE FROM `tb_student_course`;OPTIMIZE TABLE `tb_student_course`; 2.3 避免使用SELECT * 使用SELECT *会查询全表字段作为结果集，当我们只需要部分而不是全部字段作为结果集时，就会造成资源浪费，不仅降低了查询效率，还增加了网络IO使用率；当多人维护表时，如果表字段发生变化（增加或者删除）就会造成预期外的结果，或者需要额外的后台代码进行过滤，不利于维护。 安全性考虑，如果发生SQL注入风险，有可能被攻击者创建联表条件，从而更多信息被暴露。 SELECT 目标列正好是索引时，会更快的从内存（B+Tree）中读取数据并返回而不产生本地IO。 【本条转自】：https://blog.csdn.net/u013240038/article/details/90731874 连接查询时，* 无法进入缓冲池查询。 每次驱动表加载一条数据到内存中，然后被驱动表所有的数据都需要往内存中加载一遍进行比较。效率很低，所以mysql中可以指定一个缓冲池的大小，缓冲池大的话可以同时加载多条驱动表的数据进行比较，放的数据条数越多io操作就越少，性能也就越好。所以，如果此时使用select * 放一些无用的列，只会白白的占用缓冲空间。浪费本可以提高性能的机会。 2.4 分页优化&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据库层级的分页优化，实际上就是针对LIMIT关键字的优化，Mysql支持的LIMIT关键字系统中可以用作分页处理，表达式：LIMIT offset,rows 或 LIMIT rows。比如LIMIT 123456,50，从123456行开始查询50条数据；LIMIT 50将查询结果取前50条数据。LIMIT虽然使用方便但在大数据量时就会出现性能问题，随着offset增大，性能急剧下降，究其原因是因为LIMIT 123456,50会先扫描123456+50=123506条数据，然后返回50条，额外扫描的123456并不是我们需要的，事实上这样的设计显得多余，当数据量级增加之后，性能必然骤降。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;优化方式一：从实际情况来看，受影响的似乎只有LIMIT offset, rows形式，而LIMIT rows并不影响性能，实际上后者只扫描rows行数据，所以我们可以进行如下替换： 12345SELECT * FROM table LIMIT 123456,50;#LIMIT扫描123506条数据#转换后SELECT * FROM table WHERE id &gt;= 123456 LIMIT 50;#LIMIT只扫描50条数据 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;优化方式二：使用覆盖索引，在后续介绍索引时将深入介绍，在此简单提一下：覆盖索引是指，查询列正好全部具有索引，则结果直接从B+Tree中获取而不用回表操作。比如SELECT col1 FROM tb_test LIMIT 123456,50；，正好col1具有索引，那么执行该语句的时候会直接从col1索引对应的数据结构中获得结果，而不用回表到tb_test中再次查询，减少了磁盘IO。 12ALTER TABLE `tb_test` ADD INDEX col1_index(col1);SELECT col1 FROM tb_test LIMIT 123456, 50; 注：创建表并添加主键之后，会自动创建主键索引 Primary Key Index。 2.5 JOIN时使驱动表为小数据量表&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在前文中我们已经提到使用JOIN代替子查询可以明显提高效率，但JOIN本身也有需要更进一步的优化。首先介绍一下JOIN的原理：将驱动表的查询结果作为“输入”，当作条件逐条的在被驱动表内进行过滤，最终返回过滤后的数据，这种JOIN的方式被称作NEST LOOP。 1234567SELECT t1.* FROM tabel1 t1 LEFT JOIN table2 t2 ON t1.condition = t2.conditionJOIN table3 t3ON t1.condition = t3.condition AND t2.condition = t3.conditionWHERE ....ORDER BY t2.id DESC; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这是一个相对复杂的例子，其中用到了LEFT JOIN和JOIN，解析一下过程：将table1当作驱动表查询并返回结果result1，将result1当作被驱动表t2的“输入”，并将result1中的数据逐条取出在table2中匹配，并将最终的结果返回result2，将table1和table2联合查询后的结果result2当作table3的”输入“，并逐条匹配并返回最终结果result3，排序后当作最终的结果返回。所以在使用多表JOIN时，明确表的数据条的多少，有助于我们确定使用小表作为驱动表，从而减少循环次数，达到优化的目的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;庆幸的是，在不显式地指定驱动表时，SQL优化器帮助我们将最小的表作为驱动表来执行，所谓显式指定是指LEFT JOIN时左侧表为驱动表，RIGHT JOIN时右侧表为驱动表，而JOIN属于隐式，SQL优化器会自行决定小表为驱动表。 思考：上述SQL还存在严重影响性能的地方，可以先做思考，将在后续索引章节介绍。 2.6 避免使用LIKE和正则匹配 题：找出年龄是5或5的倍数的所有老师信息。 使用LIKE关键字查找如下： 12SELECT * FROM tb_teacherWHERE age LIKE \"%5\" OR age LIKE \"%0\"; 使用正则表达式如下： 12SELECT * FROM tb_teacherWHERE age REGEXP \".5|0\"; 两者效果等价，结果如下： id name age course_id 1 老张 55 1 4 老赵 45 4 5 老吴 35 5 6 老孙 45 6 7 老刘 40 7 可以看到LIKE和正则在进行内容过滤检索时很灵活，也很方便，但是 这两者查询都是基于全表的查询，大数据量时查询效率很低，所以给出一下两种解决办法。 当使用MyISAM引擎时，建议使用全文索引FULLTEXT，因为只有MyISAM引擎是支撑该特殊索引方式的，所以可以在创建表的时候使用FULLTEXT(字段)来定义，如下： 1234567CREATE table `tb_student`( id int primary key auto_increment, name varchar(20), age int, description text, FULLTEXT(description))ENGINE = Myisam CHARACTER SET = utf8; Mysql会创建索引表维护该索引列，查询时使用如下方式： 1SELECT * FROM tb_student WHERE MATCH(description) AGAINST('关键字'); 当业务中不得不实现LIKE模糊匹配时，最好在匹配字符中不以占位符开始LIKE '关键字%'，这样可以使查询字段的索引生效，相反则会索引失败进行全表查询；如果不得不以占位符开始，那么可以使用覆盖索引来提高效率。","link":"/2020/02/12/2%20MySQL/MySQL%E4%BC%98%E5%8C%96%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%80%EF%BC%89/"},{"title":"MySQL优化系列（二）","text":"MySQL优化系列（二） 3 数据库级优化3.1 索引数据结构&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;索引是DQL优化的核心，可以显著提高查询的效率，但值得注意的是凡事有利则有弊，数据检索性能的提高是依赖数据库内部维护的检索表，它是一个B+Tree的数据结构，本身就会占用空间且当对表做出DML操作时，需要同步维护该索引，所以DML的效率则会下降，效果可以类比线性表存储结构。建议对频繁进行数据查询的表创建索引，并选择合适的索引类型，同时以满足业务需求为目的创建，否则会适得其反。 3.1.1 B-Tree&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B-Tree是数据库中使用最为常见的数据结构，由平衡二叉查找树演化而来，现在多使用的B+Tree结构由B-Tree演化而来，所以有必要先介绍B-Tree。 B-Tree中的B是Balance而非Binary；”B-Tree”和”B+Tree”会被有些人称为”B减树“和”B加树“，这样称呼看似合人情但不合理，因为B-Tree创建之时不可能已经意识到B+Tree的存在，也就不存在名称对立，所以个人认为，“-”只是一个连接符，没有任何含义，应该称为“B树”和“B加树”。 B-Tree并不是二叉树，而是多叉树。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先需要理解，为什么需要B-Tree，这就需要从非关系型数据库的存储来讲，我们在开发过程中通常将MVC分层中数据库DML操作称为“持久化”过程，其实是因为数据库中的数据都是保存在磁盘中的。系统从磁盘读取数据并保存到内存是以“磁盘块（block）”为基础单位，位于同一磁盘块的数据会被一次性加载进来，而不是按需加载。那么，B-Tree的意义就是如何以最优的方式找到数据所在的磁盘块加载需要的数据，减少io损耗。 众所周知，磁盘IO是系统的瓶颈之一，且无法大幅优化提升效率，那么减少IO次数就很关键。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;定义一组普通映射[key, data]，key为索引的键（数据不同则键值不同），data为索引对应的一行表数据。B-Tree数据结构定义如下： 所有叶子节点都在同一层，且没有指向其他节点的指针。 每个非末端节点包含n个关键字或引用，保存每个节点保存更多的信息。 满足平衡二叉树的定义，即节点的索引值大于左子树索引值，小于右子树索引值，但可以有多个子节点。 索引值必包含对应的映射数据信息。 题：以查找索引值51为例，经过的步骤如下： 根节点磁盘块读取并存入内存中，48 &lt; 51，所以指向指针P2的子节点。（1次磁盘IO，1次内存加载） P3指向的磁盘块读取并存入内存，找到目标51并返回对应的行数据。（1次磁盘IO，1次内存加载） 题：以查找索引值15为例，经过的步骤如下： 根节点磁盘块读取并存入内存中，13 &lt; 15 &lt; 48，所以指向指针P2的子节点。（1次磁盘IO，1次内存加载） P2指向的磁盘块读取并存入内存，15 &lt; 20，所以指向指针P1的子节点。（1次磁盘IO，1次内存加载） P1指向的磁盘块读取并存入内存，找到目标15返回对应行数据。（1次磁盘IO，1次内存加载） 经过上述步骤查找，最终经过3次磁盘IO和3次内存加载，定位到索引15并读取的对应的行信息返回。所以在当前索引中最优结果是1次磁盘IO和1次内存加载，即根节点位置定位。最差结果是3次磁盘IO和3次内存加载，相比于平衡二叉树或红黑树等数据结构，由于节点保存的信息较多，所以树的高度偏扁平，这样减少了磁盘IO，从而提高了查找效率。 3.1.2 B+Tree&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B-Tree虽然已经很适合作为非关系型数据库的索引模型，但它还存在一些弊端：索引根节点是固定的，没必要每次查找都经过1次磁盘IO读取和加载；由于磁盘块容量有限，而节点保存数据会占用节点很大的空间，所以一个磁盘块能够保存的索引个数会减少，导致树高度可能增加；范围查找变得效率不高。所以在此基础之上，B+Tree做到了如下优化: 根节点常驻内存，减少1次IO损耗； 所有非叶子节点只保存索引值或引用，不保存映射数据，扩增节点保存关键信息的能力； 有两个全局指针，一个指向根节点，另一个指向索引值最小的节点（最左叶子节点）。 所有叶子节点保存索引值和映射数据，且叶子节点由单链表关联，指针指向相邻的右兄弟节点，结合第3条信息，这就提供了查找的多样性，可以选择从根节点开始检索树，也可以从顺序链表头开始进行查询，因为有链表的存在，支持范围查询且效率比B-Tree更好。 题：以查找索引值41为例，经过的步骤如下（与B-Tree一致）： 从内存加载根节点，找到 13 &lt; 41 &lt; 48，指向指针P2的节点。 读取磁盘块，找到 20 &lt; 41 &lt; 42，指向指针P2的几点。 读取磁盘块，找到目标并返回对应数据。 题：以查找索引值13为例，过程如下： 内存加载根节点，找到目标13（此时从右指针查找）。 加载磁盘块，13 &lt; 20，指向P1指针节点。 加载磁盘块，找到目标并返回。 注：为什么非叶子节点定位到目标值要从右侧指针查找，查找了大量网络资源都没有找到合适的答案，最终从维基百科和《高性能Mysql》一书中推测而来。欢迎各位交换意见。 3.1.3 扩展：B*Tree&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B*Tree是B+Tree的优化，进一步要求，在非根非叶子节点各层中的节点之间互相产生指向相邻右兄弟 节点的指针。 3.1.4 聚簇索引、非聚簇索引和辅助索引&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B+Tree在数据库中的实现方式即为聚簇索引，其含义是指在一张索引表中，既存在索引值同时存在映射数据；在Innodb引擎中使用聚簇索引，在MyISAM引擎中就使用非聚簇索引，其含义是指，在B+Tree结构中叶子节点索引值对应的并不是数据而是数据在数据页中的地址，通过地址就可以直接在内存中获得数据，数据页是管理磁盘块的最小单位；辅助索引是指，在索引表Index1中查找关键字key1时，需要先从索引表Index2中查找关键字key2对应的关键字key1。 3.2 索引的创建与修改&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在我们已经创建的四张表中，都设置了整型自增主键id，Mysql已经自动帮我们添加了主键索引（MySQL 5.7+版本），也就是说当我们使用id查询时是通过主键索引进行查询。索引按照创建方式可以分为：一般索引、唯一索引、复合索引、主键索引和全文索引。其中主键索引属于唯一索引范畴，全文索引已经在2.6章节中进行了介绍，是MyISAM引擎支持的文本类索引，在此不再深入讨论。 3.2.1 一般索引 创建tb_course表时对课程名name添加索引 12345678#创建课程表，包括id，名称，是否必修课，学分CREATE table `tb_course`( id int primary key auto_increment, name varchar(20), is_required boolean, credit int, INDEX course_name_index(name))ENGINE = InnoDB CHARACTER SET = utf8; 执行命令SHOW INDEX FROM tb_course \\G查看当前表的所有索引： 1234567891011121314151617181920212223242526272829mysql&gt; show index from tb_course \\G*************************** 1. row *************************** Table: tb_course Non_unique: 0 Key_name: PRIMARY Seq_in_index: 1 Column_name: id Collation: A Cardinality: 0 Sub_part: NULL Packed: NULL Null: Index_type: BTREE Comment:Index_comment:*************************** 2. row *************************** Table: tb_course Non_unique: 1 Key_name: course_name_index Seq_in_index: 1 Column_name: name Collation: A Cardinality: 0 Sub_part: NULL Packed: NULL Null: YES Index_type: BTREE Comment:Index_comment: 可以看到row1是系统自动添加的主键索引，row2是我们创建的自定义索引。 通过 CREATE 和 ALTER关键字创建索引 123CREATE INDEX course_name_index ON `tb_course`(name);ALTER TABLE `tb_course` ADD INDEX course_name_index(name); 删除索引 1DROP INDEX course_name_index ON `tb_course`; 3.2.2 唯一索引&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;唯一索引与一般索引的不同之处在于，它所持有的字段必须是唯一的，如果是唯一复合索引，那么复合字段必须是唯一的，其他用法一直，只需要将INDEX替换为UNIQE INDEX。 创建tb_course表时对课程名name添加唯一索引 12345678#创建课程表，包括id，名称，是否必修课，学分CREATE table `tb_course`( id int primary key auto_increment, name varchar(20), is_required boolean, credit int, UNIQUE INDEX course_name_index(name))ENGINE = InnoDB CHARACTER SET = utf8; 通过 CREATE 和 ALTER关键字创建唯一索引 123CREATE UNIQUE INDEX course_name_index ON `tb_course`(name);ALTER TABLE `tb_course` ADD UNIQUE INDEX course_name_index(name); 3.2.3 复合索引&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;复合索引语法与一般索引一致，只是持有多个字段。 创建tb_course表时对课程名、是否必修和学分添加复合索引 12345678#创建课程表，包括id，名称，是否必修课，学分CREATE table `tb_course`( id int primary key auto_increment, name varchar(20), is_required boolean, credit int, INDEX union_index(name, is_required, credit))ENGINE = InnoDB CHARACTER SET = utf8; 通过 CREATE 和 ALTER关键字创建复合索引 123CREATE INDEX union_index ON `tb_course`(name, is_required, credit);ALTER TABLE `tb_course` ADD INDEX union_index(name, is_required, credit); 通过命令SHOW INDEX FROM tb_course \\G查看结果 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152mysql&gt; show index from tb_course \\G*************************** 1. row *************************** Table: tb_course Non_unique: 0 Key_name: PRIMARY Seq_in_index: 1 Column_name: id Collation: A Cardinality: 0 Sub_part: NULL Packed: NULL Null: Index_type: BTREE Comment:Index_comment:*************************** 2. row *************************** Table: tb_course Non_unique: 1 Key_name: union_index Seq_in_index: 1 Column_name: name Collation: A Cardinality: 0 Sub_part: NULL Packed: NULL Null: YES Index_type: BTREE*************************** 3. row *************************** Table: tb_course Non_unique: 1 Key_name: union_index Seq_in_index: 2 Column_name: is_required Collation: A Cardinality: 0 Sub_part: NULL Packed: NULL Null: YES Index_type: BTREE*************************** 4. row *************************** Table: tb_course Non_unique: 1 Key_name: union_index Seq_in_index: 3 Column_name: credit Collation: A Cardinality: 0 Sub_part: NULL Packed: NULL Null: YES Index_type: BTREE 可以看到row 2.3.4信息中，索引名称相同，都是union_index，并且索引序列Seq_in_index分别是1,2,3，也就是说我们复合索引查找是有序列限制的（最左原则，后续章节介绍）。","link":"/2020/02/22/2%20MySQL/MySQL%E4%BC%98%E5%8C%96%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%8C%EF%BC%89/"},{"title":"MySQL优化系列（五）（缓存篇）","text":"MySQL优化系列（五）（缓存篇） 4 缓存&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;说明：由于是MySQL优化系列，本篇缓存旨在引导向，并不深入介绍，后续将开篇深入介绍，同样的，作为Java端持久化框架Mybatis、Hibernate和 JPA 也适用此说明，由于JPA基于Hibernate轻量化封装，所以框架缓存策略选择JPA介绍即可。 4.1 为什么使用缓存&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;缓存，是以提升数据响应为目的，以合理的缓存策略为条件，以数据中间件的形式为手段的一种技术。在计算机纵向世界，软硬件都有各自的缓存，并且缓存都是以提升现有框架约束的性能为价值，且成为衡量系统性能的重要指标之一。再此我们仅讨论软件领域以内存为驱动模型的缓存。 缓存的用途，使用缓存可以减少某条频繁进行数据处理链路上的性能损耗，通常是读多写少的场景，缓存是一种空间换时间的解决方案，通常在内存中进行。 缓存的位置，众所周知，在计算机科学发展历史长河中，困难的问题往往都是通过增加第三方中间件来解决的，而缓存的位置随着业务覆盖的不同而不同。 比如，在web前端页面，经常会缓存页面渲染或数据处理常用的信息，如多次浏览同一商品的信息，这一功能现代浏览器基本都支持。 比如，还是web前端，用户通过点击页面按钮多次进行查询，通过http访问后端控制器接口就可以添加相应的缓存，而无需经过服务层乃至持久层查询。 再比如，数据频繁的读场景，通常是在业务层和持久层中添加缓存，以减少数据库访问带来的IO性能损耗和数据连接开销。 缓存的分类，应用开发层面的数据结构如Map，持久层数据中间件如EhCache、Memcache、Redis等，数据库自带的缓存特性如Mysql和Oracle等。 缓存的策略，由于不同系统的数据访问模式不同，同一种缓存策略很难在不同的数据访问模式下取得满意的性能，通常使用如下几种策略： 基于访问的时间：按各缓存项被访问时间来组织缓存队列，决定替换对象。如 FIFO，LRU； 基于访问频率：用缓存项的被访问频率来组织缓存。如 LFU、LRU2； 访问时间与频率兼顾：兼顾访问时间和频率，使数据在变化时缓存策略仍有较好性能。多数此类算法具有一个可调或自适应参数，通过该参数的调节使缓存策略在基于访问时间与频率间取得一个平衡，如 FBR； 基于访问模式：某些应用有较明确的数据访问特点，进而产生与其相适应的缓存策略。 FIFO，First In Last Out，即先进先出，如果一个数据是最先进入的，那么可以认为在将来它被访问的可能性很小，当缓存空间不足，它最先被释放。 LRU：Least Recently Use，即最近最久未使用算法，选择最近最久未使用的数据予以淘汰，在缓存空间内，最近一直没有被使用的数据会被释放，由新访问数据代替 LFU：Least Frequently Used，即最近最少使用算法，如果一个数据在最近一段时间很少（频率）被访问到，那么可以认为在将来它被访问的可能性也很小。因此，当空间满时，最小频率访问的数据最先被释放。 LRU2：Least Recently Used 2，即LRU的改进版本，每当一次缓存记录的使用，会把它放到栈的顶端。当栈满了的时候，再把栈底的对象给换成新进来的对象。 FBR：Frequency-based replacement， 需要可调参数平衡访问时间和频率。 数据一致性，当分布式缓存中读写并发执行，有可能导致同一数据先读后写，那么缓存中保存的将是旧数据；先操作数据库，再清除缓存，如果缓存删除失败，就会出现数据不一致问题，解决方案如下： 前者可以采取将读写纳入同一节点的缓存按照同步处理，或者采取数据写入前后一并删除相关缓存。 缓存删除失败，可以将删除失败的key存入队列中并重复删除直到成功为止。 缓存的指标，命中率=命中次数/访问次数，命中率是缓存最重要的指标，它直接决定了缓存设计的合理性，若查询一个缓存，十次查询九次能得到正确结果，那么命中率就是90%。而直接影响缓存命中率的因素包含：缓存容量、内存空间和缓存策略。 4.2 MySQL缓存&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从前面介绍已经可以了解到，Mysql在执行时需要经过四层逻辑分层（客户端接口 -&gt; 服务 -&gt; 引擎 -&gt; 持久层），并在服务层和插件引擎层还需要经过解析、优化、执行过程，并最终在持久层完成IO操作，这一些列耗时耗力的操作在QPS峰值时是噩梦般的存在，所以Mysql自身也支持缓存策略，在MyIsam中使用缓存策略，在InnoDB中使用缓冲池策略。 12345678910111213141516171819202122# 查询是否支持查询缓存mysql&gt; show variables like 'have_query_cache';+------------------+-------+| Variable_name | Value |+------------------+-------+| have_query_cache | YES |+------------------+-------+# 查询当前缓存状态mysql&gt; show status like '%qcache%';+-------------------------+-------+| Variable_name | Value |+-------------------------+-------+| Qcache_free_blocks | 0 | 缓存空闲的内存块| Qcache_free_memory | 0 | 在query_cache_size设置的缓存中的空闲的内存| Qcache_hits | 0 | 缓存的命中次数| Qcache_inserts | 0 | 查询缓存区此前总共缓存过多少条查询命令的结果| Qcache_lowmem_prunes | 0 | 查询缓存区已满而从其中溢出和删除的查询结果的个数| Qcache_not_cached | 0 | | Qcache_queries_in_cache | 0 | 缓存查询次数| Qcache_total_blocks | 0 | 缓存总的内存块+-------------------------+-------+ 通过修改mysql配置文件来配置缓存 1234[mysqld]query_cache_type=1 #0不使用，1使用，2适时使用query_cache_size=10485760 #10M，单位字节query_cache_limit=1048576 #1M, 单个查询允许使用的最大缓存 4.3 Memcache和Redis&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;缓存界两位翘楚，都是基于内存的存储机制，亦可称为内存数据库。两者都是高性能的分布式缓存，在缓存层面都可以很好的满足业务要求。 4.3.1 Memcache&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Memcache是一套分布式的高速缓存系统，对于大型的需要频繁访问数据库的网站访问速度提升效果十分显著，以提升网站的访问速度。通过在内存里维护一个统一的巨大的hash表，它能够用来存储各种格式的数据，包括图像、视频、文件以及数据库检索的结果等。简单来说就是将数据调用到内存中，然后从内存中读取，从而提高读取速度。Memcached是以守护线程方式运行于一个或多个服务器中，随时接收客户端的连接和操作。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于完全依赖内存，使Memcache的容量根据部署节点的内存而定，在32位系统中容量最大限定2G。由于在内存中维护一个Hash表结构来缓存数据，所以它的数据保存形式遵从K-V简单结构，key默认最大不能超过128个字 节，value默认大小是1M，不过可以针对每条数据设定过期策略。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Memcache操作也相对简单，针对Hash表常用的操作如：set设置、get读取、replace替换、delete删除和flush刷新等。目前只支持文本类型的存取，所以在面向对象中可以将需要存储的对象经过序列化处理并保存，读取之后可以通过反序列化还原。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Memcache采用LRU过期策略，不支持持久化，所以当内存断电时会发生数据丢失，且无备份功能。 123456789101112131415161718192021222324252627282930/* Java 调用Memcache */public class MemcachedJava { public static void main(String[] args) { try{ // 连接本地的 Memcached 服务 MemcachedClient mcc = new MemcachedClient(new InetSocketAddress(\"127.0.0.1\", 11211)); // 存储数据 Future future = mcc.set(\"key1\", 900, \"This is Value1\"); // 查看存储状态 System.out.println(\"set status:\" + future.get()); // 缓存读取 System.out.println(\"value in cache : \" + mcc.get(\"key1\")); // 新增数据 mcc.add(\"key2\", 900, \"This is Value2\"); // 数据替换 mcc.replace(\"key2\", 900, \"is\"); // 数据后向追加 mcc.append(\"key2\", 900, \" Value2\"); // 数据前向追缴 mcc.prepend(\"key2\", 900, \"This \"); // 删除数据 mcc.delete(\"key1\"); // 关闭连接 mcc.shutdown(); }catch(Exception ex){ System.out.println( ex.getMessage() ); } }} 4.3.2 Redis&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;相比于Memcache，Redis具有更多的功能扩展性，可以满足更复杂的场景，并且支持String、Hash、Set、List和sorted set类型，还支持数据的持久化、虚拟内存等。它可以用作数据库、缓存和消息中间件。redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Redis支持主从同步，数据可以从主服务器向任意数量的从服务器上同步，从服务器可以是关联其他从服务器的主服务器。由于完全实现了发布/订阅机制，使得从数据库在任何地方同步树时，可订阅一个频道并接收主服务器完整的消息发布记录。同步对读取操作的可扩展性和数据冗余很有帮助 发布/订阅机制：是一种消息通信模式,发布者(pub)发送消息到特定的频道( channel)，订阅者(sub)通过观察频道( channe)接收消息。在分布式环境中，Redis分为客户端和服务端，消息的发布和订阅都是客户端行为，服务端提供channel，如果没有订阅消息，客户端会进入订阅监听状态，一旦接收到订阅消息就会进行消息同步。 类似于，有一档天气预报的节目，节目主持人（channel）实时获得气象局（pub）发布的气象信息，并把消息告诉正在观看节目的观众（订阅者），观众会一直盯着节目看直到收到气象预报并更新大脑关于今天的气象信息。 Redis的虚拟内存技术VM提升了数据保存的界限，其实际原理是当数据量已经达到存储边界，会对数据进行冷热隔离，将热数据继续保存在内存中，而冷数据通过压缩手段保存到磁盘上，压缩后的数据仅为原数据的1/10，虽然冷数据的查询效率不及热数据，但考虑到本身查询频率低，并不会影响整体性能。 12345678910111213141516171819202122232425262728293031323334353637public class RedisJava { public static void main(String[] args) { //连接本地的 Redis 服务 Jedis jedis = new Jedis(\"localhost\"); //查看服务是否运行 System.out.println(\"服务正在运行: \"+jedis.ping()); // redis 字符串 jedis.set(\"key1\", \"This is key1\"); System.out.println(jedis.get(\"key1\")); // redis list jedis.lpush(\"list\", \"val1\"); jedis.lpush(\"list\", \"val2\"); jedis.lpush(\"list\", \"val3\"); jedis.llen(\"list\"); // 长度 List&lt;String&gt; list = jedis.lrange(\"list\", 0 ,2); // 排序 SortingParams sortingParams = new SortingParams(); sortingParams.alpha(); sortingParams.limit(0, 3); jedis.sort(list, sortingParams) // keys Set&lt;String&gt; keys = jedis.keys(\"*\"); Iterator&lt;String&gt; it=keys.iterator() ; // 删除 if (jedis.exists(\"key1\")) jedis.del(\"key1\"); // 过期策略 jedis.persist(\"key1\"); jedis.ttl(\"key1\"); }} 4.3.3 Memcache和Redis的区别以及性能比较&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;实际上，两者既存在竞争又存在互补，由于实现细节的不同触手会伸向对方触及不到的业务场景。 内存空间：MemCached可以修改最大内存，但终究首先于内存。Redis增加了VM的特性，突破了物理内存的限制，实现冷热分离。 操作：MemCached数据结构单一，仅用来缓存数据，面向对象保存需要用到序列化和反序列化手段；Redis支持更加丰富的数据类型，也可以在服务器端直接对数据进行丰富的操作,这样可以减少网络IO次数和数据体积。 可靠性：MemCache不支持数据持久化，断电或重启后数据消失。Redis支持数据持久化和数据恢复，允许单点故障，但是同时也会付出性能的代价。 应用场景：Memcache动态系统中减轻数据库负载，提升性能，适合多读少写标准缓存场景。 Redis适用于对读写效率要求都很高，数据处理业务复杂和对安全性要求较高的系统。 性能：性能上都很出色，具体到细节，由于Redis只使用单核，而Memcached可以使用多核，所以平均每一个核上Redis在存储小数据时比 Memcached性能更高。而在100k以上的数据中，Memcached性能要高于Redis。 4.4 JPA缓存策略&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;JPA是Java持久层接口，是官方发布的ORM标准，Hibernate是对JPA的全量实现框架支持全自动处理，移植性更好；Mybatis仅部分遵循JPA规则实现半自动处理，灵活性更高。我们再此不展开讨论两者孰优孰劣，仅从缓存层面来看框架级对持久层缓存的支持情况。 4.4.1 Spring-JPA-Data(Hibernate)缓存策略&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Spring-JPA-Data是Spring框架对实现JPA接口的ORM框架的轻量级封装，默认使用Hibernate。Hibernate缓存包括两大类：一级缓存和二级缓存。一级缓存又被称为“Session的缓存”。Session缓存是内置的，不能被卸载，是事务范围的缓存，session级别缓存数据不共享；二级缓存又称为“SessionFactory的缓存”，由于SessionFactory对象的生命周期和应用程序的整个过程对应，因此Hibernate二级缓存是进程范围或者集群范围的缓存，是可以被不同Session所共享的，默认使用EhCache也可显示的替换为Memcache或其他缓存产品。 一级缓存默认开启，仅存在与session周期内，缓存时间短，范围小，效果不明显。 二级缓存默认不开启，需手动配置开启，是热拔插设计，不影响整体性能，可以显著提高效率，同样的将占用更多的内存空间。 4.4.2 Mybatis缓存策略&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mybatis同样采用一二级缓存策略，情况类似与Hibernate的使用。一级缓存称为SqlSession级缓存，与Hibernate的Session缓存一致，二级缓存属于SqlSessionFactory级别，类似于SessionFactory，不同的是，Mybatis二级缓存支持在单一对象映射中使用，比如针对特定的Mapper进行缓存，这就对选择业务场景使用更有帮助。 4.5 缓存击穿、穿透、雪崩、污染 缓存穿透：缓存和数据库中都没有真实数据，此时大量访问该无效数据造成系统压力增大，如果攻击者使用缓存穿透持续攻击，将造成持久层崩溃。 解决方案 对无效数据访问后可在缓存创建值为null的返回，并设定较短的过期时间，可以有效避免攻击。 业务层添加有效性校验，拦截较容易识别的风险。 缓存击穿：当缓存数据过期，此时QPS达到峰值并对该数据进行大规模并发访问，造成数据库瞬间访问量骤增导致奔溃。击穿的特点是少量数据过期，之后对这些数据高并发访问。 解决方案 甄别热点数据，采用合适的过期策略如LRU或LFU。 在高QPS来临前，设置Redis冷热数据隔离，假如冷数据发生高并发访问，也可以保证从缓存冷数据读取。 业务层创建互斥锁，当缓存数据不存在时，可以保证同一类请求只有一个可以访问数据库，其他请求阻塞，访问成功后刷新缓存，使其他请求通过缓存访问。 缓存雪崩：缓存数据大量过期，此时业务查询增大，同样导致数据库压力骤增容易发生奔溃。雪崩特点是大量数据过期，之后被大量访问，区别于缓存击穿。 解决方案 设置合理的缓存过期策略和过期时间，可以自定义一个过期时间范围，并将缓存单数据过期时间以哈希方式分散到不同时间范围。 可以设置热点数据永久不过期。 缓存污染：系统将不常用的数据从内存移到缓存，造成常用数据的失效，降低了缓存的利用率。缓存容量是弥足珍贵的，容量过大反而容易影响查询效率，所以在有效的空间内保证热点数据很重要。 解决策略 设置合理的过期策略，FIFO、LRU、LFU等。 业务层识别，避免大而全的数据添加进缓存中。","link":"/2020/03/10/2%20MySQL/MySQL%E4%BC%98%E5%8C%96%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%94%EF%BC%89/"},{"title":"MySQL优化系列（三）","text":"MySQL优化系列（三） 3.2 EXPLAIN 执行计划&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;执行计划的查看是进行SQL调优的重要步骤，也是收集可调优选项的信息集中地，Mysql中通过关键EXPLAIN来查看SELECT的查询效率。我们已经知道在逻辑分层中，服务层存在SQL优化器，它可以对我们的SQL进行优化并最终在引擎中执行，EXLAIN可以模拟SQL优化器执行结果。 3.2.1 使用和介绍1EXPLAIN SELECT * FROM `tb_course`\\G 该语句将模拟优化器执行，并将执行信息打印在控制台，如下： 1234567891011121314mysql&gt; EXPLAIN SELECT * FROM tb_course \\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_course partitions: NULL type: indexpossible_keys: NULL key: union_index key_len: 70 ref: NULL rows: 14 filtered: 100.00 Extra: Using index [id]： SELECT执行语句的编号，是一组整型数值。 ① 当Explain某一复杂语句时，可能包含多条子查询，那么查询顺序按照编号由大到小执行； ② 当查询编号一致时，按照从上到下的顺序执行。 例1： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455 mysql&gt; EXPLAIN SELECT * FROM tb_student WHERE id IN (SELECT student_id FROM tb_student_course WHERE course_id=(SELECT id FROM tb_course WHERE name='JAVA') ) \\G *************************** 1. row *************************** id: 1 select_type: PRIMARY table: &lt;subquery2&gt; partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: NULL filtered: 100.00 Extra: NULL*************************** 2. row *************************** id: 1 select_type: PRIMARY table: tb_student partitions: NULL type: eq_refpossible_keys: PRIMARY key: PRIMARY key_len: 4 ref: &lt;subquery2&gt;.student_id rows: 1 filtered: 100.00 Extra: NULL*************************** 3. row *************************** id: 2 select_type: MATERIALIZED table: tb_student_course partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 1 filtered: 100.00 Extra: Using where*************************** 4. row *************************** id: 3 select_type: SUBQUERY table: tb_course partitions: NULL type: refpossible_keys: union_index key: union_index key_len: 63 ref: const rows: 5 filtered: 100.00 Extra: Using index4 rows in set, 1 warning (0.04 sec) 该用例中查询序列按照3 -&gt; 2 -&gt; 1 -&gt; 1执行，首先查询SELECT id FROM tb_course WHERE name='JAVA'接着将子查询结果作为条件执行SELECT student_id FROM tb_student_course WHERE course_id=subquery，SQL优化器将最后主查询SELECT * FROM tb_student WHERE id IN (subquery) 与上一步子查询进行联接，先查询student_id，再查询最终结果。 例2： 1234567891011121314151617181920212223242526272829#查询选修了Java的学生课程关联表mysql&gt; EXPLAIN SELECT * FROM tb_student_course WHERE course_id IN (SELECT id FROM tb_course WHERE name LIKE 'JAVA%') \\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_student_course partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 1 filtered: 100.00 Extra: NULL*************************** 2. row *************************** id: 1 select_type: SIMPLE table: tb_course partitions: NULL type: eq_refpossible_keys: PRIMARY,union_index key: PRIMARY key_len: 4 ref: optdemo.tb_student_course.course_id rows: 1 filtered: 35.71 Extra: Using where 该例子中应当属于子查询嵌套类型且id顺序不一致，只是SQL优化器进行了优化，将嵌套查询优化为联接，所以编号一致，执行顺序从上到下。 [select_type]：表示查询类型：Simple简单查询，Primary外侧主查询，Subquery内测子查询，Derived驱动查询表示当前语句位于FROM后，Materialized被物化的子查询。 [table]：当前查询所在的目标表。 [partitions]：查询目标是分区表的位置，如果查询目标在其他分区的表中将显示出来。在早先Mysql版本中，需要使用Exlain Extends才会显示该选项。 [type]：访问类型，用于判断当前查询优化类别，是单表优化的重要指标，type效率指标由优到差依次为： system &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; all *system : * 查询结果只有一条数据，且扫描表中数据也只有一条，WHERE后使用主键索引查询。 *const : * 查询结果只有一条数据，且扫面表中有多条数据，WHERE后使用主键索引查询，system是const的特殊情况。 *eq_ref : * 查询结果为多条数据，使用唯一索引或主键索引查询。 *ref : * 查询结果为多条数据，使用普通索引或复合索引查询。 *range : * 查询结果为多条数据，使用索引查询且是范围索引，如使用&gt;、&lt;、BETWEEN、AND、OR、IN等 *index : * 查询结果为多条数据，使用索引查询，但对索引表进行全表扫描。 *all : * 查询结果为多条数据，未使用索引，对全表数据扫描。 一般来说，在实际业务中，system和const情况几乎不可能达到，而index和all的效率过低是主要的被优化目标，而期望的优化目标则为eq_ref、ref，range情况特殊它属于范围内的索引表扫描，实际优化应考虑索引扫描范围。 [possible_keys]： SQL优化器执行前预估的索引类型。 [key]： 实际执行时用到的索引类型。 [key_len]： 实际使用的索引长度，用于确认用到的索引。 [ref]： 联接查询时的联接条件 [rows]： 返回结果集时，所查询的总行数。 [filtered]： 表示返回数据在server层过滤后，剩下多少满足查询的记录数量的百分比，同partition，在5.7版本以前需要使用explain extended查询。 [Extra]： 表示查询时额外的说明信息，该字段与type一样，同样时需要优化时特别关注的，常遇到的类型包括：using filesort、using temporary、using index、using where、distinct等，特别需要关注前两项，它们出现是性能损耗过大的表现。 using filesort : ** 常见于使用order by 排序，由于索引查询后排序并未使用索引字段或索引字段失效，导致排序时将在内存中的“排序空间”进行，排序空间通过sort_buffer_size**设置，会额外产生空间和时间的浪费。 *using temporary : * 常见于group by或order by操作，当查询结果进行分组时会在内存中额外使用“临时表”空间存储和聚合，额外产生空间和时间的浪费，多见于order by或group by字段并非多表查询结果字段。 *using index : * 表示当前查询使用了索引查询，无需回表查询，性能提升。 using where : ** 返回的记录并不是所有的都满足查询条件，需要在server层进行过滤，即回表查询，属于“后置过滤”，在版本5.6之后出现了using index condition**，它的含义是先在索引表中查询复合索引过滤条件的数据，再将这些数据使用where其他条件进行过滤，与using where相比，将索引过滤提前到索引表内，所以where条件优先设置索引过滤（SQL优化器是否会自动优化还待确认）。 *distinct : * 表示数据查询后使用了distinct筛查，将查询结果进行二次全部扫描，排除重复项。 3.2.2 最左前缀原则&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在复合索引的使用过程中经常提到”最左前缀原则“，它的意思是说，当创建一个复合索引tb_index(col1, col2, col3)，使用索引必须按照严格的定义顺序，比如SELECT * FROM tb WHERE col=x and col2=xx and col3=xxx，这样联合索引才能达到最高效率。如果执行如下破坏顺序索引的例子将不能完全发挥索引功能或丧失索引功能：SELECT * FROM tb WHERE col1=x and col3=xxx（跳过中间索引col2，只有col1生效，col3无法使用索引），SELECT * FROM tb WHERE col2=xx and col3=xxx(跳过col1，索引全部失效)。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了探讨最左前缀原则的原理，重新回到索引的数据结构，我们已经讨论过无论是MyISAM还是InnoDB引擎都是用B+Tree数据结构进行索引表的保存，不妨以科学疑问的形式提出两种数据结构的假设： 一个列代表一个b+tree结构，多个列则分表对应多个b+tree，比如上例提到的复合索引，col1、col2、col3在物理模型上分别对应三个b+tree文件，当执行SELECT * FROM tb WHERE col1=x and col2=xx and col3=xxx时，首先从col1索引表中定位，定位到的data域保存指向col2索引的指针从而在col2对应的索引表中定位，col3同理，最终在col3的叶子节点data域中找到目标结果。 在物理模型上只有一个索引文件，即一个b+tree保存联合索引，并在key域中以严格的定义顺序保存多列索引字段按次序依次定位，从而最终获得目标结果。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;先确认第二条为正确结果，那么我们来讨论第一条为什么不是，为什么数据库大多都采用B+Tree作为结构模型而非红黑树之类，主要原因在于索引本身就是一张容量较大的表结构，使用内存是性能损耗和系统稳定性都受点影响的一件事，那么只能考虑作为文件保存在物理磁盘上，而物理磁盘的IO效率又过低容易影响系统整体的吞吐量，所以衡量索引最重要的标准就是在查询中减少磁盘IO的次数，显然第一条假设严重违背了这样的标准，因为如果复合索引的多列分别对应多张索引的话，那么磁盘上的文件也会一一对应，当我们执行覆盖索引的SQL语句时，就代表经过多次磁盘IO，效率很可能反而降低了。 3.2.3 避免索引失效&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在索引列上做任何操作(计算、 函数、自动\\手动类型转换)，会导致索引失效而转向全表扫描，究其原因，主要是破坏（不满足）B+Tree索引表的查询条件，以下常见场景需要注意（由于MYSQL优化器在不同版本之间表现不同，所以结果可能有出入）： 模糊查询LIKE后接匹配符 % 或 _时，只能出现在最末位置。 当前tb_course表中数据，以及索引情况如下，索引只有主键索引和联合索引union_index(name, is_required, credit)。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869mysql&gt; SELECT * FROM tb_course ;+----+-----------------+-------------+--------+| id | name | is_required | credit |+----+-----------------+-------------+--------+| 1 | C | 1 | 4 || 5 | C# | 0 | 2 || 11 | JAVA | 0 | 0 || 12 | JAVA | 1 | 1 || 13 | JAVA | 1 | 2 || 14 | JAVA | 1 | 3 || 2 | JAVA | 1 | 4 || 10 | Linux | 0 | 2 || 6 | Python | 0 | 3 || 8 | TCP/IP | 0 | 2 || 7 | 人工智能 | 1 | 4 || 4 | 操作系统 | 1 | 4 || 3 | 数据结构 | 1 | 3 || 9 | 计算机网络 | 1 | 3 |+----+-----------------+-------------+--------+mysql&gt; SHOW INDEX IN tb_course\\G*************************** 1. row *************************** Table: tb_course Non_unique: 0 Key_name: PRIMARY Seq_in_index: 1 Column_name: id Collation: A Cardinality: 14 Sub_part: NULL Packed: NULL Null: Index_type: BTREE*************************** 2. row *************************** Table: tb_course Non_unique: 1 Key_name: union_index Seq_in_index: 1 Column_name: name Collation: A Cardinality: 10 Sub_part: NULL Packed: NULL Null: YES Index_type: BTREE*************************** 3. row *************************** Table: tb_course Non_unique: 1 Key_name: union_index Seq_in_index: 2 Column_name: is_required Collation: A Cardinality: 11 Sub_part: NULL Packed: NULL Null: YES Index_type: BTREE*************************** 4. row *************************** Table: tb_course Non_unique: 1 Key_name: union_index Seq_in_index: 3 Column_name: credit Collation: A Cardinality: 14 Sub_part: NULL Packed: NULL Null: YES Index_type: BTREE 12345678910111213141516#MySQL 5.7mysql&gt; EXPLAIN SELECT * FROM tb_course WHERE name LIKE '%AVA%'\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_course partitions: NULL type: indexpossible_keys: NULL key: union_index key_len: 70 ref: NULL rows: 14 filtered: 11.11 Extra: Using where; Using index1 row in set, 1 warning (0.00 sec) 从结果可以看到，最终的执行计划显示使用了联合索引union_index，这是因为在当前测试使用的MYSQL版本为5.7，SQL优化器对%AVA%进行了优化处理使得可以进行索引查询，但在MYSQL5.6之前版本中将显示无法使用索引，执行计划显示使用全局查询，如下： 123456789101112131415#MySQL 5.6mysql&gt; EXPLAIN SELECT * FROM tb_course WHERE name LIKE '%AVA%'\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_course partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 10 filtered: 11.11 Extra: Using where 建议：尽量使用全文索引，如果必须使用模糊查询，建议匹配条件不以%或_开头。 2. OR的前后字段必须都为索引字段，否则索引失效 当OR前后字段只有一个是索引时，那么全部不使用索引，相反当所有字段是索引时才会使用索引。 123456789101112131415mysql&gt; EXPLAIN SELECT * FROM tb_teacher WHERE id=2 OR name='老张'\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_teacher partitions: NULL type: ALLpossible_keys: PRIMARY key: NULL key_len: NULL ref: NULL rows: 10 filtered: 19.00 Extra: Using where1 row in set, 1 warning (0.00 sec) id是主键索引，而name字段无索引，所以结果显示possible_keys: PRIMARY可能使用到主键索引，实际并未使用索引key: NULL。 3. 联合索引需要满足最左原则，否则索引部分失效或全部失效 见 3.2.2 4. 堤防隐式转换破坏索引查询 当字段为varchar类型索引时，如果使用整型类型当作条件查询，则会破坏索引规则，使失效。 1234567891011121314151617181920212223242526272829303132333435363738# 测试新增索引mysql&gt; ALTER TABLE tb_teacher ADD INDEX name_index(name);Query OK, 0 rows affected (0.07 sec)Records: 0 Duplicates: 0 Warnings: 0# 正常mysql&gt; EXPLAIN SELECT * FROM tb_teacher WHERE name='123' \\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_teacher partitions: NULL type: refpossible_keys: name_index key: name_index key_len: 63 ref: const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec)# 失效mysql&gt; EXPLAIN SELECT * FROM tb_teacher WHERE name=123 \\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_teacher partitions: NULL type: ALLpossible_keys: name_index key: NULL key_len: NULL ref: NULL rows: 11 filtered: 10.00 Extra: Using where1 row in set, 3 warnings (0.00 sec) 建议： 日常开发中在书写SQL时需要细心注意索引字段类型，此类问题一般比较隐蔽。 5. 不等表示( &lt;&gt;, !=) 和 空判断(is null, is not null) 使索引失效 因为不等和为空都不会进入索引表，所以即使针对索引列判断也无法生效，将进行全表扫描。 12345678910111213141516171819202122232425262728293031# 不等操作mysql&gt; EXPLAIN SELECT * FROM tb_teacher WHERE name &lt;&gt; '123' \\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_teacher partitions: NULL type: ALLpossible_keys: name_index key: NULL key_len: NULL ref: NULL rows: 11 filtered: 100.00 Extra: Using where # 为空判断 mysql&gt; EXPLAIN SELECT * FROM tb_teacher WHERE name is not null \\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_teacher partitions: NULL type: ALLpossible_keys: name_index key: NULL key_len: NULL ref: NULL rows: 11 filtered: 100.00 Extra: Using where 注意，possible_keys计划使用name_index索引，实际并未使用索引。 当针对整型类型进行不等操作时，被优化器优化处理： 12345678910111213141516171819202122232425262728293031# 主键索引mysql&gt; EXPLAIN SELECT * FROM tb_teacher WHERE id &lt;&gt; 10 \\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_teacher partitions: NULL type: rangepossible_keys: PRIMARY key: PRIMARY key_len: 4 ref: NULL rows: 10 filtered: 100.00 Extra: Using where # 联合索引mysql&gt; explain select * from tb_course where credit &lt;&gt; 0 \\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_course partitions: NULL type: indexpossible_keys: NULL key: union_index key_len: 70 ref: NULL rows: 14 filtered: 90.00 Extra: Using where; Using index 6. 计算、函数表达式使索引失效 当对索引字段进行函数或计算则有可能使其失效，之所以说有可能同样时因为不同版本中SQL优化器表现不同所致。 1234567891011121314mysql&gt; EXPLAIN SELECT * FROM tb_course WHERE credit*2 &gt; 2 \\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_course partitions: NULL type: indexpossible_keys: NULL key: union_index key_len: 70 ref: NULL rows: 14 filtered: 100.00 Extra: Using where; Using index 可以看到，此处使用了索引查询，也就表示在当前测试版本中优化器进行了优化操作，在MYSQL 5.6之前索引无效。 7. 当全表扫描速度更快时，索引失效 当优化器认为全表扫描速度优于索引查找时，使索引失效，这种场景往往牵扯到创建索引时涉及的块的读取成本问题。 3.2.4 单表优化实战&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了演示单表优化过程，以tb_course为例，首先删除已经存在的索引drop index union_index on tb_course;，再次申明，测试使用的MYSQL当前版本为5.8，不同版本SQL优化器执行计划不一定相同。 查询学分不为0的必修课的名称。 1234567891011121314mysql&gt; EXPLAIN SELECT name FROM tb_course WHERE is_required=1 AND credit &lt;&gt;0 ORDER BY credit DESC \\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_course partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 14 filtered: 9.00 Extra: Using where; Using filesort 当前type为ALL表示未使用索引，最差全表扫描查询。 是否可以将name, is_required, credit设置为联合索引提高效率？ 123456789101112131415161718mysql&gt; ALTER TABLE tb_course ADD INDEX test_index1(name,is_required,credit);Query OK, 0 rows affected (0.05 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; EXPLAIN SELECT name FROM tb_course WHERE is_required=1 AND credit &lt;&gt;0 ORDER BY credit DESC \\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_course partitions: NULL type: indexpossible_keys: NULL key: test_index1 key_len: 70 ref: NULL rows: 14 filtered: 9.00 Extra: Using where; Using index; Using filesort 在当前版本测试中显示type升级为index，Extra新增 Using index项，效率确实有少许提升（在MySQL5.6版本中可能显示任然无法使用索引或索引失效），但没有达到我们优化的目标级别(range以上)，此时我们仔细分析一下：在SQL执行顺序中（1.3章节），执行顺序应该为 FROM &gt; WHERE &gt; SELECT &gt; ORDER BY，所以当前创建的索引test_index1(name,is_required,credit)并没有按照执行顺序执行，换句话说，当前SQL使用的索引顺序是乱序的。 按照执行顺序创建联合索引是否可行？ 12345678910111213141516171819202122mysql&gt; drop index test_index1 ON tb_course;Query OK, 0 rows affected (0.03 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; ALTER TABLE tb_course ADD INDEX test_index2(is_required, credit, name);Query OK, 0 rows affected (0.04 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; EXPLAIN SELECT name FROM tb_course WHERE is_required=1 AND credit &lt;&gt;0 ORDER BY credit DESC \\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_course partitions: NULL type: rangepossible_keys: test_index2 key: test_index2 key_len: 7 ref: NULL rows: 10 filtered: 100.00 Extra: Using where; Using index 很明显可以看到type效率达到了range级别，并且减少了Using filesort这个特别消耗性能的操作。 3.3 ORDER BY排序原理与优化&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在Mysql版本中ORDER BY的排序算法经过了两代演进，最初使用双路排序算法：将排序字段和对应的行指针从磁盘中读出并在内存中进行排序，遍历该排序列表并从磁盘读取原表匹配返回查询结果。可以看到双路排序算法经过了两次磁盘IO，这在效率上很受影响，属于空间优于时间策略。对应的优化版本则是单路排序算法：将排序字段及其对应的所有查询项一次从磁盘读取获得并在内存中进行排序，排序后的结果即是输出结果，在该算法下将磁盘IO降到最低，对时间效率进行了提升，但同时需要注意多字段读出到内存中，如果数据量巨大则容易导致OOM，如果在系统内存范围内但同样数据量较大则容易产生回表操作，反而不如双路排序算法有优势。在内存中进行排序操作依赖Mysql在内存中创建的缓存区buffer大小，如果数据量超出buffer就会导致创建临时表或回表操作甚至发生OOM，Mysql默认buffer为1024字节，所以预估数据量大小很重要或者使用explain执行计划查看低效率风险，将buffer设置在预估范围边界。 12345678910111213141516mysql&gt; show variables like &quot;max_length_for_sort_data&quot;;+--------------------------+-------+| Variable_name | Value |+--------------------------+-------+| max_length_for_sort_data | 1024 |+--------------------------+-------+mysql&gt; set global max_length_for_sort_data=2048;Query OK, 0 rows affected (0.01 sec)mysql&gt; show variables like &quot;max_length_for_sort_data&quot;;+--------------------------+-------+| Variable_name | Value |+--------------------------+-------+| max_length_for_sort_data | 2048 |+--------------------------+-------+ 以此类比，GROUP BY也是类似问题，只不过优先排序后分组，所以优化策略相同。 3.4 慢查询日志&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在复杂业务场景下，Mysql经常会因为某个复杂查询SQL影响整体性能，甚至出现假死状态，主要原因是因为数据业务量大致使该查询语句耗费太多时间，Mysql提供慢查询日志支持在线网环境下定位具体慢查询语句，默认情况下慢查询处于关闭状态，可以使用指令slow_query_log查看（配置文件开启可以永久生效），通过指定日志文件（默认文件为slow-query-log-file）和设定慢查询时间阈值来截取SQL。 1234567891011121314151617181920212223242526272829303132333435# 查询慢查询开关是否开启mysql&gt; show variables like &quot;slow_query_log&quot; ;+----------------+-------+| Variable_name | Value |+----------------+-------+| slow_query_log | OFF |+----------------+-------+# 开启慢查询mysql&gt; set global slow_query_log=1;Query OK, 0 rows affected (0.02 sec)mysql&gt; show variables like &quot;slow_query_log&quot;;+----------------+-------+| Variable_name | Value |+----------------+-------+| slow_query_log | ON |+----------------+-------+# 查询时间阈值mysql&gt; show variables like 'long_query_time';+-----------------+-----------+| Variable_name | Value |+-----------------+-----------+| long_query_time | 10.000000 |+-----------------+-----------+# 设置时间阈值mysql&gt; set global long_query_time=5;mysql&gt; show variables like 'long_query_time';+-----------------+-----------+| Variable_name | Value |+-----------------+-----------+| long_query_time | 5.000000 |+-----------------+-----------+ 配置my.ini或my.cnf配置慢查询（需重启服务） 12345# 配置文件[mysqld]slow_query_log =1long_query_time=5slow_query_log_file=D:\\\\software\\\\work\\\\mysql-5.7.23-winx64\\\\slowquery.log 1234567891011121314151617181920mysql&gt; show variables like &quot;slow_query_log&quot;;+----------------+-------+| Variable_name | Value |+----------------+-------+| slow_query_log | ON |+----------------+-------+mysql&gt; show variables like &quot;long_query_time&quot;;+-----------------+----------+| Variable_name | Value |+-----------------+----------+| long_query_time | 5.000000 |+-----------------+----------+mysql&gt; show variables like &quot;slow_query_log_file&quot;;+---------------------+----------------------------------------------------+| Variable_name | Value |+---------------------+----------------------------------------------------+| slow_query_log_file | D:\\software\\work\\mysql-5.7.23-winx64\\slowquery.log |+---------------------+----------------------------------------------------+ 通过sleep函数设置睡眠时间来测试： 1mysql&gt; select sleep(6); 手动查看日志 1234567MySQL, Version: 5.7.23-log (MySQL Community Server (GPL)). started with:TCP Port: 3306, Named Pipe: (null)Time Id Command Argument# User@Host: root[root] @ localhost [::1] Id: 3# Query_time: 5.999740 Lock_time: 0.000000 Rows_sent: 1 Rows_examined: 0SET timestamp=1586104027;select sleep(6); 使用mysql提供的慢查询命令mysqldumpslow查看 123# 按照平均时长排序并输出前十项C:\\Users\\zhy&gt;mysqldumpslow.pl -s at -t 10 D:\\software\\work\\mysql-5.7.23-winx64\\slowquery.log# mysqldumpslow.pl需要安装perl环境才能使用 3.5 使用Profiling&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;除了使用Explain执行计划查看单个SQL的执行效率外，还可以通过Profiling来查看当前会话中多条SQL的执行性能，主要指标是CPU、BLOCK IO、CONTEXT SWITCH、MEMORY、SWAPS等。 CPU：显示CPU使用相关开销 BLOCK IO：阻塞IO相关开销 CONTEXT SWITCH：上下文切换相关开销 MEMORY：内存相关开销 SWAPS：交换之间的开销 默认情况下是关闭的，可以开启并显示最近执行的SQL 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292# 查看开关和历史记录数mysql&gt; show variables like 'profiling%';+------------------------+-------+| Variable_name | Value |+------------------------+-------+| profiling | OFF || profiling_history_size | 15 |+------------------------+-------+# 设置开关和记录数mysql&gt; set global profiling=1;mysql&gt; set global profiling_history_size=20;mysql&gt; show variables like 'profiling%';+------------------------+-------+| Variable_name | Value |+------------------------+-------+| profiling | ON || profiling_history_size | 20 |+------------------------+-------+# 查看最近的执行SQLmysql&gt; show profiles;+----------+------------+----------------------------------+| Query_ID | Duration | Query |+----------+------------+----------------------------------+| 1 | 0.00040300 | select @@version_comment limit 1 || 2 | 0.00438400 | show variables like 'profiling%' || 3 | 0.00018800 | show version() || 4 | 0.00028400 | select version() || 5 | 0.00044975 | show engines || 6 | 0.00034125 | SELECT DATABASE() || 7 | 0.00255175 | select * from tb_teacher |+----------+------------+----------------------------------+# 查看Query_ID为7的所有性能信息*************************** 1. row *************************** Status: starting Duration: 0.000211 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: NULL Source_file: NULL Source_line: NULL*************************** 2. row *************************** Status: checking permissions Duration: 0.000016 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: check_access Source_file: sql_authorization.cc Source_line: 810*************************** 3. row *************************** Status: Opening tables Duration: 0.001980 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: open_tables Source_file: sql_base.cc Source_line: 5685*************************** 4. row *************************** Status: init Duration: 0.000026 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: handle_query Source_file: sql_select.cc Source_line: 121*************************** 5. row *************************** Status: System lock Duration: 0.000012 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: mysql_lock_tables Source_file: lock.cc Source_line: 323*************************** 6. row *************************** Status: optimizing Duration: 0.000004 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: JOIN::optimize Source_file: sql_optimizer.cc Source_line: 151*************************** 7. row *************************** Status: statistics Duration: 0.000016 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: JOIN::optimize Source_file: sql_optimizer.cc Source_line: 367*************************** 8. row *************************** Status: preparing Duration: 0.000017 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: JOIN::optimize Source_file: sql_optimizer.cc Source_line: 475*************************** 9. row *************************** Status: executing Duration: 0.000003 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: JOIN::exec Source_file: sql_executor.cc Source_line: 119*************************** 10. row *************************** Status: Sending data Duration: 0.000078 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: JOIN::exec Source_file: sql_executor.cc Source_line: 195*************************** 11. row *************************** Status: end Duration: 0.000042 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: handle_query Source_file: sql_select.cc Source_line: 199*************************** 12. row *************************** Status: query end Duration: 0.000010 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: mysql_execute_command Source_file: sql_parse.cc Source_line: 4937*************************** 13. row *************************** Status: closing tables Duration: 0.000009 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: mysql_execute_command Source_file: sql_parse.cc Source_line: 4989*************************** 14. row *************************** Status: freeing items Duration: 0.000107 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: mysql_parse Source_file: sql_parse.cc Source_line: 5594*************************** 15. row *************************** Status: cleaning up Duration: 0.000022 CPU_user: 0.000000 CPU_system: 0.000000 Context_voluntary: NULLContext_involuntary: NULL Block_ops_in: NULL Block_ops_out: NULL Messages_sent: NULL Messages_received: NULL Page_faults_major: NULL Page_faults_minor: NULL Swaps: NULL Source_function: dispatch_command Source_file: sql_parse.cc Source_line: 1924 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107# 使用具体的性能分类，比如cpu和block iomysql&gt; show profile cpu,block io for query 7 \\G*************************** 1. row *************************** Status: starting Duration: 0.000211 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL*************************** 2. row *************************** Status: checking permissions Duration: 0.000016 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL*************************** 3. row *************************** Status: Opening tables Duration: 0.001980 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL*************************** 4. row *************************** Status: init Duration: 0.000026 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL*************************** 5. row *************************** Status: System lock Duration: 0.000012 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL*************************** 6. row *************************** Status: optimizing Duration: 0.000004 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL*************************** 7. row *************************** Status: statistics Duration: 0.000016 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL*************************** 8. row *************************** Status: preparing Duration: 0.000017 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL*************************** 9. row *************************** Status: executing Duration: 0.000003 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL*************************** 10. row *************************** Status: Sending data Duration: 0.000078 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL*************************** 11. row *************************** Status: end Duration: 0.000042 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL*************************** 12. row *************************** Status: query end Duration: 0.000010 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL*************************** 13. row *************************** Status: closing tables Duration: 0.000009 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL*************************** 14. row *************************** Status: freeing items Duration: 0.000107 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL*************************** 15. row *************************** Status: cleaning up Duration: 0.000022 CPU_user: 0.000000 CPU_system: 0.000000 Block_ops_in: NULLBlock_ops_out: NULL 应重点关注以下几个情况： converting HEAP to MyISAM ：查询结果太大，内存转磁盘。 Creating tmp table :创建了临时表，性能损耗严重。 Copying to tmp table on disk：把内存中的临时表复制到磁盘，性能损耗严重 locked ：被加锁。","link":"/2020/02/24/2%20MySQL/MySQL%E4%BC%98%E5%8C%96%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89/"},{"title":"mysql-5.7.x zip版详细配置","text":"MySQL 5.7.x 解压版配置和服务安装 下载5.7.x zip版本MySQL并解压到本地 在文件根目录下新建my.ini文件，并编辑如下内容 1234567891011121314151617181920[mysqld]port = 3306basedir=A:\\dev\\mysql-5.7.23-winx64datadir=A:\\dev\\mysql-5.7.23-winx64\\data#如果不生效，则使用下列表达形式#basedir=A:\\\\dev\\\\mysql-5.7.23-winx64#datadir=A:\\\\dev\\\\mysql-5.7.23-winx64\\\\datamax_connections=200character-set-server=utf8default-storage-engine=INNODBsql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES[mysql]default-character-set=utf8 注意： 此时根目录下还不存在data文件，但需要提前配置好datadir 以管理员权限打开cmd，并进入根目录 123c:\\ &gt; cd A:\\dev\\mysql-5.7.23-winx64A:\\dev\\mysql-5.7.23-winx64 &gt; cd bin 先执行remove操作删除可能存在的服务，在安装新服务 123A:\\dev\\mysql-5.7.23-winx64\\bin &gt; mysqld -removeA:\\dev\\mysql-5.7.23-winx64\\bin &gt; mysqld -instal 初始化MySQL服务，并在根目录下创建data目录，并创建初始化用户root 1A:\\dev\\mysql-5.7.23-winx64\\bin &gt; mysqld --initialize-insecure --user=mysql 启动MySQL服务 1A:\\dev\\mysql-5.7.23-winx64\\bin &gt; net start mysql 为root用户创建登录密码 12A:\\dev\\mysql-5.7.23-winx64\\bin &gt; mysqladmin -u root -p password 新密码Enter password: 由于初始化密码为空，所以第二步骤可以直接enter 登录mysql成功！ 12345678910111213141516A:\\dev\\mysql-5.7.23-winx64\\bin &gt; mysql -u root -p Enter password:密码Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 9Server version: 5.7.23 MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.mysql&gt;show databases; 安装和破解Navicat Premium 12.x 在这里下载64位12.x版本Navicat Premium 在这里下载注册机 正常安装后使用注册机破解 毕竟不是光彩的事情，详情请戳这里","link":"/2018/10/08/2%20MySQL/mysql-5-7-x%20zip%E7%89%88%E8%AF%A6%E7%BB%86%E9%85%8D%E7%BD%AE/"},{"title":"MySQL优化系列（四）","text":"MySQL优化系列（四） 3.6 锁机制和执行引擎3.6.1 锁定义和分类&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在数据连接池提供数据库多线程操作时，资源的并发访问或操作就容易导致各种并发问题，例如脏读、幻读、不可重复读等。按照锁的控制粒度分类：行级锁（包括间隙锁和next-key）、表级锁、页锁； 按照锁的功能分类：共享锁（读锁）、排它锁（写锁）。由于锁的目的是保证在任务执行过程中避免并发问题，所以它通常由Mysql逻辑分层中的引擎层持有，对服务层隔离，同时由引擎层的特性决定了插件式使用方式。 行级锁：锁的持有对象是数据库表中的一行数据，是粒度最小的锁同时并发程度最高。 间隙锁（Gap-Lock）：是特殊的行级锁，物理空间不存在但逻辑上被锁定的行锁，具有行锁的特性。 next-key：是行锁和间隙锁的组合形式。 表级锁：锁的持有对象是数据库的表，粒度最大同时并发程度最低。 页锁：锁的持有对象是数据库中的磁盘管理最小单位——页，粒度和并发度介于行锁和表锁之间。 共享锁：又称为读锁，其他事务可读不可写。 排它锁：又称为写锁，其他事务不可读不可写。 粒度锁的性能比较： 行级锁： 锁的开销大，影响性能。 粒度最小，并发程度最高，锁冲突概率低，会出现死锁。 适合大量的并发更新同时又有并发查询的场景。 表级锁: 锁的开销小，性能影响低。 粒度最大，并发程度低，锁冲突概率大，不会出现死锁。 适合以查询为主，并发数小的场景。 页锁介于行级锁和表锁之间。 3.6.2 并发和事务&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;常见的关系型数据库事务并发问题包括：脏读、幻读、不可重复读。 脏读：当事务1查询某一行数据时，此时事务2进行了修改，那么事务1读取的数据并非当时查询需要返回的数据。 事务1 事务2 SELECT money FROM tb_account WHERE userId=1; UPDATE tb_account SET money = 1000WHERE userId=1;COMMIT; 查询成功（脏读） 不可重复读：当事务1在第一次查询后事务2进行了修改未提交，事务1再次进行查询发现数据与上一次不一致。 事务1 事务2 SELECT money FROM tb_account WHERE userId=1; UPDATE tb_account SET money = 1000WHERE userId=1; SELECT money FROM tb_account WHERE userId=1;(与上一次上旬结果不一致) ROLLBACK； 幻读：当事务1第一次进行范围查询后事务2新增/删除了一条数据，当事务1再次查询时范围总量与上一次不一致。 事务1 事务2 SELECT * FROM tb_account DELETE FROM tb_accountWHERE userId=1; SELECT * FROM tb_account ROLLBACK; 可以发现，幻读和不可重复读的区别在于前者是多数量查询且修改点是新增和删除，后者是单数据查询且修改点是更新操作。结合3.6.1节介绍，脏读可以通过对事务1查询添加行级共享锁从而避免其他事务写操作干扰，或者对事务2添加行级排他锁，使事务1读操作阻塞；不可重复读可以采取对事务2的写操作添加行级排他锁，使事务1的读取阻塞，在完成写操作后开放读取；幻读，通常采用MVCC + next-key方式保护并发执行。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;事务是数据库处理业务逻辑的基本单位，通常由一组SQL组成，执行状态分为成功和失败回滚。事务具有ACID（酸）特性： 原子性(Actomicity)：业务处理的最小单位，全部成功则事务执行成功，否则事务执行失败进行回滚。 一致性(Consistency)：事务操作中，期间的数据要保持一致状态，例如事务A由增删改查组成，当修改其中一条数据后，相关的比如查询操作也应该进行更改。事务执行结束，内部结构如B+Tree也应该是正确的。 隔离性(Isolation)：由于原子性的存在，不同的事务都应该是独立的个体且不能互相影响，这就需要隔离机制进行保护。 持久性(Durability)：事务执行结束后，对数据的更改应该是永久的。 在Mysql的InnoDB引擎中，所有的操作都被当作事务处理，如果存在并发行为就会产生上节所说的情况发生，而如何防止并发问题发生就依赖锁机制和隔离机制。 3.6.3 多版本并发控制&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;多版本并发控制MVCC（Multi-Version Concurrency Control）是数据库级避免并发的实现手段，主要包括：快照读Snapshot Read和当前读Current Read。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;快照读是指读取操作针对某一版本，普通的查询都为快照读，比如SELECT * FROM tb WHERE ?。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当前读是特殊的快照读，是指读取记录的最新版本，为了避免并发问题对读取结果加锁，以排它锁为主，主要使用在增删改场景中，举个例子，UPDATE tb SET name=xxx WHERE id=1，将WHERE=1读取出的结果进行加锁避免其他事务并发干扰，更新之后成功后释放锁同时更新版本。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;针对MVCC中的版本解释，在mysql中每行数据都有两个隐藏列：创建版本号和删除版本号，这是数据库自动添加的，针对不同CRUD操作进行版本号的更新。 SELECT：只能读取的创建版本号≤系统版本号的，删除版本号≥版本号。 INSERT：把当前事务版本号作为创建版本号。 DELETE：把当前事务版本号作为删除版本号。 UPDATE：将当前的事务版本号标记为旧数据的删除版本号，同时创建新的数据把事务版本号标记为创建版本号，事务提交后覆盖旧数据。 3.6.4 隔离机制&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mysql提供四种隔离级别，包括：Read Uncommitted、Read Committed、Repeatable Read、Serializable。隔离机制的存在使事务并发执行过程中保证数据对外的可见性，或者说针对隔离的场景不同（对并发问题的容忍程度）通常需要按业务实际情况选择，隔离级别越低则性能损耗越小。 Read Uncommitted（RU）：在该隔离级别，所有事务都可以看到其他未提交事务的执行结果，很少用于实际，因为性能不比其他级别好多少，允许脏读、不可重复读、幻读。 Read Committed（RC）：该隔离级别支持一个事务只能看见已经提交事务所做的改变，即未提交数据不可见，允许不可重复读和幻读，不允许脏读。 Repeatable Read（RR）：这是MySQL的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行，允许幻读，不允许脏读和不可重复读，InnoDB可以通过MVCC解决幻读问题。 Serializable：这是最高的隔离级别，它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题，不允许脏读、不可重复读、幻读。 隔离级别 脏读 不可重复读 幻读 Read Uncommitted √ √ √ Read Committed × √ √ Repeatable Read × × √ Serializable × × × 1234567891011121314151617# 查看当前隔离级别mysql&gt; show variables like '%isolation%';+-----------------------+-----------------+| Variable_name | Value |+-----------------------+-----------------+| transaction_isolation | REPEATABLE-READ || tx_isolation | REPEATABLE-READ |+-----------------------+-----------------+mysql&gt; set global transaction_isolation ='read-committed';mysql&gt; show variables like '%isolation%';+-----------------------+----------------+| Variable_name | Value |+-----------------------+----------------+| transaction_isolation | READ-COMMITTED |+-----------------------+----------------+ 3.6.5 MyISAM与InnoDB引擎及其锁和死锁&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MyISAM和InnoDB是Mysql最典型的两个插件式数据库引擎，两者最大的不同除了在事务支持上不一样（MyISAM不支持事务），以及B+Tree索引结构不同（非聚簇索引和聚簇索引），最主要的就是锁机制不同。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MyISAM在执行SELECT时，会自动给涉及的表加共享锁，在执行UPDATE、INSERT、DELETE前添加表排他锁，加锁过程不需要用户通过LOCK TABLE显示的干预。它支持在一个线程进行共享锁查询时，其他线程可以并发插入数据以减少线程对表的竞争，例如，在一个线程A对表添加共享锁之后执行SELECT查询，此时线程B可以同时将新数据从表的末尾插入，但这需要满足一个条件，就是当表中没有空闲块（由删除造成的一行数据缺失）时，才能触发此并发行为，否则并发插入是被禁止的。 12345678910111213141516# 查询MyISAM并发插入的状态mysql&gt; show variables like &quot;concurrent_insert&quot;;+-------------------+-------+| Variable_name | Value |+-------------------+-------+| concurrent_insert | AUTO |+-------------------+-------+# 查询表锁竞争mysql&gt; show status like 'table_lock%';+-----------------------+-------+| Variable_name | Value |+-----------------------+-------+| Table_locks_immediate | 111 || Table_locks_waited | 5 |+-----------------------+-------+ 可以通过查询表锁竞争来查看当前锁的阻塞情况，当 Table_locks_waited 过大时存在严重的竞争影响系统执行效率。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;InnoDB支持行级共享锁和排它锁，同时内部实现对表添加的意向锁（IS和IX），意向锁是内部实现的不需要用户手动干预，当InnoDB准备对一行数据添加共享锁时需要先获得该表的意向共享锁IS，同理添加排它锁需要获得意向排它锁IX。在执行UPDATE、INSERT、DELETE时会自动添加排它锁，对于SELECT，一般查询不会加锁。 1234567891011121314151617# 对SELECT手动添加共享锁mysql &gt; SELECT * FROM tb_course WHERE id=5 LOCK IN SHARE MODE;# 对SELECT手动添加排它锁mysql &gt; SELECT * FROM tb_course WHERE id=5 FOR UPDATE;# 查看innodb锁竞争情况mysql&gt; show status like 'innodb_row_lock%';+-------------------------------+-------+| Variable_name | Value |+-------------------------------+-------+| Innodb_row_lock_current_waits | 0 || Innodb_row_lock_time | 0 || Innodb_row_lock_time_avg | 0 || Innodb_row_lock_time_max | 0 || Innodb_row_lock_waits | 0 |+-------------------------------+-------+ 手动添加锁的场景一般时为了保证查询的数据在事务执行期内是最新的，排它锁更为严格因为它不运行其他线程添加锁且必须阻塞，而共享锁则有可能存在其他线程也对同一数据添加了共享锁而不能更新。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;死锁更容易在严格的隔离级别上以及InnoDB上发生，当事务试图以不同的顺序锁定资源时，就可能产生死锁，多个事务同时锁定同一个资源时也可能会产生死锁。所以InnoDB本身也有检测死锁的机制，可以检测到死锁的循环依赖并立即返回一个错误，当死锁发生后，只有部分或完全回滚其中一个事务，才能打破死锁，所以InnoDB采取将持有最少行级排他锁的事务进行回滚作为解除死锁的策略，或者设置死锁超时策略。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MyISAM由于在执行DML或DQL时，一次性添加锁，所以不会发生死锁现象，而InnoDB想要规避死锁发生通常建议在单个事务中一次性获得需要的锁避免加锁顺序不一致导致死锁发生，或者当事务中可能既存在共享锁又存在排它锁，那么直接使用排它锁，以一点效率来换取无死锁的发生；如果修改多个表，则先按顺讯依次添加锁；降低隔离级别。 3.7 Mysql主从复制与Binlog&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主从复制是指数据可以从一个Master节点数据库异步复制到其他服务器Slaver节点，是实现数据库集群的策略，支持横向扩展，提供”一主多从“或”主主复制“，使用主从复制具有以下优势： 实现数据的热备份，提高数据安全性和容灾率。 满足高性能要求，在QPS达到瓶颈时可以通过多个Slavers策略来实现负载均衡。 对于数据采集、建模等内部业务需求可以采取在Slaver进行而不影响生产性能。 数据下发，远程系统创建本地副本而无需建立远程连接访问。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mysql主从复制原理，如上图所示，在开启主从复制功能后，主节点Master会启动一个IO线程，将主服务器发生的改变通过线程写入转储文件bin-log中，从节点Slaver会启用两个线程——IO线程和SQL线程，IO线程负责从Master的binlog中读取数据并写入本地延迟文件Relay Log中，SQL线程从延迟文件中读取数据并写入本地数据库，这样就完成了一个主从复制周期。 Master的IO线程基于数据库改变（DDL和DML）来启动写binlog操作，而Slaver通过定期探查binlog改变状态来启动IO线程读取数据； 确保Master的数据库版本小于等于Slaver数据库版本； 保证Master和Slaver的时间同步。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;更进一步来说，Master向binlog写操作是基于DDL和DML的发生，SELECT除外，也就说是基于事件保存，所以Slaver读取的也是事件信息并在本地通过SQL线程进行回放重演来完成数据库副本的同步，当master发生高吞吐量导致数据量激增超过Slaver线程的处理能力，或者Slaver发生排它锁导致SQL线程同步数据长时间阻塞就会发生相对较高的延迟反应，为了解决这一状况可以采用如下措施： 实现读写分离，按照常规场景的经验DQL的频率远远高于DML（当然也有少量特殊的相反场景），所以将Master设为只写而从库只读，再通过横向扩展从库或者提高从库硬件性能并配合负载均衡，从而保护Master，并将性能均摊。 再应用层面通过在服务层和持久层添加缓存，缓解数据底层压力。 Master配置 1234567891011121314151617# 编辑配置文件my.ini或my.cnf[mysqld]# 保证log-bin权限开放log-bin=/var/mysql/log/master-log-bin//日志文件格式binlog_format=&quot;mixed&quot; # 集群标识server-id=1# 要进行复制的数据库名binlog-do-db=copied_table# 重启服务 12345678# 保证skip_networking关闭状态，否则Slaver无法完成基于网络的通信mysql&gt; show variables like '%skip_networking%';+-----------------+-------+| Variable_name | Value |+-----------------+-------+| skip_networking | OFF |+-----------------+-------+ Slaver配置 123456789101112131415# 同样修改配置文件my.ini或my.cnf[mysqld]server-id=2# Master IPmaster-host=192.168.XXX.XXX # 用于创建连接的验证信息master-user=user master-password=123456 master-port=3306master-connect-retry=60# 要进行复制的数据库名binlog-do-db=copied_table 1234# 开启Slavermysql&gt; slave start;# 查看状态mysql&gt; show slave status; 建议专门创建一个用于进行主从复制的数据库账号。 Binlog的另一重要作用 binlog除了是主从复制的重要文件，同时它还是数据恢复的重要文件，当数据库由于各种原因导致数据丢失时，通过mysqlbinlog工具从binlog文件中恢复最近的正常状态。 以上介绍的属于Mysql Replication主从同步方案，由于所有的IO线程（Master的IO Thread和Slaver的IO Thread）都是单向执行的，也就是说Slaver假如发生数据库的改变是无法同步到Master的，所以既要求有客观的管理规则来约束又要求在架构上设计严谨，可想而知如果应用在电商平台或者金融平台等对数据一致性和传输安全性有更高要求的地方则并不那么稳妥，PXC主从同步是为满足这一场景而设计，它以牺牲同步效率为代价，提高了数据传输安全性并通过任意节点的数据更高都会对整个集群节点进行广播更新，从而达到数据一致的要求。 3.8 分库分表策略和MyCAT&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;单机使用的关系型数据库，在遇到业务膨胀数据量递增之后，很容易出现性能瓶颈，例如单表或单库数据量达到千万级甚至更高或者磁盘容量超过100G后，IO损耗、内存占用和CPU占用率都会飙升；当业务请求增多会导致数据访问连接数受限长时间阻塞甚至出现锁竞争和死锁的发生。而此时就需要考虑对现有数据库进行拆分，目的为了减少数据库承载的压力，恢复甚至提升数据库性能。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;做数据拆分时从两个维度进行——垂直拆分和水平拆分，而针对业务场景调整拆分的目标包括数据库和表。拆分（Sharding）就是分区分片的意思。 垂直分库，严格来讲不应该算作优化范畴，更确切的应该归为项目架构的改变。随着业务发展，原先设计的关系型数据库架构中的某些子部分迅速膨胀，已经达到了可以单独成库的条件，那么根据业务耦合性，将这类表及附表抽出单独成库并部署在新环境中，达到垂直分库的目的。这类问题一般出现在关系型数据库设计失误或者初创公司为了考虑成本前期内聚式设计，随着业务攀升不得不面临数据分库，而垂直分库本身的思想与业务层面的大系统按照”微服务”思想拆分子系统类似，要求每个子库完成特定范围的数据持久化能力。 水平分库，当垂直分库之后，数据垂直拆分达到了最小原子，数据库中任然存在大量数据，单表性能仍然受限此时需要进行水平分库，参考3.7（主从复制），创建数据库副本形成集群，通过数据库负载均衡将压力均摊，减小单库压力。 垂直分表，配合业务逻辑将表按照字段进行拆分为原子粒度，减小单表内过多的列从而减少量，提高性能，同时达到了解耦的作用。 水平分表，当单表经过垂直拆分后，任然存在大量数据则针对数据行选择进行库内分表或分库分表策略，库内分表只解决单一数据库内表的拆分，对于单机压力的缓解有限；分库分表实际上就是分布式数据库的原理，在集群中相同的数据表按照一定策略保存不同的数据，但这对数据一致性和分布式事务提出了更高的要求。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在进行水平分表时，需要考虑采用怎样的策略来分配数据保存到不同表中，这种策略一般使用特定字段或哈希存储：（1）特定字段是指，可以按照自增整型主键id划分每张表保存的数据量，例如上图所示，每张子表保存id在百万以内的数据，如果超出则继续水平拆分；这样的特定字段根据业务选取，也可按照时间等信息划分。（2）哈希存储是指，按照某一特性的字段求哈希值的方式将数据均匀分布到多个表中，当然需要考虑极端的哈希冲突问题。特定字段可以是一个也可以是一组，这同样需要与业务逻辑相接壤，比如水平拆分5张用户表User，选择userId和userName作为特征字段求哈希值并保存到对应的一张表中，这样就可以保证该用户的多条数据落入同一个库中，减少简单查询的复杂度。 优缺点比较 垂直拆分 优点 以业务逻辑将数据库解耦，业务清晰； 在高并发场景下，可以有效解决IO、数据连接数的性能问题。 缺点 提高了分布式事务的处理难度； 当进行连接操作JOIN时，只能通过代码层进行高层次的聚合，提升了开发难度。 水平拆分 优点 降低了单个数据库的负载能力，保证了系统稳定，提高并发性。 非设计架构层面的解耦，不会带来应用层更大的修改。 缺点 分片的事务处理难度高，数据同步存在延时可能，一致性无法保障 维护难度大，特别是多次改造后。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在垂直拆分和水平拆分中都提到了分布式事务问题，可想而知在单库单表内的事务总是易于控制的，而分布式下需要考虑到节点之间的通信问题，这就导致分布式事务会损耗相对更多的时间，同时导致在共享资源访问时更容易产生并发冲突和死锁问题，并且在数据不断膨胀拆分越来越频繁时会，这种问题会更加严重并不断放大。分布式事务一般采用事务补偿机制，即当事务发生错误通过采用数据对账检查、日志比对、与标准数据对齐等方式进行检查，不同于事务回滚机制，补偿机制属于事后补救措施，会影响数据一致性。一般采用XA协议方式进行分布式事务控制。限于篇幅，只做简单介绍：XA是一个分布式事务协议，包含两个概念：事务管理器（Transaction Manager,TM）和本地资源管理器（Resources Manager, RM）。事务管理器属于协调者，负责各个本地资源的提交和回滚，资源管理器属于参与者，进行实际的执行操作。它通常有二阶段提交和三阶段提交两种，二阶段是指准备阶段和提交阶段： 准备阶段，协调者给每个参与者发送Prepare消息，每个参与者要么直接返回失败，要么在本地执行事务，执行完毕不进行提交并等待，如果参与者节点的事务操作实际执行成功，则它返回一个”同意”消息；如果参与者节点的事务操作实际执行失败，则它返回一个”中止”消息。 提交阶段，如果协调者收到了参与者的失败消息或者超时，直接给每个参与者发送回滚消息；否则，发送提交消息，参与者根据协调者的指令执行提交或者回滚操作，释放所有事务处理过程中使用的锁资源。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当前行业内使用的分库分表数据中间件有很多，通常分为两类：Client和Proxy。client方案的的优点在于不用部署，运维成本很低，但是各个服务之间都需要耦合client依赖；proxy方案优点是对个各个服务都是透明的，但是得需要专门去部署运维。目前使用范围最广的是MyCat，它的前身是阿里维护的Cobar，目前已开源并且社区活跃，用户量也很多，属于proxy层方案。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MyCat发展到目前的版本，已经不是一个单纯的MySQL代理了，它的后端可以支持MySQL、SQL Server、Oracle、DB2、PostgreSQL等主流数据库，也支持MongoDB这种新型NoSQL方式的存储，未来还会支持更多类型的存储。而在最终用户看来，无论是那种存储方式，在MyCat里，都是一个传统的数据库表，支持标准的SQL语句进行数据的操作，这样一来，对前端业务系统来说，可以大幅降低开发难度，提升开发速度。 MyCAT的目标是：低成本的将现有的单机数据库和应用平滑迁移到“云”端，解决数据存储和业务规模迅速增长情况下的数据瓶颈问题。从这一点介绍上来看，能满足数据库数据大量存储，提高了查询性能。","link":"/2020/03/03/2%20MySQL/MySQL%E4%BC%98%E5%8C%96%E7%B3%BB%E5%88%97%EF%BC%88%E5%9B%9B%EF%BC%89/"},{"title":"Java源码——HashMap","text":"HashMap &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;HashMap应该是Java世界应用最多的数据结构对象，HashMap从Java1.2开始存在，在Java8经过一次大改，Java5又出现了同步包java.util.concurrent其中就包含高性能的同步HashMap对象ConcurrentHashMap，这其中精彩世界以下揭晓。 0. 哈希表&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;哈希表，又称散列表，是计算机从业者最熟悉不过的数据结构，由于其在数据索引时只拥有O(n)时间复杂度而广泛应用。生活中我们也经常遇到类似情景，比如查字典过程、电话本黄页等等，通过索引关键字获得对应映射的值。 上图为哈希表的简易示意图，哈希表的基本组成包括： 桶，是保存一组数据的空间，通常是一个链表（单向链表）或者一棵树（红黑树）； 槽位，是通过关键字定位数据桶的索引位置，通过关键字定位到槽位我们一般称为落槽。 哈希冲突，理想状态下每个槽位对应的桶只保存一个数据，那么就形成了槽位和数据一一对应关系（例如图中0号槽位和k号槽位），然而由于哈希表中的槽位通常是有限的，当数据量大于槽位个数时势必会产生一个槽位对应的桶中保存了多个数据，这就称为哈希冲突。解决哈希冲突的方式一般采用单向链表或者红黑树。 哈希函数，将数据元素的关键字key作为自变量，通过一定的函数关系计算出的值，即为该元素的存储地址，这个函数称为哈希函数。哈希函数需要解决的两大问题分别是均匀落槽和哈希冲突。前者在极端情况下会产生所有数据落槽在同一个桶中，产生空间资源浪费和性能低效。 加载因子（load factor），理想状态下，如果空间无限大则槽位可以保证只映射一个数据那么它的时间复杂度是完美的O(1)，如果空间有限，为了保证数据查询时间，可以将数据均分到每个桶中形成较短链路的链表，设计合理的哈希表可以平衡空间与时间。空间和时间永远是计算机领域的哲学问题，加载因子就是平衡这一问题的系数(0~1的浮点数)，研究表明，加载因子系数值越大空间损耗越小时间损耗越大，相反如果系数值越小空间损耗越大时间损耗越小，在Java中默认为0.75。 再哈希（rehash），当一个哈希表向槽位更多的哈希表迁移时，由于槽位个数发生变化，这就要求原哈希表中的每个元素重新计算哈希值并落槽到新哈希表中，这一过程称为rehash，是哈希表的重要性能指标之一。 概念虽然较多，但都比较容易理解且重要 举个例子： 当我们需要从手机通讯录中查找某一个人的联系电话，这个过程可以拟化为一次哈希表的查找。 首先，有张三、李三、王三三个人的联系方式需要存入通讯录，我们假定他们的名字就是关键字，通过哈希函数计算关键字获得落槽位置分别是拼音首字母z、l和w，此时这个哈希表槽位数共有三个，每个槽位对应的桶分别是张三、李三、王三的电话号码，当我们需要拨打张三电话时直接找到z对应的电话拨出即可。 然而，又来了三个人需要保存电话，他们分别是张四、李四、王四，通过哈希函数计算名字关键字得到槽位还是z、l和w，这就是哈希冲突，三个槽位对应的桶分别保存了两个电话号码，当我们拨打张四电话时，需要先找到z，在从z的列表中找到张四电话并拨出。 最后，又来了两个人赵三和刘三，很明显目前的通讯录无法保存他们的电话，这时我们需要对通讯录扩容并将现在的六个人电话迁移过去，但是迁移时需要满足一个约束，原来的六个人必须都重新通过哈希函数计算落槽位置，这就是rehash。 1. JAVA 7的实现成员变量 HashMap的实现实际是一个桶数组或者称为链表数组，数组下标索引即槽位，同时定义了默认的加载因子、初始容量、最大范围和扩容条件。 12345678910111213141516171819202122232425262728public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable{ static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // HashMap的默认初始大小16 static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; // HashMap允许的最大范围2^30 static final float DEFAULT_LOAD_FACTOR = 0.75f; // 默认加载因子 /** * The table, resized as necessary. Length MUST Always be a power of two. */ // table就是哈希表，它实际是一个桶数组或者链表数组，官方注释表明哈希表容量必须满足2的幂(向上取值) transient Entry&lt;K,V&gt;[] table = (Entry&lt;K,V&gt;[]) EMPTY_TABLE; transient int size; // 当前HashMap以保存数据的个数 // HashMap扩容阈值，threshlod = capacity * loadFactor,如果size &gt;= threshlod则需要扩容，而并非size == table.length。 int threshold; /** * A randomizing value associated with this instance that is applied to * hash code of keys to make hash collisions harder to find. If 0 then * alternative hashing is disabled. */ // 哈希种子，哈希冲突更难被发现 transient int hashSeed = 0;} Java7采用单向链表来解决哈希冲突，以下是定义的私有内部类链表节点 1234567static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final K key; // 关键字 V value; // 值 Entry&lt;K,V&gt; next; // 后继 int hash; // 哈希值 ...} put方法 put方法是研究HashMap的精髓，掌握了该方法也就基本掌握HashMap的原理。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101public V put(K key, V value) { if (table == EMPTY_TABLE) { inflateTable(threshold); // 空表时扩容默认大小的哈希表 } if (key == null) return putForNullKey(value); // HashMap允许存在一个关键字为null的值（区别于HashTable） int hash = hash(key); // 哈希函数，求关键字对应的哈希值，如下方法 int i = indexFor(hash, table.length); // 计算落槽位置，如下方法 // 遍历槽位对应的链表，从第一个节点开始 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) { Object k; // 当哈希值相同并且key相同时则在链表中找到了对应数据，更改value值 if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++; // 如果链表没有找到数据则说明是新数据，将该数据添加到链表头部，如果容量达到阈值，则需要扩容，对旧表做迁移，需要rehash，如下方法。 addEntry(hash, key, value, i); return null;}// 关键的哈希函数，由复杂的移位和异或等位运算组成，为了保证发生哈希冲突时，数据可以均匀落槽// 当加载因子默认是0.75时，哈希冲突最多为8个。final int hash(Object k) { int h = hashSeed; if (0 != h &amp;&amp; k instanceof String) { return sun.misc.Hashing.stringHash32((String) k); } h ^= k.hashCode(); // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);}// 1.参数h是哈希函数计算的哈希值，length是哈希表的长度，将哈希值与length-1求与操作可以保证索引在length范围内// 2.在哈希表中每个元素的操作都离不开落槽，通常采用模运算，但模运算在计算机原理中是比较耗时的方式，所以HashMap要求容量必须是2次幂，这样依据二进制特点，length-1永远是多个1的排列，这样与运算就是与h的对位与，效率更高。static int indexFor(int h, int length) { // assert Integer.bitCount(length) == 1 : \"length must be a non-zero power of 2\"; return h &amp; (length-1);}void addEntry(int hash, K key, V value, int bucketIndex) { // 如果当前size达到阈值则扩容并数据迁移 if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) { resize(2 * table.length); // 扩容和rehash hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length); } createEntry(hash, key, value, bucketIndex);}// 新数据添加到链表头部void createEntry(int hash, K key, V value, int bucketIndex) { Entry&lt;K,V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); size++;}// 扩容新表容量是旧表的2倍（保证2次幂）void resize(int newCapacity) { Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return; } Entry[] newTable = new Entry[newCapacity]; transfer(newTable, initHashSeedAsNeeded(newCapacity)); // 数据迁移 table = newTable; threshold = (int) Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1);}// 数据迁移，需要对旧表每个数据重新计算哈希值void transfer(Entry[] newTable, boolean rehash) { int newCapacity = newTable.length; for (Entry&lt;K,V&gt; e : table) { while(null != e) { Entry&lt;K,V&gt; next = e.next; if (rehash) { e.hash = null == e.key ? 0 : hash(e.key); } int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; } }} 哈希函数 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;哈希函数对String类型做了特殊处理，实际上是调用了String类中的hash32()方法，计算获得String类型的32位哈希值，native底层使用murmur3_32()哈希方法而非String的hashCode()方法，原因是因为hashCode()方法将字符串的每个字符Unicode值累加获得，如果使用String作为key的话这样的处理方式会增加哈西冲突的概率。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;同时可以看到哈希函数入参是对象，计算哈希码过程中也使用到了对象的hashCode方法，也就是说，哈希函数依赖对象自身的哈希码，并以此额外计算了新的哈希码供HashMap使用，这样的好处是可以防止低质量的哈希函数。 123456789101112131415161718192021222324252627282930313233343536373839// HashMap 哈希函数final int hash(Object k) { int h = hashSeed; // 字符串哈希值 if (0 != h &amp;&amp; k instanceof String) { return sun.misc.Hashing.stringHash32((String) k); } h ^= k.hashCode(); // 对象自身哈希码 h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);}// String的hashCode方法和hash32方法// 每个字符Unicode码累加public int hashCode() { int h = hash; if (h == 0 &amp;&amp; value.length &gt; 0) { char val[] = value; for (int i = 0; i &lt; value.length; i++) { h = 31 * h + val[i]; } hash = h; } return h;}// 底层使用murmur3_32方法int hash32() { int h = hash32; if (0 == h) { // harmless data race on hash32 here. h = sun.misc.Hashing.murmur3_32(HASHING_SEED, value, 0, value.length); // ensure result is not zero to avoid recalcing h = (0 != h) ? h : 1; hash32 = h; } return h;} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在不同的编码规范中都要求软件工程师重写equals方法时要重写hashCode方法，从HashMap中就可以看出，当新元素落槽之后，链表从头结点开始比较的第一个条件就是对象的哈希值是否相等，且这一条件是短路的，即使key的值相同也会认为是不同的对象。 123456789101112131415public V put(K key, V value) { ... // 遍历槽位对应的链表，从第一个节点开始 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) { Object k; // 首先比较哈希码，且是短路条件，不满足立刻退出，即使key相等 if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } ...} 所以建议采用《Effective Java》中的第8、9条条例，小心的设计自己对象的equals和hashCode方法。假如，两个方法设计糟糕将会导致该对象作为key保存在HashMap中时，当对该对象执行get或put等操作，即使客观上两者是相同的对象，也会被区别对待，最终导致HashMap产生内存泄漏问题。 落槽方法indexFor &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在解决哈希冲突问题上，通常采用取余操作将数据均分到不同桶中，然而在计算机中取余操作相比于其他逻辑运算比较消耗性能，首先需要对数值转换十进制，然后不断的执行被除数和除数的除法操作最终获得余数，这在大范围使用上是不合适的，HashMap采用对2次幂减1并求异或是在二进制层面的位运算，效率很高更适合作为解决哈希冲突的手段，唯一约束就是要求容量必须保证是2次幂 123static int indexFor(int h, int length) { return h &amp; (length-1);} 假设，整型h=101，length=8，s=length-1=7 h对应二进制是 0000 0000 0000 0000 0000 0000 0110 0101‬ length对应二进制是 0000 0000 0000 0000 0000 0000 0001 0000 length-1对应的二进制是 0000 0000 0000 0000 0000 0000 0000 1111 按照异或的特性会将h的高位全部截断只保留与h对应的位数 0000 0000 0000 0000 0000 0000 0000 0101‬ 相当于求0101^1111，最终的哈希值是0101，即5","link":"/2020/05/07/4%20%E6%BA%90%E7%A0%81/Java%E6%BA%90%E7%A0%81%E2%80%94%E2%80%94HashMap/"},{"title":"Java源码——LinkedList","text":"LinkedList (version: JDK 11) 总结： LinkedList底层采用双向列表数据结构。 载JDK11中包含两个指针成员变量first和last分别指向链表的首位和末位，JDK7采用单指针header，末位对象指向header，也就是说header的后继next指向链表第一个节点，header的前驱previous指向链表最后一个节点。LinkedList实现前驱指针目的是为了实现单向队列和双端队列数据结构。源码以jdk11为例。 成员变量 1234transient int size;transient LinkedList.Node&lt;E&gt; first;transient LinkedList.Node&lt;E&gt; last;private static final long serialVersionUID = 876323262645176354L; 节点对象 123456789101112private static class Node&lt;E&gt; { E item; // 元素 LinkedList.Node&lt;E&gt; next; // 后继 LinkedList.Node&lt;E&gt; prev; // 前驱 Node(LinkedList.Node&lt;E&gt; prev, E element, LinkedList.Node&lt;E&gt; next) { this.item = element; this.next = next; this.prev = prev; }} add方法 123456789101112131415161718public boolean add(E e) { this.linkLast(e); return true;}void linkLast(E e) { LinkedList.Node&lt;E&gt; l = this.last; LinkedList.Node&lt;E&gt; newNode = new LinkedList.Node(l, e, (LinkedList.Node)null); this.last = newNode; if (l == null) { this.first = newNode; // 链表为空则指向对一个节点 } else { l.next = newNode; // 链表不为空则指向末尾节点 } ++this.size; ++this.modCount;} get方法 12345678910111213141516171819202122232425262728293031323334353637public E get(int index) { this.checkElementIndex(index); // 索引越界检查 return this.node(index).item; // 返回节点的元素}private void checkElementIndex(int index) { if (!this.isElementIndex(index)) { throw new IndexOutOfBoundsException(this.outOfBoundsMsg(index)); }}private boolean isElementIndex(int index) { return index &gt;= 0 &amp;&amp; index &lt; this.size;}// 因为指针遍历较慢，所以对链表从中间截取判断index落位上半位还是下半位LinkedList.Node&lt;E&gt; node(int index) { LinkedList.Node x; int i; if (index &lt; this.size &gt;&gt; 1) { // 上半位，从first节点查找 x = this.first; for(i = 0; i &lt; index; ++i) { x = x.next; } return x; } else { //下半位，从last节点查找 x = this.last; for(i = this.size - 1; i &gt; index; --i) { x = x.prev; } return x; }} 浅克隆 123456789101112131415161718192021// 浅克隆并重置modCount=0public Object clone() { LinkedList&lt;E&gt; clone = this.superClone(); clone.first = clone.last = null; clone.size = 0; clone.modCount = 0; for(LinkedList.Node x = this.first; x != null; x = x.next) { clone.add(x.item); } return clone;}private LinkedList&lt;E&gt; superClone() { try { return (LinkedList)super.clone(); } catch (CloneNotSupportedException var2) { throw new InternalError(var2); }} modCount 与ArrayList一样，当链表进行了增删改操作，modCount会自增，这是为了保证在并发访问和操作时保证链表数据统一，否则抛出ConcurrentModificationException异常，抛出异常的目的是快速反馈给调用者以发现并发问题并采取有效的措施。 12345final void checkForComodification() { if (LinkedList.this.modCount != this.expectedModCount) { throw new ConcurrentModificationException(); }} 队列实现方法 队列属于基础数据结构，采用FIFO先进先出原则，这就要求具有首位获得数据和末位添加数据的能力。 12345678910111213141516171819202122232425262728293031323334353637383940// 返回头结点元素，不删除，为空则返回nullpublic E peek() { LinkedList.Node&lt;E&gt; f = this.first; return f == null ? null : f.item;}// 返回头结点元素，不删除，为空则抛出异常public E element() { return this.getFirst();}public E getFirst() { LinkedList.Node&lt;E&gt; f = this.first; if (f == null) { throw new NoSuchElementException(); } else { return f.item; }}// 返回头结点元素，删除，为空则返回nullpublic E poll() { LinkedList.Node&lt;E&gt; f = this.first; return f == null ? null : this.unlinkFirst(f);}// 返回头结点元素，删除，为空则抛出异常public E remove() { return this.removeFirst();}// 末位添加元素public boolean offer(E e) { return this.add(e);}// 返回头结点元素，删除，为空则抛出异常public E pop() { return this.removeFirst();} 双端队列实现方法 双端队列是队列的变形，支持头结点和尾节点的添加元素和获得元素。 与链表类似使用的方法则不在额外说明。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// 头结点前驱添加元素public void push(E e) { this.addFirst(e);}public E pop() { return this.removeFirst();}public E peekFirst() { LinkedList.Node&lt;E&gt; f = this.first; return f == null ? null : f.item;}public E peekLast() { LinkedList.Node&lt;E&gt; l = this.last; return l == null ? null : l.item;}public E removeFirst() { LinkedList.Node&lt;E&gt; f = this.first; if (f == null) { throw new NoSuchElementException(); } else { return this.unlinkFirst(f); }}public E removeLast() { LinkedList.Node&lt;E&gt; l = this.last; if (l == null) { throw new NoSuchElementException(); } else { return this.unlinkLast(l); }}public E pollFirst() { LinkedList.Node&lt;E&gt; f = this.first; return f == null ? null : this.unlinkFirst(f);}public E pollLast() { LinkedList.Node&lt;E&gt; l = this.last; return l == null ? null : this.unlinkLast(l);}public boolean offerFirst(E e) { this.addFirst(e); return true;}public boolean offerLast(E e) { this.addLast(e); return true;}","link":"/2020/05/05/4%20%E6%BA%90%E7%A0%81/Java%E6%BA%90%E7%A0%81%E2%80%94%E2%80%94LinkedList/"},{"title":"Java源码——ArrayList","text":"ArrayList ArrayList和LinkedList &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ArrayList和LinkedList是List接口的重要实现类，也是开发中最常用到的线性表和链表数据结构。ArrayList底层采用数组结构，而LinkedList底层采用Entry节点和指针的链表结构，两者主要性能区别： 查询性能ArrayList更高 中间插入和删除数据LinkedList效率更高 总结： ArrayList底层使用数组结构，数据检索能力更高，默认初始容量10。 当size==length时触发扩容，每次扩容为1.5倍，扩容时旧数组和新数组并存，在GC之前占用两份空间，大数据量会不断扩容导致性能问题，一般建议评估保存数据的容量并赋初始值，如new ArrayList(10000)。 实现了RandomAccess接口，在使用Collections进行二分查找binarySearch时，会使用索引查找，效率更高。 非线程安全，并发修改和查找会影响modCount不匹配，导致快速抛出ConcurrentModificationException异常。 clone方法属于浅克隆shallow clone。 SubList是ArrayList在使用subList方法时的返回内部类，它同样继承自AbstracList但会共享当前ArrayList实例的数组，对SubList的修改其实是对原始数据的修改，一般建议，如果希望subList作为独立副本使用，使用Arrays.copyOf创建。 Arrays.asList方法返回的是Arrays的内部类ArrayList，区别于List中的ArrayList类，虽然同名但只提供了少量的功能，切不可作为后者进行数据处理，否则有NoSuchMethodException风险，一般建议，如果希望作为线性表List接口中的ArrayList，采用以下方式:List dupList = new Array(Arrays.asList(&quot;abc&quot;, &quot;def&quot;, &quot;ghi&quot;))。 成员变量 12345678// 默认容量private static final int DEFAULT_CAPACITY = 10; private static final Object[] EMPTY_ELEMENTDATA = new Object[0];private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = new Object[0];// 实际存储数组transient Object[] elementData;private int size; // size是已存数据容量，length是数组容量，一般size &lt;= lengthprivate static final int MAX_ARRAY_SIZE = 2147483639; add方法和扩容 123456789101112131415// 在数组末尾新增数据，当前数组size==length时触发扩容方法this.growprivate void add(E e, Object[] elementData, int s) { if (s == elementData.length) { elementData = this.grow(); } elementData[s] = e; this.size = s + 1;}public boolean add(E e) { ++this.modCount; this.add(e, this.elementData, this.size); return true;} 12345678910111213141516171819202122232425// Arrays.copyOf使用System.arrayCopy本地方法将原数组数据移植到新容量数组中private Object[] grow(int minCapacity) { return this.elementData = Arrays.copyOf(this.elementData, this.newCapacity(minCapacity));}private Object[] grow() { return this.grow(this.size + 1);}// 如果需要扩容，则会扩大为原来容量的1.5倍private int newCapacity(int minCapacity) { int oldCapacity = this.elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); // aka 1.5倍 if (newCapacity - minCapacity &lt;= 0) { if (this.elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { return Math.max(10, minCapacity); } else if (minCapacity &lt; 0) { throw new OutOfMemoryError(); } else { return minCapacity; } } else { return newCapacity - 2147483639 &lt;= 0 ? newCapacity : hugeCapacity(minCapacity); }} 浅克隆 1234567891011// 浅克隆并重置modCount=0public Object clone() { try { ArrayList&lt;?&gt; v = (ArrayList)super.clone(); v.elementData = Arrays.copyOf(this.elementData, this.size); v.modCount = 0; return v; } catch (CloneNotSupportedException var2) { throw new InternalError(var2); }} 实现RandomAccess接口 123// List接口只有ArrayList实现了RandomAccess声明式接口// 其目的是在使用Collections.binarySearch二分查找方法时采用索引遍历而非迭代器遍历public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, Serializable {...} 12345// Collections方法类// List实现了RandomAccess直接使用indexBinarySearch方法，该方法执行效率高于iteratorBinarySearch方法。public static &lt;T&gt; int binarySearch(List&lt;? extends Comparable&lt;? super T&gt;&gt; list, T key) { return !(list instanceof RandomAccess) &amp;&amp; list.size() &gt;= 5000 ? iteratorBinarySearch(list, key) : indexedBinarySearch(list, key); } modCount 当数组进行了增删改操作，modCount会自增，这是为了保证在并发访问和操作时保证线性表数据统一，否则会抛出ConcurrentModificationException异常，抛出异常的目的是快速反馈给调用者以发现并发问题并采取有效的措施。 123456789101112131415161718192021222324252627282930313233343536public boolean add(E e) { ++this.modCount; this.add(e, this.elementData, this.size); return true;}public void clear() { ++this.modCount; Object[] es = this.elementData; int to = this.size; for(int i = this.size = 0; i &lt; to; ++i) { es[i] = null; }}private void fastRemove(Object[] es, int i) { ++this.modCount; int newSize; if ((newSize = this.size - 1) &gt; i) { System.arraycopy(es, i + 1, es, i, newSize - i); } es[this.size = newSize] = null;}public void sort(Comparator&lt;? super E&gt; c) { int expectedModCount = this.modCount; Arrays.sort(this.elementData, 0, this.size, c); // 如果不相等则抛出异常 if (this.modCount != expectedModCount) { throw new ConcurrentModificationException(); } else { ++this.modCount; }} subList方法和SubList内部类 12345// subList方法返回ArrayList内部类SubListpublic List&lt;E&gt; subList(int fromIndex, int toIndex) { subListRangeCheck(fromIndex, toIndex, this.size); return new ArrayList.SubList(this, fromIndex, toIndex);} 123456789101112131415161718// 同样实现AbstractList抽象类与父类ArrayList相同private static class SubList&lt;E&gt; extends AbstractList&lt;E&gt; implements RandomAccess { private final ArrayList&lt;E&gt; root; private final ArrayList.SubList&lt;E&gt; parent; private final int offset; private int size; ... // 没有trimToSize()方法 // 共享修改父类 public E set(int index, E element) { Objects.checkIndex(index, this.size); this.checkForComodification(); E oldValue = this.root.elementData(this.offset + index); this.root.elementData[this.offset + index] = element; return oldValue; } ...} 12345678910111213141516// 当采用如下用法将抛出NoSuchMethodException异常List mainList = new ArrayList(Arrays.asList(1,2,3,4,5));List subList = mainList.subList(0,2);subList.trimToSize(); // NoSuchMethodException// 当想当然的以为对subList修改时，mainList也被修改subList.set(0, 10);print(mainList); // 10,2,3,4,5print(subList); // 10,2// 可以采取如下措施避免subList问题List dupSubList = new ArrayList(subList);dupSubList.set(0, 10);System.out.println(mainList); // 1,2,3,4,5System.out.println(subList); // 1,2System.out.println(dupSubList); // 10,2 Arrays.asList和Arrays内部类ArraysList 12345public class Arrays {... // 作为内部类虽然与ArrayList同名，但只有部分方法，容易产生NoSuchMethodException异常 private static class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements RandomAccess, Serializable {...} } 123456List list = Arrays.asList(1,2,3,4,5)list.trimToSize(); // NoSuchMethodException// 采取如下措施转换为ArraysListList _list = new Array(list);_list.trimToSize();","link":"/2020/05/01/4%20%E6%BA%90%E7%A0%81/Java%E6%BA%90%E7%A0%81%E2%80%94%E2%80%94ArrayList/"},{"title":"常用算法——贪心算法","text":"贪心算法 每一次站在十字路口，我们总是选择看起来最安全的大路。 概念&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;贪心算法，是指将一个问题拆解成多个子问题，而在解决子问题时，我们总是选择最优方案，也就是说，并不从整体考虑最优解，最终可能是接近最优解的解法。 算法框架 一个复杂问题可以拆分位多个最优子结构。 对子问题进行最优解法。 所有子问题的最优解可能是最终最优解的趋近解法。 算法举例 输入一组整数N的序列A， 满足0&lt;N&lt;10，A的个数为6，每组数据以,分隔 求：整数组合构成的最大hh:mm:ss时间组合，如果不满足则输出invalid。 例1 输入：[3, 2, 4, 9, 5, 5] 输出：23:59:54 例2 输入：[9, 9, 9, 9, 9, 9] 输出：invalid 思路：最大时间组合，从时分秒依次求得，按照贪心算法思想，每一个求得的最大值，最终组合的必然是最大值。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public static void main(String[] args) { Scanner sc = new Scanner(System.in); String input = sc.nextLine(); String[] array = input.substring(1, input.length() - 1).split(\",\"); List&lt;String&gt; inputList = new ArrayList&lt;&gt;(Arrays.asList(array)); // hour List&lt;String&gt; list1 = recombine(inputList); String hour; int intHour = list1.stream() .filter(x -&gt; !x.startsWith(\"0\") &amp;&amp; Integer.valueOf(x) &lt; 23) .map(Integer::valueOf).max(Integer::compare).orElse(-1); if (intHour == -1) { System.out.println(\"invalid\"); return; } hour = intHour + \"\"; inputList.remove(getIndex(hour.substring(0, 1), inputList)); inputList.remove(getIndex(hour.substring(1), inputList)); // min List&lt;String&gt; list2 = recombine(inputList); String min; int intMin = list2.stream().filter(x -&gt; !x.startsWith(\"0\") &amp;&amp; Integer.valueOf(x) &lt; 60) .map(Integer::valueOf).max(Integer::compare).orElse(-1); if (intMin == -1) { System.out.println(\"invalid\"); return; } min = intMin + \"\"; inputList.remove(getIndex(min.substring(0, 1), inputList)); inputList.remove(getIndex(min.substring(1), inputList)); // second List&lt;String&gt; list3 = recombine(inputList); String second; int intSecond = list3.stream().filter(x -&gt; !x.startsWith(\"0\") &amp;&amp; Integer.valueOf(x) &lt; 60) .map(Integer::valueOf).max(Integer::compare).orElse(-1); if (intSecond == -1) { System.out.println(\"invalid\"); return; } second = intSecond + \"\"; System.out.println(String.join(\":\", hour, min, second)); } private static List&lt;String&gt; recombine(List&lt;String&gt; list) { List&lt;String&gt; newList = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; list.size(); i++) { for (int j = 0; j &lt; list.size(); j++) { if (i != j) { newList.add(list.get(i) + list.get(j)); } } } return newList; } private static int getIndex (String target, List&lt;String&gt; list) { int ret = -1; for (int index = 0; index &lt; list.size(); index++) { if (list.get(index).equals(target)) { ret = index; break; } } return ret; }","link":"/2020/04/03/3%20%E7%AE%97%E6%B3%95%E5%92%8C%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/"},{"title":"常用算法——动态规划法","text":"动态规划法 概念&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;动态规划法，是指将一个问题拆解成多个子问题，若干子问题的解是其他子问题的条件，而这些子问题构成了局部最优解，并最终得到整体问题的解。它与分治法最大的区别在于具有公共子问题，即它不是独立的。常用递推方式解决。 有最优子结构特征。 无后效性，子问题的状态一旦确定，不受其后操作的影响。 重叠子问题：分治法讲求无公共子问题，而动态规划法要求，每个子问题并不是独立的，前一子问题状态会应用于后一子问题。 算法框架 定义状态方程F(i,j)=A(i,j) 定义状态转移方程F(i,j)=max(F(i-1,j),F(i-1,B(j))) | F(i-1,j) 构造关于i和j的动态规划表并填表。 算法举例 01背包问题： 描述：在N件物品取出若干件放在容量为W的背包里，每件物品的体积为W1，W2……Wn（Wi为整数），每件物品都有其价值P1,P2……Pn（Pi为整数）。求如何让背包装入最大价值的物品。 思路： 拆分：将该问题拆分，记录存入物品编号与背包容量的矩阵。 状态转移：假设当前的商品编号为i，背包容量为j，商品价值为P(i)，重量为W(i)，那么当前背包容量对应放入的商品最大价值为V(i,j) (1) 当前商品重量&gt;背包重量无法放入，那么最大价值等于该容量下的上一个商品的最大价值，即W(i) &gt; j, 则V(i,j) = V(i-1,j) (2) 当前商品重量&gt;背包重量可以放入，那么需要衡量如果将该商品放入背包是否能产生最大价值，如果不满足也不放。即W(i) &lt;= j, 则 V(i,j) = max(V(i-1, j - W(i)) + P(i), V(i-1, j)) 求V(i,j)中的最大值即背包容纳的最大价值。 12345678910111213141516171819202122232425262728293031public class Knapsack { static int maxValueOfKnapsack(int[] weights, int[] values, int capacity) { int[][] dpTable = new int[values.length][capacity + 1]; int maxValue = 0; // DP问题都是从(1,1)开始 for (int i = 1; i &lt; values.length; i++) { for (int j = 1; j &lt;= capacity; j++) { if (weights[i] &gt; j) { // 放不下 dpTable[i][j] = dpTable[i-1][j]; } else { // 衡量价值 // dpTable[i-1][j-weights[i]] + values[i]表示放入商品之后对应的价值 dpTable[i][j] = Math.max(dpTable[i-1][j-weights[i]] + values[i], dpTable[i-1][j]); } if (maxValue &lt; dpTable[i][j]) { maxValue = dpTable[i][j]; } } } return maxValue; } public static void main(String[] args) { int[] weights = {0, 8, 10, 6, 3, 7, 2}; int[] values = {0, 4, 6, 2, 2, 5, 1}; int capacity = 16; System.out.println(maxValueOfKnapsack(weights, values, capacity)); }}","link":"/2020/04/07/3%20%E7%AE%97%E6%B3%95%E5%92%8C%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"title":"常用算法——分治法","text":"分治法 分而治之，治而合之 说明：文中动态插图援引微信公众号：《五分钟学算法》 概念​ 分治法（Divide and Conquer, D&amp;C），将一个复杂问题纵向拆分为多个最优结构原子问题，再将原子问题的求解合并，组成复杂问题的最终解。递归是分治法的常用手段，也是经典手段。 使用条件 复杂问题拆分后就容易求解。 拆分后的原子问题具有最优子结构。 原子问题的解可以合并成复杂问题的解。 原子问题独立存在，不含公共子问题。 算法框架 分解：纵向拆分复杂问题为原子问题。 解决：对原子问题进行求解。 合并：将原子问题解合并，组成最终解。 算法复杂度​ 一个规模为n的实例可以划分为k个规模为n/k的实例，其中α个实例是需要求解的： O(N) = αT(n/k) + f(n) ​ f(n)与具体的拆分和合并算法有关。 经典算法归并排序（Merge Sort）： 拆分：将一个无序序列递归拆分，最终拆分为长度为1的子序列。 解决：一次将子序列两两返回并进行排序。 合并：自下而上的对子序列进行合并，最终组成完整的排序后序列。 自上而下拆分， 自下而上合并 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class MergeSorted implements IArraySort { @Override public int[] sort(int[] srcArray) { int[] arr = Arrays.copyOf(srcArray, srcArray.length); // 操作副本排序，不影响原数组 if (arr.length &lt; 2) { return arr; } int middle = srcArray.length / 2; int[] leftArr = Arrays.copyOfRange(arr, 0, middle); int[] rightArr = Arrays.copyOfRange(arr, middle, arr.length); return merge(sort(leftArr), sort(rightArr)); } private int[] merge(int[] leftArr, int[] rightArr) { int[] result = new int[leftArr.length + rightArr.length]; int index = 0; while (leftArr.length &gt; 0 &amp;&amp; rightArr.length &gt; 0) { if (leftArr[0] &lt;= rightArr[0]) { result[index] = leftArr[0]; index++; leftArr = Arrays.copyOfRange(leftArr, 1, leftArr.length); } else { result[index] = rightArr[0]; index++; rightArr = Arrays.copyOfRange(rightArr, 1, rightArr.length); } } while (leftArr.length &gt; 0) { result[index] = leftArr[0]; index ++; leftArr = Arrays.copyOfRange(leftArr, 1, leftArr.length); } while (rightArr.length &gt; 0) { result[index] = rightArr[0]; index ++; rightArr = Arrays.copyOfRange(rightArr, 1, rightArr.length); } return result; } public static void main(String[] args) { int[] arr = {3,23,4,13,4,5,63,6,2}; arr = new MergeSorted().sort(arr); System.out.println(arr.length); for (int i : arr) { System.out.print(i + &quot; &quot;); } }} 快速排序（Quick Sort） 解决：设置基准点pivot，并定位两端索引位置left和right，一边排序找到比pivot小的放在左边，比pivot大的放在右边。 拆分：一边排序后，以pivot为分割点，将左右两边序列依次执行步骤1。 合并：将原子序列排序后合并成为最终有序序列。 自上而下排序并拆分 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class QuikSort implements IArraySort { @Override public int[] sort(int[] sourceArray) { // 对 arr 进行拷贝，不改变参数内容 int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); quickSort(arr, 0, arr.length - 1); return arr; } private void quickSort(int[] arr, int left, int right) { if (left &gt; right) { return; } int leftIndex = left; int rightIndex = right; int pivot = arr[leftIndex]; // 基准 while (left &lt; right) { while (left &lt; right &amp;&amp; arr[right] &gt;= pivot) { right--; } while (left &lt; right &amp;&amp; arr[left] &lt;= pivot) { left++; } if (left &lt; right) { swap(arr, right, left); } } swap(arr, leftIndex, left); // 此时left==right quickSort(arr, leftIndex, left-1); quickSort(arr,left + 1, rightIndex); } private void swap(int[] arr, int i, int j) { int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; } public static void main(String[] args) { int[] arr = {6,1,2,7,9,3,4,5,10,8}; arr = new QuikSort().sort(arr); System.out.println(arr.length); for (int i : arr) { System.out.print(i + &quot; &quot;); } }} 注意：如果基准点pivot设置为左顶端数，那么应该保证先从右侧开始比较，这样可以保证最终left和right停留的相同位置是小于pivot的，所以可以交换，否则，交换的将是大于pivot的值，不满足快排条件。 汉诺塔 A、B、C三根柱子，其中A柱子上面有从小叠到大的n个圆盘，现要求将A柱子上的圆盘移到C柱子上去，期间只有一个原则：一次只能移到一个盘子且大盘子不能在小盘子上面，求移动的步骤和移动的次数 把n-1个盘子由A 移到 B；把第n个盘子由 A移到 C；把n-1个盘子由B 移到 C； 1234567891011121314151617181920212223242526272829public class Hanoi { private static int step = 1; // 步数 public static void hanoi(int plateNum, String a, String b, String c) { if (plateNum == 1) { move(plateNum, a, c); } else { hanoi(plateNum - 1, a, c, b); move(plateNum, a, c); hanoi(plateNum - 1, b, a, c); } } private static void move(int plateNum, String from, String to) { System.out.printf(&quot;%d step, %d plate move from %s to %s \\n&quot;, step++, plateNum, from, to); } public static void main(String[] args) { hanoi(3, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;); }}1 step, 1 plate move from A to C 2 step, 2 plate move from A to B 3 step, 1 plate move from C to B 4 step, 3 plate move from A to C 5 step, 1 plate move from B to A 6 step, 2 plate move from B to C 7 step, 1 plate move from A to C","link":"/2020/04/01/3%20%E7%AE%97%E6%B3%95%E5%92%8C%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E5%88%86%E6%B2%BB%E6%B3%95/"},{"title":"Java源码——TreeMap和红黑树","text":"TreeMap和红黑树 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;红黑树与AVL树类似，都可以在插入和删除时通过旋转来保持自身树的平衡，从而获得较高的查找性能。与AVL树相比，红黑树并不是严格的平衡树，只要保证从根节点出发到叶子节点的最长路径不超过最短路径的2倍，最坏情况算法复杂度依然可以保证O（logn）。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;红黑树的每个节点都会着色，要么是黑色要么是红色，通过重新着色和左右旋转完成自身平衡的调整，它需要满足以下4个要求： 节点只能是红色或者黑色。 根节点和NIL节点必须是黑色。 一条路径不能存在连续的两个红色节点。 任何树内，根节点到叶子节点的路径上包含相同黑色节点的个数。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NIL是每个叶子节点有两个NIL子节点，NIL节点物理上并不存在，只存在与逻辑空间，主要是为了满足红黑树自旋稳定性。红黑树的旋转在3次之内可以达到平衡。 TreeMap&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TreeMap适用于对key有排序要求的场景中，TreeMap使用红黑树作为底层数据结构，TreeMap继承自AbstractMap并实现了NavigableMap接口，该接口要求提供排序算法。作为TreeMap的key必须具有比较能力Comparable或者自定义实现比较器Comparator以支持排序规定，所以key不允许为null。 12public class TreeMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements NavigableMap&lt;K,V&gt;, Cloneable,Serializable &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TreeMap优先使用比较器Comparator，如果比较器不存在则使用key自然排序Comparable，如果两者都不存在则会抛出异常ClassCastException 成员变量和构造器 123456789101112131415161718192021222324252627282930313233343536373839404142// 全局比较器private final Comparator&lt;? super K&gt; comparator;// 根节点private transient Entry&lt;K,V&gt; root;private transient int size = 0;private transient int modCount = 0;// boolean类型表示红黑两色private static final boolean RED = false;private static final boolean BLACK = true;// 内部类，红黑树节点，其中color是节点的颜色static final class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { K key; V value; Entry&lt;K,V&gt; left; Entry&lt;K,V&gt; right; Entry&lt;K,V&gt; parent; boolean color = BLACK;｝// 默认构造器，比较器为nullpublic TreeMap() { comparator = null;}// 带自定义比较器的构造器public TreeMap(Comparator&lt;? super K&gt; comparator) { this.comparator = comparator; }// 使用有序Map的比较器，并将数据转移public TreeMap(SortedMap&lt;K, ? extends V&gt; m) { comparator = m.comparator(); try { buildFromSorted(m.size(), m.entrySet().iterator(), null, null); } catch (java.io.IOException cannotHappen) { } catch (ClassNotFoundException cannotHappen) { } } 结构调整 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;红黑树的结构变化发生在数据的插入和删除，一旦发生变化红黑树的平衡就有可能被破坏，这时就需要旋转重新达到平衡。需要考虑以下三种条件： 被调整的节点总是红色节点。 如果新增节点的父节点是黑色的就无需改变，因为可以保证红黑树的约束。 如果新节点的父节点是红色，需要进行重新着色、左右旋转最终达到约束条件重新保持红黑树的平衡。 插入数据 ​ TreeMap的插入就是按照key的比较进行遍历，按照二分查找的特点大于当前节点向右遍历，小于当前节点向左遍历，当确定节点之后再考虑着色和旋转，保证红黑树的约束。 put()方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public V put(K key, V value) { // t表示当前节点 Entry&lt;K,V&gt; t = root; // 如果当前是空树，则新插入数据设为根 if (t == null) { compare(key, key); // type (and possibly null) check root = new Entry&lt;&gt;(key, value, null); size = 1; modCount++; return null; } // 接收比较结果 int cmp; Entry&lt;K,V&gt; parent; Comparator&lt;? super K&gt; cpr = comparator; // 比较方式分支 if (cpr != null) { // 循环目标：入参key与当前节点key不断比较 do { parent = t; // 比较当前key和入参key cmp = cpr.compare(key, t.key); if (cmp &lt; 0) t = t.left; //小就置为左节点 else if (cmp &gt; 0) t = t.right; //大置为右节点 else return t.setValue(value); //相等覆盖 // 如果没有相等节点，则会进入NIL节点 } while (t != null); } else { // 使用Comparable不允许key为空 if (key == null) throw new NullPointerException(); @SuppressWarnings(\"unchecked\") Comparable&lt;? super K&gt; k = (Comparable&lt;? super K&gt;) key; do { parent = t; cmp = k.compareTo(t.key); if (cmp &lt; 0) t = t.left; else if (cmp &gt; 0) t = t.right; else return t.setValue(value); } while (t != null); } // 新节点创建并根据比较结果置于父节点的左或右节点 Entry&lt;K,V&gt; e = new Entry&lt;&gt;(key, value, parent); if (cmp &lt; 0) parent.left = e; else parent.right = e; // 对新节点着色和旋转达到平衡 fixAfterInsertion(e); size++; modCount++; return null;}// 选择比较方法final int compare(Object k1, Object k2) { return comparator==null ? ((Comparable&lt;? super K&gt;)k1).compareTo((K)k2) : comparator.compare((K)k1, (K)k2);} 由于TreeMap通过比较来判断key的唯一性，所以equals和hashCode方法不是必须覆写的。 fixAfterInsertion()方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879private void fixAfterInsertion(Entry&lt;K,V&gt; x) { // 新节点着色为红色，满足约束条件1 x.color = RED; // 遍历使树达到平衡的条件 while (x != null &amp;&amp; x != root &amp;&amp; x.parent.color == RED) { // 如果父节点是祖父节点的左子节点 if (parentOf(x) == leftOf(parentOf(parentOf(x)))) { // 查看父节点的兄弟节点（右叔）颜色 Entry&lt;K,V&gt; y = rightOf(parentOf(parentOf(x))); if (colorOf(y) == RED) { // 如果右叔节点是红色 setColor(parentOf(x), BLACK); // 父节点着黑色 setColor(y, BLACK); // 右叔节点着黑色 setColor(parentOf(parentOf(x)), RED); // 祖父节点着红色 x = parentOf(parentOf(x));// 当前节点指向红色的祖父节点 } else { // 如果右叔节点是黑色 if (x == rightOf(parentOf(x))) { x = parentOf(x); // 当前节点指向红色的父节点 rotateLeft(x); // 左旋调整平衡 } setColor(parentOf(x), BLACK); // 父节点着黑色 setColor(parentOf(parentOf(x)), RED); // 祖父节点着红色 rotateRight(parentOf(parentOf(x))); // 左旋红色的祖父节点 } } else {// 如果父节点是祖父节点的右子节点，过程与上述类似 Entry&lt;K,V&gt; y = leftOf(parentOf(parentOf(x))); if (colorOf(y) == RED) { setColor(parentOf(x), BLACK); setColor(y, BLACK); setColor(parentOf(parentOf(x)), RED); x = parentOf(parentOf(x)); } else { if (x == leftOf(parentOf(x))) { x = parentOf(x); rotateRight(x); } setColor(parentOf(x), BLACK); setColor(parentOf(parentOf(x)), RED); rotateLeft(parentOf(parentOf(x))); } } } root.color = BLACK;}// 左旋private void rotateLeft(Entry&lt;K,V&gt; p) { if (p != null) { Entry&lt;K,V&gt; r = p.right; p.right = r.left; if (r.left != null) r.left.parent = p; r.parent = p.parent; if (p.parent == null) root = r; else if (p.parent.left == p) p.parent.left = r; else p.parent.right = r; r.left = p; p.parent = r; }}// 右旋private void rotateRight(Entry&lt;K,V&gt; p) { if (p != null) { Entry&lt;K,V&gt; l = p.left; p.left = l.right; if (l.right != null) l.right.parent = p; l.parent = p.parent; if (p.parent == null) root = l; else if (p.parent.right == p) p.parent.right = l; else p.parent.left = l; l.right = p; p.parent = l; }} 使用左旋或者右旋的条件： 父节点是红色，叔叔节点是红色，则重新着色。 父节点时红色，叔叔节点是黑色，如果新节点是父节点的左节点，右旋。 父节点时红色，叔叔节点是黑色，如果新节点是父节点的右节点，左旋。 插入举例 123456789TreeMap&lt;Integer, String&gt; treeMap = new TreeMap&lt;&gt;();treeMap.put(13,\"\");treeMap.put(14,\"\");treeMap.put(15,\"\");treeMap.put(16,\"\");treeMap.put(42,\"\");treeMap.remove(15);treeMap.put(20,\"\");// 从空树开始演示红黑树插入，调整平衡的过程。 插入13、14、15 插入16 插入42 删除15，插入20 AVL树和红黑树 时间复杂度：对于任意高度的节点，它的黑深度满足≥ height / 2，即对于任意包含n个节点的红黑树，它的根节点高度h≤2log2 (n+1)，当树失去平衡，时间复杂度有可能变为O(n)，即h=n。所以，可以保证树的高度始终保持在O(logn)时，所有操作的时间复杂度保持在O(logn)以内。 平衡性：AVL是严格的平衡二叉查找树，任意子树的高度差始终在1以内，红黑树平衡没有如此严格，所以当节点个数一致，红黑树的高度可能大于AVL树，换句话说，平均查找次数会高于相同情况下的AVL树。插入时，两者都可以保证最多两次旋转就可以使树恢复平衡；删除时，由于红黑树对高度差的不严格，最多三次就可以恢复平衡，而AVL可能需要更多的旋转。因此，频繁的插入和删除红黑树更合适；低频修改，高频查询AVL树更合适","link":"/2020/05/15/4%20%E6%BA%90%E7%A0%81/Java%E6%BA%90%E7%A0%81%E2%80%94%E2%80%94TreeMap%E5%92%8C%E7%BA%A2%E9%BB%91%E6%A0%91/"},{"title":"Java源码——HashMap","text":"HashMap（一） &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;HashMap应该是Java世界应用最多的数据结构对象，HashMap从Java1.2开始存在，在Java8经过一次大改，Java5又出现了同步包java.util.concurrent其中就包含高性能的同步HashMap对象ConcurrentHashMap，这其中精彩世界以下揭晓。 0. 哈希表&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;哈希表，又称散列表，是计算机从业者最熟悉不过的数据结构，由于其在数据索引时只拥有O(n)时间复杂度而广泛应用。生活中我们也经常遇到类似情景，比如查字典过程、电话本黄页等等，通过索引关键字获得对应映射的值。 上图为哈希表的简易示意图，哈希表的基本组成包括： 桶，是保存一组数据的空间，通常是一个链表（单向链表）或者一棵树（红黑树）； 槽位，是通过关键字定位数据桶的索引位置，通过关键字定位到槽位我们一般称为落槽。 哈希冲突，理想状态下每个槽位对应的桶只保存一个数据，那么就形成了槽位和数据一一对应关系（例如图中0号槽位和k号槽位），然而由于哈希表中的槽位通常是有限的，当数据量大于槽位个数时势必会产生一个槽位对应的桶中保存了多个数据，这就称为哈希冲突。解决哈希冲突的方式一般采用单向链表或者红黑树。 哈希函数，将数据元素的关键字key作为自变量，通过一定的函数关系计算出的值，即为该元素的存储地址，这个函数称为哈希函数。哈希函数需要解决的两大问题分别是均匀落槽和哈希冲突。前者在极端情况下会产生所有数据落槽在同一个桶中，产生空间资源浪费和性能低效。 加载因子（load factor），理想状态下，如果空间无限大则槽位可以保证只映射一个数据那么它的时间复杂度是完美的O(1)，如果空间有限，为了保证数据查询时间，可以将数据均分到每个桶中形成较短链路的链表，设计合理的哈希表可以平衡空间与时间。空间和时间永远是计算机领域的哲学问题，加载因子就是平衡这一问题的系数(0~1的浮点数)，研究表明，加载因子系数值越大空间损耗越小时间损耗越大，相反如果系数值越小空间损耗越大时间损耗越小，在Java中默认为0.75。 再哈希（rehash），当一个哈希表向槽位更多的哈希表迁移时，由于槽位个数发生变化，这就要求原哈希表中的每个元素重新计算哈希值并落槽到新哈希表中，这一过程称为rehash，是哈希表的重要性能指标之一。 概念虽然较多，但都比较容易理解且重要 举个例子： 当我们需要从手机通讯录中查找某一个人的联系电话，这个过程可以拟化为一次哈希表的查找。 首先，有张三、李三、王三三个人的联系方式需要存入通讯录，我们假定他们的名字就是关键字，通过哈希函数计算关键字获得落槽位置分别是拼音首字母z、l和w，此时这个哈希表槽位数共有三个，每个槽位对应的桶分别是张三、李三、王三的电话号码，当我们需要拨打张三电话时直接找到z对应的电话拨出即可。 然而，又来了三个人需要保存电话，他们分别是张四、李四、王四，通过哈希函数计算名字关键字得到槽位还是z、l和w，这就是哈希冲突，三个槽位对应的桶分别保存了两个电话号码，当我们拨打张四电话时，需要先找到z，在从z的列表中找到张四电话并拨出。 最后，又来了两个人赵三和刘三，很明显目前的通讯录无法保存他们的电话，这时我们需要对通讯录扩容并将现在的六个人电话迁移过去，但是迁移时需要满足一个约束，原来的六个人必须都重新通过哈希函数计算落槽位置，这就是rehash。 1. JAVA 7的实现成员变量 HashMap的实现实际是一个桶数组或者称为链表数组，数组下标索引即槽位，同时定义了默认的加载因子、初始容量、最大范围和扩容条件。 12345678910111213141516171819202122232425262728public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable{ static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // HashMap的默认初始大小16 static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; // HashMap允许的最大范围2^30 static final float DEFAULT_LOAD_FACTOR = 0.75f; // 默认加载因子 /** * The table, resized as necessary. Length MUST Always be a power of two. */ // table就是哈希表，它实际是一个桶数组或者链表数组，官方注释表明哈希表容量必须满足2的幂(向上取值) transient Entry&lt;K,V&gt;[] table = (Entry&lt;K,V&gt;[]) EMPTY_TABLE; transient int size; // 当前HashMap以保存数据的个数 // HashMap扩容阈值，threshlod = capacity * loadFactor,如果size &gt;= threshlod则需要扩容，而并非size == table.length。 int threshold; /** * A randomizing value associated with this instance that is applied to * hash code of keys to make hash collisions harder to find. If 0 then * alternative hashing is disabled. */ // 哈希种子，哈希冲突更难被发现 transient int hashSeed = 0;} Java7采用单向链表来解决哈希冲突，以下是定义的私有内部类链表节点 1234567static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final K key; // 关键字 V value; // 值 Entry&lt;K,V&gt; next; // 后继 int hash; // 哈希值 ...} put方法 put方法是研究HashMap的精髓，掌握了该方法也就基本掌握HashMap的原理。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101public V put(K key, V value) { if (table == EMPTY_TABLE) { inflateTable(threshold); // 空表时扩容默认大小的哈希表 } if (key == null) return putForNullKey(value); // HashMap允许存在一个关键字为null的值（区别于HashTable） int hash = hash(key); // 哈希函数，求关键字对应的哈希值，如下方法 int i = indexFor(hash, table.length); // 计算落槽位置，如下方法 // 遍历槽位对应的链表，从第一个节点开始 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) { Object k; // 当哈希值相同并且key相同时则在链表中找到了对应数据，更改value值 if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++; // 如果链表没有找到数据则说明是新数据，将该数据添加到链表头部，如果容量达到阈值，则需要扩容，对旧表做迁移，需要rehash，如下方法。 addEntry(hash, key, value, i); return null;}// 关键的哈希函数，由复杂的移位和异或等位运算组成，为了保证发生哈希冲突时，数据可以均匀落槽// 当加载因子默认是0.75时，哈希冲突最多为8个。final int hash(Object k) { int h = hashSeed; if (0 != h &amp;&amp; k instanceof String) { return sun.misc.Hashing.stringHash32((String) k); } h ^= k.hashCode(); // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);}// 1.参数h是哈希函数计算的哈希值，length是哈希表的长度，将哈希值与length-1求与操作可以保证索引在length范围内// 2.在哈希表中每个元素的操作都离不开落槽，通常采用模运算，但模运算在计算机原理中是比较耗时的方式，所以HashMap要求容量必须是2次幂，这样依据二进制特点，length-1永远是多个1的排列，这样与运算就是与h的对位与，效率更高。static int indexFor(int h, int length) { // assert Integer.bitCount(length) == 1 : \"length must be a non-zero power of 2\"; return h &amp; (length-1);}void addEntry(int hash, K key, V value, int bucketIndex) { // 如果当前size达到阈值则扩容并数据迁移 if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) { resize(2 * table.length); // 扩容和rehash hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length); } createEntry(hash, key, value, bucketIndex);}// 新数据添加到链表头部void createEntry(int hash, K key, V value, int bucketIndex) { Entry&lt;K,V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); size++;}// 扩容新表容量是旧表的2倍（保证2次幂）void resize(int newCapacity) { Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return; } Entry[] newTable = new Entry[newCapacity]; transfer(newTable, initHashSeedAsNeeded(newCapacity)); // 数据迁移 table = newTable; threshold = (int) Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1);}// 数据迁移，需要对旧表每个数据重新计算哈希值void transfer(Entry[] newTable, boolean rehash) { int newCapacity = newTable.length; for (Entry&lt;K,V&gt; e : table) { while(null != e) { Entry&lt;K,V&gt; next = e.next; if (rehash) { e.hash = null == e.key ? 0 : hash(e.key); } int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; } }} 哈希函数 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;哈希函数对String类型做了特殊处理，实际上是调用了String类中的hash32()方法，计算获得String类型的32位哈希值，native底层使用murmur3_32()哈希方法而非String的hashCode()方法，原因是因为hashCode()方法将字符串的每个字符Unicode值累加获得，如果使用String作为key的话这样的处理方式会增加哈西冲突的概率。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;同时可以看到哈希函数入参是对象，计算哈希码过程中也使用到了对象的hashCode方法，也就是说，哈希函数依赖对象自身的哈希码，并以此额外计算了新的哈希码供HashMap使用，这样的好处是可以防止低质量的哈希函数。 123456789101112131415161718192021222324252627282930313233343536373839// HashMap 哈希函数final int hash(Object k) { int h = hashSeed; // 字符串哈希值 if (0 != h &amp;&amp; k instanceof String) { return sun.misc.Hashing.stringHash32((String) k); } h ^= k.hashCode(); // 对象自身哈希码 h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);}// String的hashCode方法和hash32方法// 每个字符Unicode码累加public int hashCode() { int h = hash; if (h == 0 &amp;&amp; value.length &gt; 0) { char val[] = value; for (int i = 0; i &lt; value.length; i++) { h = 31 * h + val[i]; } hash = h; } return h;}// 底层使用murmur3_32方法int hash32() { int h = hash32; if (0 == h) { // harmless data race on hash32 here. h = sun.misc.Hashing.murmur3_32(HASHING_SEED, value, 0, value.length); // ensure result is not zero to avoid recalcing h = (0 != h) ? h : 1; hash32 = h; } return h;} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在不同的编码规范中都要求软件工程师重写equals方法时要重写hashCode方法，从HashMap中就可以看出，当新元素落槽之后，链表从头结点开始比较的第一个条件就是对象的哈希值是否相等，且这一条件是短路的，即使key的值相同也会认为是不同的对象。 123456789101112131415public V put(K key, V value) { ... // 遍历槽位对应的链表，从第一个节点开始 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) { Object k; // 首先比较哈希码，且是短路条件，不满足立刻退出，即使key相等 if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } ...} 所以建议采用《Effective Java》中的第8、9条条例，小心的设计自己对象的equals和hashCode方法。假如，两个方法设计糟糕将会导致该对象作为key保存在HashMap中时，当对该对象执行get或put等操作，即使客观上两者是相同的对象，也会被区别对待，最终导致HashMap产生内存泄漏问题。 落槽方法indexFor &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在解决哈希冲突问题上，通常采用取余操作将数据均分到不同桶中，然而在计算机中取余操作相比于其他逻辑运算比较消耗性能，首先需要对数值转换十进制，然后不断的执行被除数和除数的除法操作最终获得余数，这在大范围使用上是不合适的，HashMap采用对2次幂减1并求异或是在二进制层面的位运算，效率很高更适合作为解决哈希冲突的手段，唯一约束就是要求容量必须保证是2次幂 123static int indexFor(int h, int length) { return h &amp; (length-1);} 假设，整型h=101，length=8，s=length-1=7 h对应二进制是 0000 0000 0000 0000 0000 0000 0110 0101‬ length对应二进制是 0000 0000 0000 0000 0000 0000 0001 0000 length-1对应的二进制是 0000 0000 0000 0000 0000 0000 0000 1111 按照异或的特性会将h的高位全部截断只保留与h对应的位数 0000 0000 0000 0000 0000 0000 0000 0101‬ 相当于求0101^1111，最终的哈希值是0101，即5 内存泄漏 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过源码可以看到，比较元素相等时优先比较哈希码是否相等，再短路比较key相等，所以使用HashMap时需要保证： 作为键值Key必须是不可变的。 如果使用类作为Key，那该类最好是是不可变类。 如果不是不可变类，那应该保证同时重写equals方法和hashCode方法，并且重写的hashCode方法是无状态的，即状态不可变，不会使用可变的依赖值，否则会导致理论上同一对象却有两种不同的hashCode，最终使HashMap发生内存泄漏。 内存溢出：存储容量过多且GC无法回收，导致内存使用量达到JVM阈值，发生OOM。 内存泄漏：对象以后不会被访问，但由于某些原因（在可用的GC ROOT上）GC无法回收，导致无效数据占用内存空间并最终发生OOM。","link":"/2020/05/07/4%20%E6%BA%90%E7%A0%81/Java%E6%BA%90%E7%A0%81%E2%80%94%E2%80%94HashMap%EF%BC%88%E4%B8%80%EF%BC%89/"},{"title":"常用算法——分治法","text":"分治法 分而治之，治而合之 说明：文中动态插图援引微信公众号：《五分钟学算法》 概念​ 分治法（Divide and Conquer, D&amp;C），将一个复杂问题纵向拆分为多个最优结构原子问题，再将原子问题的求解合并，组成复杂问题的最终解。递归是分治法的常用手段，也是经典手段。 使用条件 复杂问题拆分后就容易求解。 拆分后的原子问题具有最优子结构。 原子问题的解可以合并成复杂问题的解。 原子问题独立存在，不含公共子问题。 算法框架 分解：纵向拆分复杂问题为原子问题。 解决：对原子问题进行求解。 合并：将原子问题解合并，组成最终解。 算法复杂度​ 一个规模为n的实例可以划分为k个规模为n/k的实例，其中α个实例是需要求解的： O(N) = αT(n/k) + f(n) ​ f(n)与具体的拆分和合并算法有关。 经典算法归并排序（Merge Sort）： 拆分：将一个无序序列递归拆分，最终拆分为长度为1的子序列。 解决：一次将子序列两两返回并进行排序。 合并：自下而上的对子序列进行合并，最终组成完整的排序后序列。 自上而下拆分， 自下而上合并 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class MergeSorted implements IArraySort { @Override public int[] sort(int[] srcArray) { int[] arr = Arrays.copyOf(srcArray, srcArray.length); // 操作副本排序，不影响原数组 if (arr.length &lt; 2) { return arr; } int middle = srcArray.length / 2; int[] leftArr = Arrays.copyOfRange(arr, 0, middle); int[] rightArr = Arrays.copyOfRange(arr, middle, arr.length); return merge(sort(leftArr), sort(rightArr)); } private int[] merge(int[] leftArr, int[] rightArr) { int[] result = new int[leftArr.length + rightArr.length]; int index = 0; while (leftArr.length &gt; 0 &amp;&amp; rightArr.length &gt; 0) { if (leftArr[0] &lt;= rightArr[0]) { result[index] = leftArr[0]; index++; leftArr = Arrays.copyOfRange(leftArr, 1, leftArr.length); } else { result[index] = rightArr[0]; index++; rightArr = Arrays.copyOfRange(rightArr, 1, rightArr.length); } } while (leftArr.length &gt; 0) { result[index] = leftArr[0]; index ++; leftArr = Arrays.copyOfRange(leftArr, 1, leftArr.length); } while (rightArr.length &gt; 0) { result[index] = rightArr[0]; index ++; rightArr = Arrays.copyOfRange(rightArr, 1, rightArr.length); } return result; } public static void main(String[] args) { int[] arr = {3,23,4,13,4,5,63,6,2}; arr = new MergeSorted().sort(arr); System.out.println(arr.length); for (int i : arr) { System.out.print(i + &quot; &quot;); } }} 快速排序（Quick Sort） 解决：设置基准点pivot，并定位两端索引位置left和right，一边排序找到比pivot小的放在左边，比pivot大的放在右边。 拆分：一边排序后，以pivot为分割点，将左右两边序列依次执行步骤1。 合并：将原子序列排序后合并成为最终有序序列。 自上而下排序并拆分 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class QuikSort implements IArraySort { @Override public int[] sort(int[] sourceArray) { // 对 arr 进行拷贝，不改变参数内容 int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); quickSort(arr, 0, arr.length - 1); return arr; } private void quickSort(int[] arr, int left, int right) { if (left &gt; right) { return; } int leftIndex = left; int rightIndex = right; int pivot = arr[leftIndex]; // 基准 while (left &lt; right) { while (left &lt; right &amp;&amp; arr[right] &gt;= pivot) { right--; } while (left &lt; right &amp;&amp; arr[left] &lt;= pivot) { left++; } if (left &lt; right) { swap(arr, right, left); } } swap(arr, leftIndex, left); // 此时left==right quickSort(arr, leftIndex, left-1); quickSort(arr,left + 1, rightIndex); } private void swap(int[] arr, int i, int j) { int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; } public static void main(String[] args) { int[] arr = {6,1,2,7,9,3,4,5,10,8}; arr = new QuikSort().sort(arr); System.out.println(arr.length); for (int i : arr) { System.out.print(i + &quot; &quot;); } }} 注意：如果基准点pivot设置为左顶端数，那么应该保证先从右侧开始比较，这样可以保证最终left和right停留的相同位置是小于pivot的，所以可以交换，否则，交换的将是大于pivot的值，不满足快排条件。 汉诺塔 A、B、C三根柱子，其中A柱子上面有从小叠到大的n个圆盘，现要求将A柱子上的圆盘移到C柱子上去，期间只有一个原则：一次只能移到一个盘子且大盘子不能在小盘子上面，求移动的步骤和移动的次数 把n-1个盘子由A 移到 B；把第n个盘子由 A移到 C；把n-1个盘子由B 移到 C； 1234567891011121314151617181920212223242526272829public class Hanoi { private static int step = 1; // 步数 public static void hanoi(int plateNum, String a, String b, String c) { if (plateNum == 1) { move(plateNum, a, c); } else { hanoi(plateNum - 1, a, c, b); move(plateNum, a, c); hanoi(plateNum - 1, b, a, c); } } private static void move(int plateNum, String from, String to) { System.out.printf(&quot;%d step, %d plate move from %s to %s \\n&quot;, step++, plateNum, from, to); } public static void main(String[] args) { hanoi(3, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;); }}1 step, 1 plate move from A to C 2 step, 2 plate move from A to B 3 step, 1 plate move from C to B 4 step, 3 plate move from A to C 5 step, 1 plate move from B to A 6 step, 2 plate move from B to C 7 step, 1 plate move from A to C","link":"/2020/04/01/3%20%E7%AE%97%E6%B3%95%E5%92%8C%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E2%80%94%E2%80%94%E7%BA%A2%E9%BB%91%E6%A0%91/"},{"title":"Java源码——HashMap","text":"HashMap（二） 说明：文章部分内容来自https://coolshell.cn/articles/9606.html &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;JDK7中的HashMap是线程不安全的，尽管官方文档明确声明并发情况下不能使用，但在问题列表例总能找到强行使用的童鞋，这就会导致一个严重的问题：死链，通过恢复现场就能找到线程在调用get()方法时被堵死，重启可以临时清空内存，但时间一长又会复现。 死链 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;死链又称为无限循环（HashMap Infinite Loop），一旦产生死链，系统的CPU就飙满，导致fatal级系统问题。其根本原因主要发生在扩容方法resize()时将旧数据移动到新容器的过程中，以下通过源码来看。 12345678910111213141516171819202122232425262728293031void resize(int newCapacity) { Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return; } Entry[] newTable = new Entry[newCapacity]; // 注意transfer方法 transfer(newTable, initHashSeedAsNeeded(newCapacity)); table = newTable; threshold = (int)Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1);}void transfer(Entry[] newTable, boolean rehash) { int newCapacity = newTable.length; for (Entry&lt;K,V&gt; e : table) { while(null != e) { // 数据迁移时发生死链场景 Entry&lt;K,V&gt; next = e.next; if (rehash) { e.hash = null == e.key ? 0 : hash(e.key); } int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; } }} 当两个线程Thread1和Thread2交替执行transfer注释部分的代码时，有可能导致如下图的链表指向情况。 当执行Thread1的get(key)方法时，恰好该key落槽到死链的桶中，链表的循环遍历就会在3和7之间无限循环，拉满CPU使用率。 虽然该问题很快被SUN发现，但被认为HashMap本来就是线程非安全并不应该用在并发场景下，可以使用ConcurrentHashMap代替，所以在多个大小版本中一直没有解决更新，直到JDK8发布。 2 JDK8 HashMap改进总结 将桶内数据结构仅支持链表改为同时支持链表+红黑树。 针对死链问题，改进扩容时的插入顺序。 新增lambda表达式支持函数，如forEach()。 新增API：replace()和merge() 在源码浏览时，如果与JDK7相同则不重复介绍 由于整体源码写作风格与当前流行的编码规范格格不入（圈复杂度高，变量不优先初始化而是首次调用初始化，排版有些随意）可能会引起阅读的不适…… 成员变量 123456// 红黑树转换阈值，当桶内数据容量达到8时，会将链表转变为红黑树static final int TREEIFY_THRESHOLD = 8; // 当删除或容器扩容，桶内容量减少到6时会还原回链表static final int UNTREEIFY_THRESHOLD = 6;// 红黑树桶的最小容量是64static final int MIN_TREEIFY_CAPACITY = 64; 一般来说，当哈希码分布均匀时，很少会产生红黑树桶，TREEIFY_THRESHOLD设置为8从离散随机分布的角度来看满足泊松分布，即加载因子在默认0.75时，红黑树转换阈值达到8的概率是比较低的，因为虽然红黑树提高了元素的遍历查找和修改的效率，但是空间上比链表占用二倍的内存，同时树化的过程是需要消耗时间的。总结，链表转换红黑树的前提是哈希函数设计不合理导致大量哈希冲突，设计良好的哈希函数不会也没有必要树化。 1234567891011121314// 官方注释/* Because TreeNodes are about twice the size of regularnodes, we use them only when bins contain enough nodes to warrant use(see TREEIFY_THRESHOLD). And when they become too small (due to removal or resizing) they are converted back to plain bins. In usages with well-distributed user hashCodes, tree bins are rarely used. Ideally, under random hashCodes, the frequency of nodes in bins follows a Poisson distribution(http://en.wikipedia.org/wiki/Poisson_distribution) with a parameter of about 0.5 on average for the default resizing threshold of 0.75, although with a large variance because of resizing granularity. Ignoring variance, the expected occurrences of list size k are (exp(-0.5) * pow(0.5, k)/factorial(k)). The first values are:0: 0.606530661: 0.303265332: 0.075816333: 0.012636064: 0.001579525: 0.000157956: 0.000013167: 0.000000948: 0.00000006*/static final int TREEIFY_THRESHOLD = 8; 泊松分布与加载因子无关，而是量化发生树化的概率；加载因子与哈希表的容量有关。 比如: 默认容量capacity = 16，可放数据量为threshold = capacity * loadfactor = 16 * 0.75 = 12，那么一个桶内放入8个元素（树化阈值）的概率是0.00000006（泊松分布概率）。 同理，当容器扩容到64时，threshold = capacity * loadfactor = 64 * 0.75 = 48, 那么一个桶内放入8个元素的概率还是0.00000006。 put方法 同样的，put方法永远是研究HashMap的精髓。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public V put(K key, V value) { // putVal先计算key的哈希码，hashCode值高16位与低16异或 return this.putVal(hash(key), key, value, false, true);}final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 空表初始化并赋值 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 落位i = (n-1) &amp; hash如果元素不存在则在桶中新建节点 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else { Node&lt;K,V&gt; e; K k; // 如果桶内头元素是目标元素，则更新数据 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 如果桶内是红黑树则调用红黑树方法保存节点 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 如果桶内是链表则调用链表方法保存节点 else { for (int binCount = 0; ; ++binCount) { // 插入数据（尾插），桶内元素个数达到树化阈值则跳出循环进行树化 if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } // 链表插入数据 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; } } if (e != null) { // existing mapping for key V oldValue = e.value; // onlyIfAbsent指如果元素相同新指替换旧值 if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;}// 以下三个方法是LinkedHashMap的实现方法，void afterNodeAccess(Node&lt;K,V&gt; p) { }void afterNodeInsertion(boolean evict) { }void afterNodeRemoval(Node&lt;K,V&gt; p) { } JAVA7的HashMap在新增数据时是将新数据插入桶内头部，而JAVA8采用在链表尾部插入数据。 源码中的变量初始化赋值发生在第一次调用点，所以阅读起来并不顺畅。 哈希函数和落位操作比JAVA7更加简洁且直观。 元素相等判断不变，还是以哈希码相等开始并且是短路的（考虑内存泄漏可能）。 落槽操作 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;JAVA8没有为落槽操作单独写一个方法，而是采用table[i = (n-1) * hash]来完成，其实这与JAVA7中的操作一样h &amp; (length-1)，都依赖2次幂的容量。但JAVA8利用按位与的特性巧（骚）妙（操）地（作）解决了开篇所说的死链问题，介绍扩容方法时将详细说明。 哈希函数 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;新的hash方法，对象hashCode与自身右移16位求异或，相比JDK7的实现“清爽”了许多， 我们知道int类型是32位，右移16位再异或，就相当于将高16位和低16位进行异或操作。 1234static final int hash(Object key) { int h; return key == null ? 0 : (h = key.hashCode()) ^ h &gt;&gt;&gt; 16;} 扩容方法 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;扩容方法resize()不仅支撑扩容，还会初始化容器（将容器初始化放到第一次调用位置，减少内存无效开销），与JAVA7一样，初始默认容量是16，加载因子0.75f，阈值=加载因子*容量，为了高效落位容量适中保持2次幂，扩容为2倍。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了解决开头说的并发死链问题，巧妙地利用落槽操作时用到的按位与操作特性保证了数据迁移的顺序问题，保证数据不会发生错位。介绍源码之前先简单的介绍一下原理： 说明：按位与会截断高位数据，只保留与低位一样的位数。 假设哈希码是1010 0101 1100 0011 1101 0100 1011 1001 假设，初始容量是capacity = 16，capacity - 1对应的二进制是 0000 0000 0000 0000 0000 0000 0000 1111 与哈希码按位与时，哈希码高位截断，只保留与容量位数一致，可以简写为： 1001 &amp; 1111，结果是1001 当容量扩容后capacity = 32, capacity -1 对应的二进制是 0000 0000 0000 0000 0000 0000 0001 1111 与哈希码按位与时，哈希码高位截断，只保留与容量位数一致，可以简写为： 11001 &amp; 11111，结果是11001 从两次的按位与结果来看，低位1001不变，只有高位1变化（11001），这个变化与具体的哈希码有关，但无非是0或者1，所以JAVA8扩容方法抓住这一特性使得：数据迁移时，如果rehash后高位是0，则该数据保持在原来桶的位置不变；如果高位是1，则该数据重新分配到新空间的桶中。这样就避免了rehash重新分配所有数据落槽位置，导致元素顺序引用发生改变，从而引起并发下的死链。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980final Node&lt;K,V&gt;[] resize() { Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; // 设定阈值 if (oldCap &gt; 0) { if (oldCap &gt;= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold } else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; // 初始化HashMap容量和阈值 else { // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); } threshold = newThr; @SuppressWarnings({\"rawtypes\",\"unchecked\"}) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) { for (int j = 0; j &lt; oldCap; ++j) { Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) { oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); // 官方备注“维持顺序” // hi前缀变量表示高位，lo前缀表示低位 else { // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do { next = e.next; // 注意是oldCap，而不是oldCap-1,此时与操作就是只看上面提到的二进制高位 // 二进制高位是0 if ((e.hash &amp; oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } // 二进制高位是1 else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); if (loTail != null) { loTail.next = null; newTab[j] = loHead; // 原来桶内 } if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; // 新分配空间桶内 } } } } } return newTab;} 至此，HashMap源码介绍完毕，新API将在未来lambda表达式中详细介绍（可能）（手动狗头）。","link":"/2020/05/07/4%20%E6%BA%90%E7%A0%81/Java%E6%BA%90%E7%A0%81%E2%80%94%E2%80%94HashMap%EF%BC%88%E4%BA%8C%EF%BC%89/"},{"title":"Java源码——ConcurrentHashMap（二）","text":"ConcurrentHashMap（二） &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Java7中ConcurrentHashMap采取了数据一致性和效率的折中办法——分段锁Segment，但为了进一步提高性能，Java8中完全抛弃了分段锁，采用CAS + synchronized方式解决并发问题。这是一种更加细粒度的加锁方式，直接针对哈希表的槽。 ReservationNode，对占位节点加锁，当执行compute方法和computeIfAbsent方法时使用。 TreeBin，并不保存实际红黑树，只是对红黑树所在桶进行读写锁维护，并指向红黑树的引用。 ForwardingNode，扩容转发节点，外部对原哈希槽的操作会转发到nextTable上。 ConcurrentHashMap属性 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374// 摘抄部分重要的属性// 同JDK8的HashMap，设定了树化和回退链表的阈值，唯一不同的是，在进行// 树化操作时，要求桶数组容量必须大于64static final int TREEIFY_THRESHOLD = 8;static final int UNTREEIFY_THRESHOLD = 6;static final int MIN_TREEIFY_CAPACITY = 64;// 桶数组，哈希表的具体数据承载结构transient volatile Node&lt;K,V&gt;[] table;// 扩容时的哈希表，扩容也是2倍增长且保持2次幂容量private transient volatile Node&lt;K,V&gt;[] nextTable;// 初始化和扩容控制// sizeCtl=-1， 正在进行初始化// sizeCtl=-n， 有n-1个线程正在扩容// sizeCtl=0， 默认值，使用默认容量进行初始化// sizeCtl&gt;0， 扩容需要用到的容量，即阈值private transient volatile int sizeCtl;// ForwardingNode节点的哈希值static final int MOVED = -1; // 红黑树根节点哈希值static final int TREEBIN = -2; // 数据节点的保存结构，同HashMapstatic class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final int hash; final K key; volatile V val; volatile Node&lt;K,V&gt; next;}// 扩容转发节点，当该节点置于桶中，外部对原来哈希表的操作会转移到nextTable上进行static final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; { final Node&lt;K,V&gt;[] nextTable; Node&lt;K,V&gt; find(int h, Object k) {...}}// 预置加锁节点，对桶内的第一个数据进行加锁static final class ReservationNode&lt;K,V&gt; extends Node&lt;K,V&gt; { ReservationNode() { super(RESERVED, null, null, null); } Node&lt;K,V&gt; find(int h, Object k) { return null; }}// 维护对桶内红黑树的读写所，保存红黑树节点的引用static final class TreeBin&lt;K,V&gt; extends Node&lt;K,V&gt; { TreeNode&lt;K,V&gt; root; volatile TreeNode&lt;K,V&gt; first; volatile Thread waiter; volatile int lockState; // values for lockState static final int WRITER = 1; // set while holding write lock static final int WAITER = 2; // set when waiting for write lock static final int READER = 4; // increment value for setting read lock ...}// 红黑树的数据存储节点static final class TreeNode&lt;K,V&gt; extends Node&lt;K,V&gt; { TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; ...} Java8中显式的取消了加载因此loadFactor，但并没有取消阈值的计算threshold = capacity * loadFactor，而是采用n - (n &gt;&gt;&gt; 2) 的方式代替，构造器同样支持传入自定义加载因子，但只会在初始化容器时使用 构造器 1234567891011public ConcurrentHashMap() {} // 空构造器，初始化放在第一次添加位置// 自定义容量，对容量向上取2次幂，对sizeCtl赋值public ConcurrentHashMap(int initialCapacity) { if (initialCapacity &lt; 0) throw new IllegalArgumentException(); int cap = ((initialCapacity &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(initialCapacity + (initialCapacity &gt;&gt;&gt; 1) + 1)); this.sizeCtl = cap;} put方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879public V put(K key, V value) { return putVal(key, value, false);}final V putVal(K key, V value, boolean onlyIfAbsent) { if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode()); // 哈希函数 int binCount = 0; // 记录相应链表长度 for (Node&lt;K,V&gt;[] tab = table;;) { Node&lt;K,V&gt; f; int n, i, fh; // 空哈希表初始化 if (tab == null || (n = tab.length) == 0) tab = initTable(); // CAS初始化 // tabAt是落槽操作 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) { // 如果落槽位置无数据，使用CAS放入新值，如果CAS失败说明产生并发初始化 if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; } // MOVED表示产生数据迁移，那么当前线程帮助进行数据迁移 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else { // 当前桶不为空，且指向头结点 V oldVal = null; // 对该位置的头结点加锁，进行put操作 // synchronized已经进行了优化，采取偏向所、轻量锁和重量锁的升级 synchronized (f) { if (tabAt(tab, i) == f) { // 再次确认当前落槽位置没有发生变化 if (fh &gt;= 0) { // 头结点的 hash 值大于 0，说明是链表， binCount = 1; // 链表长度计数器 // 遍历链表确认是覆盖还是新增，与HashMap同理 for (Node&lt;K,V&gt; e = f;; ++binCount) { K ek; if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) { oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; } Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) { pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; } } } // 红黑树 else if (f instanceof TreeBin) { Node&lt;K,V&gt; p; binCount = 2; // 调用红黑树的插值方法插入新节点 if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) { oldVal = p.val; if (!onlyIfAbsent) p.val = value; } } } } if (binCount != 0) { // 判断是否需要将链表树化 // 虽然树化阈值也是8，但与HashMap不同点在于，如果容量小于64则会对哈希表扩容， // 如果大于64才进行树化 if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; } } } addCount(1L, binCount); return null;} 这里可以看到Java8中采用CAS + synchronized方式保证线程安全，那为什么synchronized在HashTable中效率很低被放弃，但在这里又被重新启用？ 实际上此时JVM对synchronied进行了优化，不在是重量级的互斥锁而变成了可自动升级的锁。我们知道按照锁的效率级别可以分为：偏向锁、轻量级锁和重量级锁，synchronized也正是按照这样的升序级别进行不断升级。 在JVM字节码中，每个对象都有monitor隐藏参数，该参数是加锁的监视器，当初始化对象锁时monitor=1采用偏向锁，当线程重复进入加锁时会判断，如果当前线程已经存在锁则使用原有锁，这样就保证在低频并发时单线程的执行效率。如果产生并发则将偏向锁升级为轻量级锁，如果并发量增加则会进一步进化为重量级锁。按照Java8中的ConcurrentHashMap采用CAS方式就可以知道设计前提，认为并发并不总是频繁发生的，所以synchronized很少会进入重量级锁。 哈希函数spread &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;与Java8中的HashMap哈希函数类似，都是采用key的哈希码自身按照高16位和低16位异或，不同的时此处还进行了常量HASH_BITS的与运算，可以理解为计算更均匀平滑的哈希值，消除负哈希。 1234567static final int HASH_BITS = 0x7fffffff; // usable bits of normal node hashint hash = spread(key.hashCode()); // put中的方法static final int spread(int h) { return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS;} 初始化initTable &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过CAS来进行初始化，如果被其他线程初始化那么使用yield进行一次自旋，并在下次while循环中退出本次初始化，因为table时volitale的。 123456789101112131415161718192021222324private final Node&lt;K,V&gt;[] initTable() { Node&lt;K,V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) { // 被其他线程初始化 if ((sc = sizeCtl) &lt; 0) Thread.yield(); // lost initialization race; just spin // CAS操作，sizeCtl = -1，表示抢到了初始化的锁 else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) { try { if ((tab = table) == null || tab.length == 0) { int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; // 初始化数组 Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = tab = nt; sc = n - (n &gt;&gt;&gt; 2); // 阈值，其实等同于n*加载因子0.75 } } finally { sizeCtl = sc; // 哈希表容量控制器赋值，如果使用默认容量，该指为12 } break; } } return tab;} 显式的取消了加载因子loadFactor，采用 n - (n &gt;&gt;&gt; 2) 作为通过加载因子求阈值的替代。 扩容tryPresize &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;HashMap都一样，都是2倍扩容，但是在这里扩容操作稍显复杂，并且伴随着数据迁移的相关操作，整个过程需要sizeCtl参与和控制。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// sizeCtl=-1， 正在进行初始化// sizeCtl=-n， 有n-1个线程正在扩容// sizeCtl=0， 默认值，使用默认容量进行初始化// sizeCtl&gt;0， 扩容需要用到的容量，即阈值private final void tryPresize(int size) { // size已经是原容量的2倍值 // 假如size=32，那么c=64 int c = (size &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(size + (size &gt;&gt;&gt; 1) + 1); int sc; while ((sc = sizeCtl) &gt;= 0) { Node&lt;K,V&gt;[] tab = table; int n; // 同初始化原理 if (tab == null || (n = tab.length) == 0) { n = (sc &gt; c) ? sc : c; if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) { try { if (table == tab) { Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = nt; sc = n - (n &gt;&gt;&gt; 2); } } finally { sizeCtl = sc; } } } else if (c &lt;= sc || n &gt;= MAXIMUM_CAPACITY) break; else if (tab == table) { int rs = resizeStamp(n); if (sc &lt; 0) { // 有多个线程正在进行扩容 Node&lt;K,V&gt;[] nt; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; // CAS对sc+1操作并参与数据迁移 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); } // 没有其他线程进行扩容，当前线程进行扩容和数据迁移 else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); } }} 数据迁移transfer &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该方法应该是最难阅读的方法，目的是一个或多个线程协作从旧哈希表将数据迁移到新哈希表中，其原理是：假设原哈希表的容量是x，每个桶做数据迁移需要保持数据一致，可以将一个桶视作一个任务单元，所以有x个迁移任务，按照分治的思想，如果当前有y个线程参与数据迁移，就会把x个迁移任务拆分给每个线程去做。当某个线程完成迁移任务后，会检查是否还有未进行的迁移任务并参与其中。transferIndex用来调度安排线程执行的迁移任务。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) { int n = tab.length, stride; // stride可以理解为拆分任务的分片，与CPU（NCPU参数）有关 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range // 如果 nextTab 为 null，先进行一次初始化 if (nextTab == null) { try { Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; // 新的哈希表 nextTab = nt; } catch (Throwable ex) { sizeCtl = Integer.MAX_VALUE; return; } nextTable = nextTab; transferIndex = n; } int nextn = nextTab.length; // 如果当前桶正在被线程迁移，则会设置 ForwardingNode 进行加锁 ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); boolean advance = true; // 表示可以执行迁移操作 boolean finishing = false; // 表示完成当前迁移操作 // i 是位置索引，bound 是边界，注意是从后往前 for (int i = 0, bound = 0;;) { Node&lt;K,V&gt; f; int fh; while (advance) { int nextIndex, nextBound; if (--i &gt;= bound || finishing) advance = false; // 如果transferIndex小于等于0，说明每个任务都有线程正在执行 else if ((nextIndex = transferIndex) &lt;= 0) { i = -1; advance = false; } else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) { // CAS操作，获取迁移边界 bound = nextBound; i = nextIndex - 1; advance = false; } } if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) { int sc; // 完成数据迁移 if (finishing) { nextTable = null; table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); // 重新计算 sizeCtl return; } if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) { // 任务结束，方法退出 if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; // 完成迁移任务，设置标志位finishing=true finishing = advance = true; i = n; } } // 如果位置 i 处是空的，表示没有任何节点，该槽设置ForwardingNode，表示已迁移或正在迁移 else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); else if ((fh = f.hash) == MOVED) advance = true; else { // 对桶加锁，进行迁移工作 synchronized (f) { if (tabAt(tab, i) == f) { // 头结点的 hash 大于 0，说明是链表 Node&lt;K,V&gt; ln, hn; if (fh &gt;= 0) { // 具体迁移方法，原理同JAVA8的HashMap int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) { int b = p.hash &amp; n; if (b != runBit) { runBit = b; lastRun = p; } } if (runBit == 0) { ln = lastRun; hn = null; } else { hn = lastRun; ln = null; } for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) { int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); } setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); // 设置ForwardingNode表示迁移完成 // advance 设置为 true，表示该桶迁移完毕 advance = true; } else if (f instanceof TreeBin) { // 红黑树的迁移，原理同HashMap和以上 TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) { int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) { if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; } else { if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; } } ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; } } } } }} 数据迁移方法还是比较复杂的，具体的迁移方法与Java8的HashMap相同，困难主要体现在对多个线程参与数据迁移过程的控制，当然，理解了Doug Lea的设计原理，源码阅读也会相对明了一些。","link":"/2020/05/25/4%20%E6%BA%90%E7%A0%81/Java%E6%BA%90%E7%A0%81%E2%80%94%E2%80%94ConcurrentHashMap%EF%BC%88%E4%BA%8C%EF%BC%89/"},{"title":"Java源码——ConcurrentHashMap（一）","text":"ConcurrentHashMap（一） &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在并发编程领域，ConcurrentHashMap是最推崇使用的哈希式集合类型。JDK8对它进行了脱胎换骨的改造，大量运用了Lock-Free技术，从而减轻因锁的竞争导致的性能问题，该类涵盖了CAS、锁、volatile、链表、红黑树等知识点，是从官方学习Java并发编程的绝佳案例。 Lock-Free：是无锁编程(Non-Blocking Sync)的实现，对于共享数据不使用锁来控制并发访问（互斥锁等排它锁类型），而是多线程并行访问，旨在避免使用Lock带来的线程阻塞等性能问题，虽然无法代替Lock。 CAS（Compare And Swap）：是乐观锁的实现，应用于轻微冲突的并发场景，因为CAS在进行自旋操作时占用CPU较多，所以不适合高并发场景。在JUC的atomic包下大量用到了CAS操作，保证变量的原子性。它的原理是，首先获得值的内存地址、预期值和新值，每个线程在获取并更新的过程中先访问内存地址的预期值比较（Compare）是否一致，如果一致则更新为新值（Swap）；如果不一致，先对预期值轮询操作，比较之后达到预期值再更新新值或者退出。CAS需要避免发生ABA问题，可以通过版本号来增加比较条件进行解决。 经典的CAS用法来自于AtomicInteger的自增操作方法，如下： 1234567891011121314public final int getAndIncrement() { for(;;) { int current = get(); // 预期值 int next = current + 1; // 新值 if (compareAndSet(current, next)) { // 自旋 return current; } }}// unsafe来自native方法，支持直接调用硬件的原子性能力。public final boolean compareAndSet(int expect, int update) { return unsafe.compareAndSwapInt(this, valueOffset, expect, update);} ConcurrentHashMap的发展 我们知道HashTable是线程安全的字典集合Dictionary，但底层实现使用了性能极差的全互斥方式，所以已经被淘汰。 HashMap是非线程安全的Map集合，容易产生并发问题，特别是死链问题。 Collections.synchronizedMap(Map&lt;K,V&gt; m)方法是集合工具类对普通Map的同步代理类，原理是使用排它锁mutex来锁定对象。 JDK8以前的ConcurrentHashMap采用分段式锁设计，旨在平衡性能和线程安全，内部采用重入锁ReentrantLock进行并发控制，将每个HashEntry进行加锁管理。 JDK8之后的ConcurrentHashMap采用Lock-Free理念，取消了分段式锁，采用CAS和其他优化设计提高了并发能力并降低了冲突概率。 1 HashTable的线程安全&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;HashTable继承自Dictionary类，与HashMap结构大致相同，几个关键的不同点：1. HashTable的哈希函数使用key的哈希码，所以key不允许为null，否则会产生NPE；2. 落槽操作采用哈希表容量取余来完成，效率低，但不会强制要求哈希表容量是2次幂；3. 扩容都是2倍；4. HashTable是线程安全的，它采用重量级的synchronized对方法加锁，效率极低。 如上图，当线程1调用put方法向1号槽插入数据时会获得整个哈希表的锁，此时线程2同时调用put方法企图向2号槽插入数据就会被阻塞，直到线程1释放锁。很明显，并发put时，如果不是同一个槽位是可以不用对哈希表整体加锁的，效率极低。 123456// HashTable内都是全互斥锁public synchronized V put(K key, V value)public synchronized V get(Object key)public synchronized boolean contains(Object k)public synchronized boolean isEmpty()public synchronized V remove(Object key) 不同于在HashMap中已经介绍的fail-fast机制同步时如果数据不一致会直接抛出ConcurrentModificationException异常，HashTable采用fail-safe机制，其原理是先获得当前哈希表的副本，并对副本进行迭代，虽然不会受到同步修改的影响，但不能保证迭代的数据是最新的。 2 Collections.synchronizedMap的线程安全&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Collections提供了多个支持对普通集合同步化的代理类，包括List、Set、Map等，针对不同类型都实现了对应的内部代理类。 12345678910111213static class SynchronizedCollection&lt;E&gt; implements Collection&lt;E&gt;, Serializable {...}static class SynchronizedSet&lt;E&gt; extends SynchronizedCollection&lt;E&gt; implements Set&lt;E&gt; {...}static class SynchronizedSortedSet&lt;E&gt; extends SynchronizedSet&lt;E&gt; implements SortedSet&lt;E&gt;{...}static class SynchronizedList&lt;E&gt; extends SynchronizedCollection&lt;E&gt; implements List&lt;E&gt; {...}static class SynchronizedRandomAccessList&lt;E&gt; extends SynchronizedList&lt;E&gt; implements RandomAccess {...}private static class SynchronizedMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Serializable {...} static class SynchronizedSortedMap&lt;K,V&gt; extends SynchronizedMap&lt;K,V&gt; implements SortedMap&lt;K,V&gt; {...} 其主要实现是采用互斥锁mutex对对象加锁，在锁级别上与HashTable是一致的，之所以Collections会提供这样重量级的锁是为了保证如果业务涉及高度的数据安全性且性能要求不严苛的场景使用，稍后介绍的ConcurrentHashMap之所以采用CAS，前提是认为并发并不总是存在的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;内部类提供了两个构造器，默认和指定互斥锁对象，默认情况下互斥锁为当前对象this。 1234567891011SynchronizedMap(Map&lt;K,V&gt; m) { if (m==null) throw new NullPointerException(); this.m = m; mutex = this;}SynchronizedMap(Map&lt;K,V&gt; m, Object mutex) { this.m = m; this.mutex = mutex;} 实际调用方法采用指定集合对象的自身方法。 12345678910111213141516171819202122232425262728public int size() { synchronized (mutex) {return m.size();}}public boolean isEmpty() { synchronized (mutex) {return m.isEmpty();}}public boolean containsKey(Object key) { synchronized (mutex) {return m.containsKey(key);}}public boolean containsValue(Object value) { synchronized (mutex) {return m.containsValue(value);}}public V get(Object key) { synchronized (mutex) {return m.get(key);}}public V put(K key, V value) { synchronized (mutex) {return m.put(key, value);}}public V remove(Object key) { synchronized (mutex) {return m.remove(key);}}public void putAll(Map&lt;? extends K, ? extends V&gt; map) { synchronized (mutex) {m.putAll(map);}}public void clear() { synchronized (mutex) {m.clear();}} 3. Java7 ConcurrentHashMap的线程安全&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Java7中的ConcurrentHashMap使用了分段锁机制，内部类Segment是实现分段锁的对象，在集合初始化时会先初始化Segment数组，其数组容量与并发级别（concurrency level，或称为并发数）有关，默认并发级别为16与默认容量一致，也就是说默认支持16个线程的并发，默认级别可以通过构造器传入，但适中保持2次幂的值，这时为了使Segment锁可以均匀管理不同的集合，与HashMap落槽操作一样，使用并发级别减1按位与操作来定位分段锁。分段锁容量初始化后将不在扩容，而分段锁管理的HashMap可以继续扩容。逻辑结构如下图所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344// 主要的构造器实现// concurrencyLevel为并发级别，可手动传入，但会进行2次幂处理public ConcurrentHashMap(int initialCapacity,float loadFactor, int concurrencyLevel) { if (!(loadFactor &gt; 0) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); if (concurrencyLevel &gt; MAX_SEGMENTS) concurrencyLevel = MAX_SEGMENTS; int sshift = 0; int ssize = 1; // 使用ssize计算分段锁的个数，即2次幂处理 while (ssize &lt; concurrencyLevel) { ++sshift; ssize &lt;&lt;= 1; } // 假设使用默认值concurrencyLevel=ssize=16，sshift=4 // segmentShift=28 // segmentMask=15 this.segmentShift = 32 - sshift; this.segmentMask = ssize - 1; // initialCapacity是哈希表初始容量 if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; // 根据 initialCapacity 设置Segment可以管理多少个哈希表 // 假设initialCapacity=16，那么每个 Segment 可以管理1个哈希表，如果initialCapacity=32则可以管理2个 int c = initialCapacity / ssize; if (c * ssize &lt; initialCapacity) ++c; int cap = MIN_SEGMENT_TABLE_CAPACITY; while (cap &lt; c) cap &lt;&lt;= 1; // 创建 Segment 数组ss Segment&lt;K,V&gt; s0 = new Segment&lt;K,V&gt;(loadFactor, (int)(cap * loadFactor), (HashEntry&lt;K,V&gt;[])new HashEntry[cap]); Segment&lt;K,V&gt;[] ss = (Segment&lt;K,V&gt;[])new Segment[ssize]; // 往数组写入s0 UNSAFE.putOrderedObject(ss, SBASE, s0); this.segments = ss;} UNSAFE是直接通过java api调用底层CAS能力，只有是bootstrap类加载器加载的类才可以调用UNSAFE，否则只能通过反射来调用。 假设我们全部使用默认值来完成了容器的初始化，此时关键参数如下： Segment数组长度为16且不可扩容 segmentShift=32-4=28， segmentMask=16-1=15。 Segment数组只初始化了Segment[0]元素，其他元素还是null。 put操作 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分两部分，第一部分首先在数组中定位Segment，确认使用的分段锁对象。 123456789101112131415public V put(K key, V value) { Segment&lt;K,V&gt; s; if (value == null) throw new NullPointerException(); // 1. 计算 key 的 hash 值 int hash = hash(key); // 2. 类比落槽操作，通过hash无符号右移与掩码按位与，找到Segement数组j位置元素 int j = (hash &gt;&gt;&gt; segmentShift) &amp; segmentMask; // ensureSegment(j) 对 segment[j] 进行初始化 if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObject (segments, (j &lt;&lt; SSHIFT) + SBASE)) == null) s = ensureSegment(j); // 3. 插入新值到 槽 s 中 return s.put(key, hash, value, false);} 第二部分，再确定了分段锁对象后，对分段对象加独占锁，再在其中进行哈希表的put操作，类似与HashMap。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253final V put(K key, int hash, V value, boolean onlyIfAbsent) { // 4.先获取 segment 的独占锁 HashEntry&lt;K,V&gt; node = tryLock() ? null : scanAndLockForPut(key, hash, value); V oldValue; try { // 该 segment 管理的哈希表 HashEntry&lt;K,V&gt;[] tab = table; // 5.落槽并使用头插法插入数据 int index = (tab.length - 1) &amp; hash; HashEntry&lt;K,V&gt; first = entryAt(tab, index); // 同HashMap，检索桶内链表 for (HashEntry&lt;K,V&gt; e = first;;) { if (e != null) { K k; if ((k = e.key) == key || (e.hash == hash &amp;&amp; key.equals(k))) { oldValue = e.value; if (!onlyIfAbsent) { e.value = value; ++modCount; } break; } e = e.next; } else { // 如果不为 null，那就直接将它设置为链表表头；如果是null，初始化并设置为链表表头。 if (node != null) node.setNext(first); else node = new HashEntry&lt;K,V&gt;(hash, key, value, first); int c = count + 1; // 如果超过了该 segment 的阈值， segment的哈希表需要扩容 if (c &gt; threshold &amp;&amp; tab.length &lt; MAXIMUM_CAPACITY) rehash(node); else // 没有达到阈值，将 node 放到数组 tab 的 index 位置， setEntryAt(tab, index, node); ++modCount; count = c; oldValue = null; break; } } } finally { // 操作完毕，释放锁 unlock(); } return oldValue;} 初始化ensureSegment方法 12345678910111213141516171819202122232425262728private Segment&lt;K,V&gt; ensureSegment(int k) { final Segment&lt;K,V&gt;[] ss = this.segments; long u = (k &lt;&lt; SSHIFT) + SBASE; Segment&lt;K,V&gt; seg; if ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) { // 使用已初始化的 segment[0] 长度来初始化segment[k] Segment&lt;K,V&gt; proto = ss[0]; int cap = proto.table.length; float lf = proto.loadFactor; int threshold = (int)(cap * lf); // 初始化 segment[k] 管理的哈希表 HashEntry&lt;K,V&gt;[] tab = (HashEntry&lt;K,V&gt;[])new HashEntry[cap]; // 检查该Segment是否被其他线程初始化 if ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) { Segment&lt;K,V&gt; s = new Segment&lt;K,V&gt;(lf, threshold, tab); // 使用 CAS，当前线程成功设值或其他线程成功设值后，退出 while ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) { if (UNSAFE.compareAndSwapObject(ss, u, null, seg = s)) break; } } } return seg;} 加锁scanAndLockForPut &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;采用ReentrantLock可重入锁来显式的对Segment加锁。 1234567891011121314151617181920212223242526272829303132333435private HashEntry&lt;K,V&gt; scanAndLockForPut(K key, int hash, V value) { HashEntry&lt;K,V&gt; first = entryForHash(this, hash); HashEntry&lt;K,V&gt; e = first; HashEntry&lt;K,V&gt; node = null; int retries = -1; // 循环获取锁，ReentrantLock的tryLock while (!tryLock()) { HashEntry&lt;K,V&gt; f; if (retries &lt; 0) { if (e == null) { if (node == null) // 这里可能是因为 tryLock() 失败，所以该槽存在并发，不一定是该位置 node = new HashEntry&lt;K,V&gt;(hash, key, value, null); retries = 0; } else if (key.equals(e.key)) retries = 0; else e = e.next; } // 重试次数如果超过 MAX_SCAN_RETRIES，进入阻塞队列等待锁 else if (++retries &gt; MAX_SCAN_RETRIES) { lock(); // ReentrantLock的lock，加锁 break; } else if ((retries &amp; 1) == 0 &amp;&amp; // 如果发生并发新元素倍插入，那么重新执行 scanAndLockForPut方法 (f = entryForHash(this, hash)) != first) { e = first = f; retries = -1; } } return node;} 扩容rehash &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Segment数组本身初始化后不能扩容，只能针对Segment内部的哈希表进行扩容，扩容容量为2倍增长(满足2次幂容量)。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950private void rehash(HashEntry&lt;K,V&gt; node) { HashEntry&lt;K,V&gt;[] oldTable = table; int oldCapacity = oldTable.length; int newCapacity = oldCapacity &lt;&lt; 1; // aka 2倍 threshold = (int)(newCapacity * loadFactor); // 创建新哈希表 HashEntry&lt;K,V&gt;[] newTable = (HashEntry&lt;K,V&gt;[]) new HashEntry[newCapacity]; int sizeMask = newCapacity - 1; // 新的掩码 // 遍历原数组，将原数组位置 i 处的链表拆分到 新数组位置 i 或 i+oldCap 两个位置 for (int i = 0; i &lt; oldCapacity ; i++) { HashEntry&lt;K,V&gt; e = oldTable[i]; if (e != null) { HashEntry&lt;K,V&gt; next = e.next; // 假设原数组长度为 16，e 在 oldTable[3] 处，那么 idx 只可能是 3 或者是 3 + 16 = 19 int idx = e.hash &amp; sizeMask; if (next == null) newTable[idx] = e; else { HashEntry&lt;K,V&gt; lastRun = e; int lastIdx = idx; // 通过 for 循环找到一个 lastRun 节点，该节点后的所有元素放到一起 for (HashEntry&lt;K,V&gt; last = next; last != null; last = last.next) { int k = last.hash &amp; sizeMask; if (k != lastIdx) { lastIdx = k; lastRun = last; } } newTable[lastIdx] = lastRun; // 这些节点可能分配在另一个链表中，也可能分配到上面的那个链表中 for (HashEntry&lt;K,V&gt; p = e; p != lastRun; p = p.next) { V v = p.value; int h = p.hash; int k = h &amp; sizeMask; HashEntry&lt;K,V&gt; n = newTable[k]; newTable[k] = new HashEntry&lt;K,V&gt;(h, p.key, v, n); } } } } // 将新来的 node 放到新数组中链表的头部 int nodeIndex = node.hash &amp; sizeMask; node.setNext(newTable[nodeIndex]); newTable[nodeIndex] = node; table = newTable;} size()方法 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;size方法通过遍历获得当前分段对象的容量并求和就是ConcurrentHashMap的size，它会多次尝试（3次）非阻塞获得size大小，但如果期间被其他线程修改，（超过3次）则会对每个分段对象强制加锁进行容量获取，这就有点类似JVM的STW感觉。 12345678910111213141516171819202122232425262728293031323334353637383940public int size() { final Segment&lt;K,V&gt;[] segments = this.segments; int size; boolean overflow; // 判断是否整型值溢出 long sum; // 记录被线程修改的次数modCount long last = 0L; int retries = -1; // 自旋次数 try { for (;;) { // 如果自旋达到3次则强制给每个Segment加锁 if (retries++ == RETRIES_BEFORE_LOCK) { for (int j = 0; j &lt; segments.length; ++j) ensureSegment(j).lock(); // force creation } sum = 0L; size = 0; overflow = false; for (int j = 0; j &lt; segments.length; ++j) { // 定位一个segment，获得其修改次数和容量 Segment&lt;K,V&gt; seg = segmentAt(segments, j); if (seg != null) { sum += seg.modCount; int c = seg.count; if (c &lt; 0 || (size += c) &lt; 0) overflow = true; } } if (sum == last) break; last = sum; } } finally { // 如果尝试了三次说明segment被加锁，这里需要解锁 if (retries &gt; RETRIES_BEFORE_LOCK) { for (int j = 0; j &lt; segments.length; ++j) segmentAt(segments, j).unlock(); } } return overflow ? Integer.MAX_VALUE : size;} 因为每个Segment的哈希表容量都是整型，如果数据量足够大，那么多个Segment的size之和就可能超出Integer的最大范围，此时需要通过overflow变量记录，如果容量爆表则只返回Integer.MAX_VALUE。","link":"/2020/05/20/4%20%E6%BA%90%E7%A0%81/Java%E6%BA%90%E7%A0%81%E2%80%94%E2%80%94ConcurrentHashMap%EF%BC%88%E4%B8%80%EF%BC%89/"},{"title":"多线程基础（一）","text":"多线程基础（一） &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;目前的计算机硬件已经突破了摩尔定律，CPU的算力可以达到每秒百亿次甚至更高，个人PC可运行的软件包括操作系统在内，进程数可以是几十个，线程数甚至更多，为了更大程度的发挥计算机自身算力，提高系统效率，更多会采用多线程并发编程的方式实现。 1 进程、线程、并行、并发和同步&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;作为计算机理论的基础知识，进程，是资源分配的最小单位；线程，是CPU调度的最小单位；线程依赖进程而执行；进程资源相互之间是独立的，线程共享同一个进程内的资源。一个程序在计算机上启动并运行，它就需要获得资源，就是一个进程；这个程序在计算上如何被CPU调度运行，就是一个或多个线程。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CPU执行程序实际调用的是指令集，现代计算机基本都是多核CPU，这就允许创建更多的进程，而多个进程同时运行，称作并行；在一个进程内，多个线程同时运行，称作并发；多个线程由于共享进程资源，在并发执行时会竞争资源，需要采取策略避免因为资源竞争而产生脏数据等线程安全问题，这种策略就称为同步。 在线程同步执行过程中，可以对单个线程处理的资源独占加锁，其他线程需要等待独占锁被释放才能继续执行，这个等待的过程称为阻塞。 引用知乎一句话:(https://zhuanlan.zhihu.com/p/54297968) 有人的地方就有江湖， 有并发的地方就有资源竞争， 有竞争的地方就有线程安全， 有线程安全的地方就有同步， 有同步的地方就有锁。 2 多线程的状态和使用 创建状态，Java中创建线程的方式有三种：继承Thread类、实现Runnable接口、实现Callable接口。其中Callable接口属于具有回调结果并可以抛出异常的接口，其实现方法是call；而普通创建线程推荐使用Runnable接口，对外暴露接口少，实现方法是run()，其内部异常只能通过主线程调用setDefaultUncaughtExceptionHandler()来捕获。 就绪状态，执行start之后线程并不会立即执行，而是进入就绪状态等待CPU分配执行时间片。线程是“一次性消费”，并不能被多次执行start方法。 运行状态，线程执行run方法，但执行过程中会被各种原因打断进入阻塞状态。 阻塞状态，线程阻塞是多方面援引，包含主动阻塞，即调用sleep、yield、join等；等待阻塞，调用了wait需要通过notify唤醒；同步阻塞，当前线程执行的资源被其他线程独占。 结束状态，执行结束或异常退出，当前线程被释放或回归线程池。 使用Thread和Runnable创建线程 12345678910111213141516171819202122232425262728293031323334353637383940// 使用Thread执行public class RunTask extends Thread { @Override public void run() { System.out.printf(\"当前线程： %s 执行\", Thread.currentThread().getName()); } public static void main(String[] args) { RunTask runTask = new RunTask(); runTask.setName(\"runTask1\"); runTask.start(); }}// 使用Runnable执行public class RunTask implements Runnable { @Override public void run() { System.out.printf(\"当前线程： %s 执行\", Thread.currentThread().getName()); } public static void main(String[] args) { RunTask runTask = new RunTask(); Thread thread = new Thread(runTask); thread.setName(\"taskRun2\"); thread.start(); }}// 使用lambda执行Runnablepublic class RunTask { public static void main(String[] args) { new Thread(() -&gt; { System.out.printf(\"当前线程 %s 执行\", Thread.currentThread().getName()); }, \"runTask3\").start(); }} 使用Callable执行线程 123456789101112131415161718public class RunTask implements Callable&lt;String&gt; { @Override public String call() { System.out.printf(\"当前线程： %s 执行 \\n\", Thread.currentThread().getName()); return \"runTask4 over\"; } public static void main(String[] args) throws ExecutionException, InterruptedException { RunTask runTask = new RunTask(); FutureTask&lt;String&gt; futureTask = new FutureTask&lt;&gt;(runTask); new Thread(futureTask, \"runTask4\").start(); System.out.println(futureTask.get()); }}// output: // 当前线程： runTask4 执行 // runTask4 over 当然，在Java世界中多线程执行使用最广泛的还是线程池套件，稍后介绍。 3 线程安全机制&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;多线程并发下的线程安全是决定系统性能的重要因素，其核心理念是“要么只读，只要加锁”。在早期的Java版本中，经常需要手写线程池或线程安全代码，往往会造成严重的性能问题，随着Java引入java.util.concurrent包使得并发编程难度降低，且具有更优秀的性能。线程安全问题可以按照以下的顺序思考： 资源向线程内移动，全局的共享资源是产生线程安全的根本援引，并且每个线程在JVM中独立拥有栈帧，如果允许的话可以将资源设置为局部变量，从而屏蔽线程安全问题，这也是ThreadLocal的核心理念。 不可变对象设计，不可变对象因为外部无法修改，它永远只是只读的，所以它总是安全的，例如String类型和Number类型等。 使用线程安全的对象，java原生提供了很多线程安全的类，比如：集合领域使用concurrent下的集合ConcurrentXXX对象或者CopyOnWriteXXX对象等，或者Collections提供的对非线程安全集合创建互斥锁转变为线程安全；StringBuffer等等。 同步机制和锁，涉及并发修改需要考虑实现同步机制，比如synchronized或者锁Lock（ReetrantLock）等。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;回过头来，对线程安全下一个定义，这个定义没有官方的或者权威的定论，个人认为适时且恰当的定义来自《Java并发编程》中的描述：“当多个线程同时访问一个对象时，①如果不考虑这些线程在运行时环境下的调度和交替执行，②也不需要进行额外的同步，③或者在调用方进行任何其他的协调操作，调用这个对象的行为都可以获得正确的结果，那就称这个对象是线程安全的。” 4 多线程特性&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;原子性是多线程的重要特点之一，所谓原子性是指在JVM字节码中的一个执行指令，JVM可以保证一个指令的执行是原子的，但不保证多个原子的指令执行是安全的。与事务中的原子性概念类似，原子指令执行过程中要么全部成功，如果失败则全部退出。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;举例说明，赋值语句int i = 1;是典型的原子指令，而自增++i在字节码中需要分别执行”对i赋值给临时变量”、”数值加1”、”返回结果保存“，对应的指令是”ILOAD、IINC、ISTORE“，虽然每一步都是原子的，但是组合在一起失去了原子性，是非线程安全的。在java.util.concurrent.atom包下，定义了一批原子性的对象，如AtomicBoolean、AtomicInteger、AtomicLong、AtomicReference等等，其内部api涉及诸如自增之类的方法，通过CAS（稍后介绍）实现原子性。 再比如，long类型是8字节64位，在32位操作系统中，对long类型进行赋值就不具有原子性。 123456789101112131415// AtomicInteger的自增方法public final int getAndIncrement() { for(;;) { int current = get(); // 预期值 int next = current + 1; // 新值 if (compareAndSet(current, next)) { // 自旋 return current; } }}// unsafe来自native方法，支持直接调用硬件的原子性能力。public final boolean compareAndSet(int expect, int update) { return unsafe.compareAndSwapInt(this, valueOffset, expect, update);} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可见性，多线程在JVM内存中是基于内存共享的，因为不同的线程独占工作栈，当对共享对象进行修改需要重新刷新到主内存中，以便其他线程也可以基于最新的对象进行操作，当多个线程对同一个共享对象进行操作，容易造成内存同步不及时导致的线程安全问题。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;有序性，通常认为程序的运行是按照字节码顺序执行的，但实际上CPU会对信息进行指令优化，分析哪些动作可以合并进行，从而导致看上去后面的代码先执行了。但是可以保证整体顺序是不变的，不会造成由于优化引起的乱序现象。 5 内存模型和Volatile&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;每个线程都有独占的内存空间（操作栈、本地变量表等等）称为工作内存，同时还存在对所有线程开放的共享区域，称为主内存。线程在本地内存对共享对象修改后，会将结果同步到主内存中，既然有同步操作就会存在时间差，而再次期间的操作对于其他线程来说却是不可见的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Java内存模型（Java Memory Model，JMM）主要目的是定义程序中各种变量的访问规则，变量范围包括实例属性、方法属性等这些可以被共享的变量，而局部变量和方法参数等属于线程本地内存所有，是线程独占的。JMM同时规定：所有的变量保存在主内存中，工作内存保存线程使用的主内存中变量的副本，每个线程对变量的操作都必须在工作内存中完成，而不能直接读写主内存中的数据。 “不能直接读写主内存中的数据” 这一描述对volatile也不例外，虽然看上去volatile描述的对象修改就是在主内存中完成，但事实上，volatile是通过特殊的操作顺序来完成这一点（确保在读操作之前先执行写入操作），实际依然有工作内存的拷贝。 JMM 和 JVM内存区域如堆、栈、方法区等概念属于不同维度对内存的描述，实际上JMM主要是针对JVM在硬件访问层面的描述。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了更好的提供多线程可见性的特性，JMM专门为volatile定制了特殊的访问规则： 保证volatile描述的变量对所有线程的可见性，即保证一致性。 禁止指令重新排序优化。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对所有线程的可见性容易理解，它可以保证不同的线程修改共享变量后能够及时的刷新回内存中而对其他线程可见，从而保证了数据的一致性，但是，并不是volatile描述的变量就是线程安全的，这依赖于变量自身的原子性。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;前文已经提到过，字节码指令在计算机中被CPU调用时并不是严格按照顺序进行，而是会被重新排序进行合并优化，即只要保证结果正确性而不考虑执行顺序的乱序。禁止指令重排序其实是指在关键的动作之前，其依赖的动作已经完成，反应到volatile上就是共享变量在被线程B读取之前已经从线程A修改后的变量写入主内存中。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里提一下有名的单例模式创建机制——双检锁（Double Check Lock, DCL）,它是指在创建单例对象时，在加锁前后都判断对象是否为空，但DCL并不一定是线程安全的。 123456789101112131415161718// 双检锁机制的单例模式public class DCLSingleton { private volatile static DCLSingleton instance; private Object o = new Object(); private DCLSingleton() {} public static DCLSingleton getInstance() { if (instance == null) { // 第一次检查 synchronized (DCLSingleton.class) { if (instance == null) { // 第二次检查 instance = new DCLSingleton(); } } } }} 理论上，双检锁已经把单例创建发挥到极致，但为什么它还可能是线程不安全的？主要问题在于instance = new DCLSingleton();，这句话在字节码层面并不具有原子性，假如线程A创建单例时执行到new DCLSingleton();，此时对象已经在内存中分配了空间但并没有完全完成初始化（还没有创建Object对象），经过volatile修饰此时被刷新回了主内存，而线程B调用了getInstance方法返回的对象也是没有完全初始化的对象，从而引起线程安全问题。 6 Happern-Before原则&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Happen-Before原则又称为先行发生原则，它是JMM中关于两个操作之间的顺序关系的保证！比如A happen before B，就是说在B进行操作前，A的操作造成的影响可以被B观察到。用函数关于可以表示成 F(x,y) 其中 x 先行发生于 y 。如果有三个线程A、B、C，它们之间的函数关系是 F(A,B) 和 F(B,C)，那么可以推导出 F(A,C)是成立的。尽管会进行指令的重排序，到只要保证最终的happen-before规则，其中的重排序是可以被允许的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;“时间上的先发生”并不代表先行发生。 12345private int value;public int getValue() { return value;}public void setValue(int value) {this.value = value;} 假如线程A先执行setValue，线程B后执行getValue，虽然在时间上“A先于B执行”，但并不满足happen-before原则，所以它还是线程不安全的。然而，先行发生也不意味着就会是“时间上的先发生”，有以上描述可以得知，还有可能存在指令重排序。 7 ThreadLocal和引用类型引用类型 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Java中存在四种引用类型，主要适用于在GC机制不同阶段下的引用对象定义： 这里所说的GC机制基于HotSpot VM。 12// 强引用类型Object object = new Object(); 强引用类型，也就是最普通的引用类型，在从GC ROOTS开始对象可达性分析时，如果不在引用链上就会被GC当作垃圾对象回收。一般发生在 Minor GC 和 Major GC阶段。 12// 软引用类型SoftReference&lt;Object&gt; softObject = new SoftReference&lt;&gt;(new Object()); 软引用类型，是比强引用效果弱一些的引用类型，在JVM内存体积膨胀到即将OOM前，GC会统一清理软饮用类型对象，以获得更多的空间避免发生OOM。通常用于缓存中间数据，当OOM前清理了缓存会在后续重新建立，延长了系统运行时间。 12// 弱引用类型WeakReference&lt;Object&gt; weakObject = new WeakReference&lt;&gt;(new Object()); 弱引用类型，是比软引用效果更弱的引用类型，在GC执行Minor GC时就会清理的类型，由于Minor GC发生的频率大于Major GC且发生时间不定，所以被回收的时间也无法确定。通常用于保存容易在内存中消失的对象。 1234// 引用队列ReferenceQueue&lt;Object&gt; phantomQueue = new ReferenceQueue&lt;&gt;();// 虚幻引用类型PhantomReference&lt;Object&gt; phantomObject = new PhantomReference&lt;&gt;(new Object(), phantomQueue); 虚幻引用类型，是效果最弱的引用类型，在创建之后就无法获得引用对象，创建时还需要赋值引用队列ReferenceQueue，当GC回收虚幻引用时，会将该对象加入队列中再回收。它唯一应用的场景就是对象被回收时，通过引用队列被外界知道。 ThreadLocal &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ThreadLocal是一种新的多线程并发安全设计思想的实现，全局变量并发操作总是充满不安全性，那么如果将全局变量设计为线程私有副本呢？ThreadLocal内部通过ThreadLocalMap的哈希表结构来保存变量的副本，哈希表的key就是当前ThreadLocal对象，value是持有的变量副本，而每个ThreadLocalMap对象都是Thread对象私有的。 123456789101112131415161718192021222324252627282930313233// 源码public class Thread implements Runnable { ... ThreadLocalMap threadLocals; ...}public class ThreadLocal&lt;T&gt; { public void set(T value) { Thread t = Thread.currentThread(); ThreadLocal.ThreadLocalMap map = this.getMap(t); // 从当前线程获得ThreadLocalMap if (map != null) { map.set(this, value); // key为当前ThreadLocal对象，value是目标值 } else { this.createMap(t, value); } } public T get() { Thread t = Thread.currentThread(); ThreadLocal.ThreadLocalMap map = this.getMap(t);// 从当前线程获得ThreadLocalMap if (map != null) { // 获得当前对象的键值对，key为当前ThreadLocal对象,value是变量副本 ThreadLocal.ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) { T result = e.value; return result; } } return this.setInitialValue(); }} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当某个频繁操作需要一个临时对象，同时希望避免每次重新分配对象就可以采用这种方式，比如在开发数据库连接时，JDBC连接是容易产生并发问题的，可以将每个连接私有到当前处理的线程中，在通过ThreadLocal调用各自的连接获得不受干扰的执行结果并返回。当线程终止，这些值会被GC回收。 12345678910// 伪代码private static final String JDBC_URL = \"jdbc:///mysql\"private static ThreadLocal&lt;JDBCConnection&gt; connectionHolder = new ThreadLocal&lt;JDBCConnection&gt;(){ @Override public JDBCConnection initialValue() { return DriverManager.getConnection(JDBC_URL); }} 在多线程场景下，同一线程内的跨方法数据传递就需要使用ThreadLocal，如果没有使用它就必须通过返回值和参数形式，增加了相互间耦合度。 ThreadLocal弊端 内存泄漏，在ThreadLocal文档中要求一般声明ThreadLocal&lt;&gt;对象是private static的，通过GC可达性分析算法来看，静态变量会被选定为GC ROOT从而一直存在于内存当中，并不会随着线程结束而释放，所以在线程中使用完ThreadLocal之后必须调用remove()方法进行释放，否则就会在内存中堆积产生内存泄漏问题。 ThreadLocal产生内存泄漏的原因与Entry对象保存的key是否是弱引用WeakReference无关，因为如果key是强引用，当线程执行结束之后，虽然key在Minor GC之后会被回收，但Entry对象还存在于内存中。 脏数据，脏数据之所以会发生，可想而知，其根本原因在于绑定的线程Thread在执行一次之后并没有被销毁，而用于其他线程使用，此场景就是线程池的线程复用情况，假如之后线程不适用set()方法设置值，直接调用get()，那么就会得到之前线程执行后的值，造成脏数据。要解决脏数据问题，还是必须在使用完毕后调用remove()方法释放。 ThreadLocal和Synchronized区别 synchronized ThreadLocal 思路 采用对变量加锁的方式解决兵法问题，是一种“时间换空间”方式。 采用为每个线程提供一个变量的私有副本，来隔离不同线程间的影响，是一种“空间换时间”的方式。 处理方式 加锁解决同步问题。 1.共享数据隔离。2.同一线程内跨组件值传递。 8 Synchronized 和 锁&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上文中谈到了线程安全，那么实际操作中保证线程安全最常用的方式之一就是采用“互斥同步（Mutual &amp; Synchronized）”方式。 8.1 Synchronized8.1.1 synchronized的使用&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;synchronized关键字是最基础也是最常用的同步手段， 通过对一个实例对象或者类对象加锁以达到线程对方法的独占使用，对其他线程阻塞，直到持有锁的线程释放锁，阻塞队列里的线程才能再次竞争锁，即竞争同步方法的使用权。我们知道，synchronized不仅可以显式的指定加锁对象，也可以隐式的直接使用，如果不明确指定加锁对象，那么JVM会通过该方法判断（实例方法 or 类方法）使用当前对象的实例对象或者类对象。 1234567891011121314151617181920212223242526272829class SynchronizedDemo { private Object param; // 全局对象变量 private static int status = 0; // 全局类变量 public synchronized Object getParam() { // 锁定实例对象 return param; } public synchronized void setParam(Object param) { // 锁定实例对象 this.param = param; } public void handleParam() { synchronized (this) { // 锁定实例对象 param = new Object(); } } public static void statusAccumulate() { synchronized (SynchronizedDemo.class) { // 锁定类对象 status += 1; } } public static synchronized int getStatus() { // 锁定类对象 return status; } } 8.1.2 synchronized的字节码层实现原理&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;接下来，让我们从JVM底层字节码来分析synchronized的原理。当源码中使用synchronized锁定一段代码块时，字节码中表现为在该代码块前后添加monitorenter和monitorexit两个指令，这两个指令都依赖于一个明确指向的对象，即持有锁的对象（实例对象或者类对象）。当 JVM 执行monitorenter时，会先判断持有的锁对象是否存在，如果对像不存在则当前线程进入阻塞队列，等待锁被释放与其他阻塞线程竞争锁；如果对象存在，那么就会将锁的计数器递增1，执行monitorexit会将锁计数器递减1，直到锁计数器为0时，当前线程释放锁。也就是说synchronized是可重入的，也就存在线程自身的死锁情况发生。 8.1.3 synchronized的优化和原理&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在历史上JDK5及以前版本中，synchronized都是通过调用硬件层的完全互斥方式解决同步问题，这就遗留了两个主要的性能问题：1.互斥同步会阻塞其他线程，当阻塞线程过多时容易使CPU假死，严重影响系统吞吐率和故障率；2.JVM调用硬件线程能力，就需要不断的从用户态（JVM）和内核态（OS）之间切换，造成了浪费太少了系统压力。通常称这样的互斥同步方式是重量级同步，所以那时候的synchronized在并发数量递增过程中，性能会严重下降，在JDK6之后，HotSpot对并发做了大量优化，其中主要包括：偏向锁、轻量级锁、自适应自旋锁、锁消除、锁升级。 偏向锁 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;偏向锁的目的是，在无竞争环境中减少锁带来的性能开销，在对象的对象头中（对象头保存对象的元数据，包括哈希值、GC分代等信息）存在一个ThreadId属性，该属性用于捆绑调用线程的ID，在偏向锁状态下，如果当前线程访问了新的锁对象，那么会判断是否是同一个线程，即ThreadID是否一致，如果是同一个线程，则不会重复获得锁，从而提高了程序的运行性能；如果不是同一个线程，则会被认为存在竞争，会自动将锁对象由偏向锁升级为轻量级锁。 偏向（Biased），就是偏袒、偏护的意思，意味锁会偏向于第一个获得他的线程。 轻量级锁 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;轻量级锁是相对于传统互斥同步的“重量级锁”而言，它的使用前提是”大部分锁在同步周期内没有竞争“。当线程第一次持有锁对象时，会通过CAS更新持有状态，更新成功则会进入同步方法执行；更新失败表明存在其他线程竞争的状态，如果出现两个以上的锁竞争，那么就会自动升级为重量级锁。 自适应自旋锁 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在当代CPU多核普及的情况下，已经可以支持线程并行，并且并发的方法通常不会占用过长时间，为了避免用户态和内核态的频繁切换，可以将原本要阻塞的线程再持有时间片期间不阻塞，而是采取自旋的方式（死循环）等待前一个线程释放锁。由此看来，虽然节约了线程切换的开销，但也增加了CPU的运算时间，所以，”持锁时间越短，自旋效果越好；相反地，持锁时间越长，自旋效果越差。“因此控制自旋时间或者次数就很有必要，HotSpot默认自旋10次，如果超过10次还没有获得锁则会进入阻塞队列中。当然有个技巧，通常在自旋过程中会附加一些简单的程序用于处理一些业务。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;顾名思义，自适应自旋锁其实就是自旋锁的自旋次数可以满足自适应的能力，自适应的能力由JVM来判断和赋予，通常是通过判断同一个锁在前一次的自旋时间和持锁线程来决定：如果在同一个锁上自旋等待的时间成功获得了锁，并且持锁的线程正在运行，那么再下一次也被认为有可能成功获得锁，所以允许自旋的次数会增加；如果某个锁很少通过自旋获得，那下一次就直接跳过自旋的方式。 锁消除和锁粗化 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于即时编译器JIT来讲，代码本身有同步声明，但在逃逸分析过程中发现不存在共享数据，也就不会存在并发问题，因此编译时不会对该代码添加锁，这称为”锁消除“。当短时间内出现大量重复的锁，比如循环体内添加锁，编译时就会将锁的控制范围外扩至循环体外，这称为”锁粗化“。这两种技术都有效的避免了开发过程中或主观或客观存在的”低效率并发问题“。 8.2 锁对象Lock&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;JDK1.5新增了java.util.concurrent.Lock接口，旨在提供可以在Java类对象层面实现，并且更加灵活的同步方式。顾名思义，该接口就是“对象锁”，是一种源码级别灵活控制的同步方式，下图展现了Lock接口继承关系。 8.2.1 ReentrantLock&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ReentrantLock是Lock接口的重要实现类，称为”可重入锁“，其含义是指一个线程可以对同一个锁对象重复访问，访问一次锁的数量会自增，访问结束锁的数量会自减，当锁容量为0时释放该锁。可重入锁是synchronized的超集，在实现效果上与synchronized一致，只不过前者是代码级别的并发控制，而后者属于语法级别的并发控制。synchronized可以通过语法自动实现加锁和解锁操作，ReentrantLock可以使开发者灵活的加锁，并且需要主动在finally中释放锁，否则就会出现永远被加锁的情况。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;除此之外ReetranLock还支持以下主要功能： 可中断，当锁对象长时间被持有时，等待线程可以选择放弃从而处理其他事情（ tryLock() 方法）。 公平和非公平竞争，synchronized是典型的非公平竞争，当锁对象被释放之后，所有阻塞队列中的线程都可以竞争该锁。ReentrantLock不仅支持非公平竞争，还支持公平竞争，它是指锁对象被释放之后，阻塞队列中的线程按照时间顺序依次获得锁，但需要知道公平竞争将会产生性能问题影响系统吞吐率。ReentrantLock拥有私有属性Sync，而从上图中可以看到Sync的实现类包含公平锁FairSync和非公平锁UnFairSync，默认情况使用UnFairSync，也可以通过构造器来声明使用FairSync。 123456789101112// ReentrantLock重要属性和构造器public class ReentrantLock implements Lock, java.io.Serializable { private final Sync sync; public ReentrantLock() { sync = new NonfairSync(); } public ReentrantLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync(); }} 12345678910111213141516171819202122232425262728293031323334353637383940// synchronizedclass SynchronizedDemo { private Object param; public synchronized Object getParam() { return param; } public synchronized void setParam(Object param) { this.param = param; } public static void main(String[] args) { SynchronizedDemo demo = new SynchronizedDemo(); demo.getParam(); demo.setParam(new Object()); }}// ReentrantLockstatic class SynchronizedDemo { private Object param; public Object getParam() { return param; } public void setParam(Object param) { this.param = param; } public static void main(String[] args) { ReentrantLock lock = new ReentrantLock(); SynchronizedDemo demo = new SynchronizedDemo(); lock.lock(); demo.getParam(); demo.setParam(new Object()); lock.unlock(); }} 多条件绑定，synchronized通过wait()和notify()来实现一个关联条件，当需要更多的关联条件时就不得不添加更多的锁。ReentranLock只需要通过多次调用newCondition()方法即可满足。 8.2.2 多条件绑定——Condition对象&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Condition对象称为线程监视器，支持更加精准的线程调度动作。传统的线程调度通过Object对象的 wait() 方法和 notify() 或 notifyAll() 对象实现，这也是synchronized互斥锁支持的线程调度方式，但其中存在两个问题： notifyAll会唤醒当前所有阻塞线程竞争锁，是一种非公平竞争，唤醒粒度面向整个线程队列。 notify只会唤醒阻塞队列中的一个线程，置于唤醒哪一个线程则是由线程调度器决定，用户并不能指定线程唤醒，虽然唤醒粒度小但无法指定线程。 对于ReentranLock可以通过注册多条监视器来显式的、准确的实现线程调度，以下分别以简化版经典的生产者消费者问题对二者做以比较。 描述：假设有action1和action2两个动作，两动作需要前后依次循环执行。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354// synchronized版本实现public class SyncProducerAndConsumer { private static volatile AtomicInteger state = new AtomicInteger(0); // 调度状态，保证原子和可见 private static class Conductor { public void action1() throws InterruptedException { synchronized (this) { while (state.get() != 0) { // 状态不为0时等待 this.wait(); } // 状态为0时处理 System.out.println(Thread.currentThread().getName() + \" execute\"); // 执行结束前修改状态并唤醒其他线程 state.set(1); notifyAll(); // 此例中只有两个子线程，也可使用notify() } } public void action2() throws InterruptedException { synchronized (this) { while (state.get() != 1) { this.wait(); } System.out.println(Thread.currentThread().getName() + \" execute\"); state.set(0); notifyAll(); } } } public static void main(String[] args) { Conductor conductor = new Conductor(); new Thread(() -&gt; { try { for (int i = 0; i &lt; 10; i++) { conductor.action1(); } } catch (InterruptedException e) { e.printStackTrace(); } },\"Thread1\").start(); new Thread(() -&gt; { try { for (int i = 0; i &lt; 10; i++) { conductor.action2(); } } catch (InterruptedException e) { e.printStackTrace(); } },\"Thread2\").start(); }} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566// ReentrantLock实现public class LockProducerAndConsumer { private static volatile AtomicInteger state = new AtomicInteger(0); private static class Conductor { private final Lock lock = new ReentrantLock(); // 可重入锁 private final Condition action1Monitor = lock.newCondition(); // 注册action1监视器 private final Condition action2Monitor = lock.newCondition(); // 注册action2监视器 public void action1() throws InterruptedException { lock.lock(); try { while (state.get() != 0) { action1Monitor.await(); // 状态不为0，等待 } // 执行 System.out.println(Thread.currentThread().getName() + \" execute\"); state.set(1); action2Monitor.signal(); // 指定唤醒action2监视器的线程 } catch (Exception e) { e.printStackTrace(); } finally { lock.unlock(); } } public void action2() throws InterruptedException { lock.lock(); try { while (state.get() != 1) { action2Monitor.await(); } System.out.println(Thread.currentThread().getName() + \" execute\"); state.set(0); action1Monitor.signal(); } catch (Exception e) { e.printStackTrace(); } finally { lock.unlock(); } } } public static void main(String[] args) { Conductor conductor = new Conductor(); new Thread(() -&gt; { try { for (int i = 0; i &lt; 10; i++) { conductor.action1(); } } catch (InterruptedException e) { e.printStackTrace(); } },\"Thread1\").start(); new Thread(() -&gt; { try { for (int i = 0; i &lt; 10; i++) { conductor.action2(); } } catch (InterruptedException e) { e.printStackTrace(); } },\"Thread2\").start(); }} 总结： Synchronized使用 wait() 、notify()、notifyAll()方法实现线程调度。调度粒度大，无法指定固定线程的调度。 ReentranLock使用Condition提高的await()、signal()、signalAll()方法实现线程调度，可以准确唤醒指定线程。 8.2.3 Lock-Free：CAS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;笼统的从策略来讲，锁可以分为以 synchronized 和 reentrant lock 为代表的互斥阻塞式锁，称为悲观锁，悲观锁总是认为如果不采取同步措施那就一定会发生并发安全问题，所以无论变量是否存在竞争都加锁；另一类是以CAS为代表的非阻塞式锁，称为乐观锁，乐观锁认为无论如何先执行操作，如果操作的结果不符合预期再考虑补救措施。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CAS （Compare and Swap），是一种Lock Free编程类型，通过编码的形式保证并发安全，这个概念不仅应用于硬件层面，同样应用于软件层面，比如通过编程语言从源码层面解决，或者某些数据库也支持保证数据一致性。CAS执行包含三个要素，数据的内存位置M、预期值P、新值N，当且仅当M的数值与P相等才会更新新值N，否则就不执行更新操作，可以通过有限制次数的自旋来重试，直到更新成功或者退出。整个过程是具有原子性的，不会被其他线程中断。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在Java9以前，Java API支持的CAS属于sun.misc.Unsafe，只有通过Bootstrap类加载器加载的类才有使用权限，因此类库中的类比如JUC可以使用，用户类可以通过反射机制获得访问和调用权限，Java9面向用户开放了VarHandle类用于操作CAS。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;再前篇多线程特性——原子性时已经介绍过AtomicInteger类中自增操作使用CAS，这里就不再獒述。 8.3 死锁&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;设计不当的多线程任务很容发生死锁现象，而且死锁的发生通常意味着系统资源走向枯竭出现假死甚至宕机现象。死锁的发生主要是因为多线程在持有锁的执行过程中阻塞获得其他的锁，此时这个锁被其他线程持有并且尝试获得各自持有的锁。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class DeadLockDemo { public static void main(String[] args) { Object obj1 = new Object(); Object obj2 = new Object(); // 线程t1持有obj1对象锁，并尝试获得obj2对象锁 new Thread(() -&gt; { synchronized (obj1) { try { System.out.println(Thread.currentThread().getName() + \" get obj1\"); System.out.println(Thread.currentThread().getName() + \" try to get obj2,waiting\"); TimeUnit.SECONDS.sleep(1); synchronized (obj2) { System.out.println(Thread.currentThread().getName() + \" get obj2\"); } } catch (InterruptedException e) { e.printStackTrace(); } } }, \"t1\").start(); // 线程t2持有obj2对象锁，并尝试获得obj1对象锁 new Thread(() -&gt; { synchronized (obj2) { try { System.out.println(Thread.currentThread().getName() + \" get obj2\"); System.out.println(Thread.currentThread().getName() + \" try to get obj1,waiting\"); TimeUnit.SECONDS.sleep(1); synchronized (obj1) { System.out.println(Thread.currentThread().getName() + \" get obj1\"); } } catch (InterruptedException e) { e.printStackTrace(); } } }, \"t2\").start(); }}// 最终t1和t2产生死锁，系统进入无限阻塞/* * output:t1 get obj1t1 try to get obj2,waitingt2 get obj2t2 try to get obj1,waiting */ 通常解决死锁的方式，要么采用乐观锁通过有限次数的自旋解决；要么对互斥锁设置超时时限，超时的线程释放锁并重新进入阻塞队列。","link":"/2020/05/27/5%20%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E5%A4%9A%E7%BA%BF%E7%A8%8B%EF%BC%88%E4%B8%80%EF%BC%89/"},{"title":"多线程基础（二）","text":"多线程基础（二） 9 线程池及其源码&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在传统多线程创建中，用户通过Thread、Runnable或者Callable手动创建线程并执行，这样做会产生两个重要的问题： 线程的创建和销毁是会造成系统资源消耗的，如果过度创建线程执行有可能造成“过度切换”问题。 线程是重要资源，过度的创建不仅使执行效率降低，还容易引起内存消耗增加的问题，对于重要的大对象创建，通常采用“池”的概念来管理，达到线程复用的目的。 9.1 Executor&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Executor是java.util.concurrent下的重要异步执行接口，它的继承关系如下： 其定义了一个接收Runnable对象的方法executor，,该方法接收一个Runable实例，它用来执行一个任务，任务即一个实现了Runnable接口的类。 123public interface Executor { void execute(Runnable command);} 9.2 ExecutorService&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ExecutorService是扩展子接口，提供了线程生命周期相关的方法。 12345678public interface ExecutorService extends Executor { void shutdown(); // 平滑关闭，收回线程池 boolean isShutdown(); //判断关闭 // 提交线程，返回Future &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task); Future&lt;?&gt; submit(Runnable task); ...} submit支持Callable和Runnable执行并返回Future对象，通过Future对象内的方法可以感知和获得执行结果。 1234567891011121314public interface Future&lt;V&gt; { // 中断任务运行 boolean cancel(boolean mayInterruptIfRunning); boolean isCancelled(); // 判断是否线程执行结束有返回结果 boolean isDone(); // 获得返回结果 V get() throws InterruptedException, ExecutionException; // 带超时的返回结果 V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;} 需要说明的是，如果使用get方法直接获取Future结果，调用线程将会阻塞，直到执行线程执行结束。可以使用isDone非阻塞式的监听执行情况并最后通过get获得结果。 9.3 Executors&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Executors类，提供了一系列工厂方法用于创建线程池，返回的线程池都实现了ExecutorService接口。主要包含以下几种类型： Executors.newCachedThreadPool();，创建可以缓存的线程池，如果有空闲线程则进行复用，否则再未达到线程个数阈值时创建新的线程。常用于创建生命周期短的任务线程，如果空闲池中的线程超过空闲阈值（默认60s）则会被线程池关闭并清理。 Executors.newFixedThreadPool(int);，创建固定线程个数的线程池，原理基本与newCachedThreadPool类似，只不过在线程不足时不会再次创建线程（不存在空闲池概念），内部通过阻塞队列来管理调用线程。 Executors.newScheduledThreadPool(int)；，创建一个具有调度能力的线程池，其线程可以实现定时任务等，相比于Timer更加安全且功能丰富。 Executors.SingleThreadExecutor()；，创建一个单例线程，即任意时间线程池中只存在一个线程，按照任务提交顺序执行，调用之后即被销毁。 这些静态工厂方法虽然可以创建不同类型的线程池，但通过源码可以看到，都是调用ThreadPoolExecutor来构造目标线程池。 1234567891011121314151617181920212223242526272829303132public class Executors { // 非调度能力的都是通过ThreadPoolExecutor来构造目标线程池 public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()); } public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;())); } public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;()); } // 调度能力通过构造ScheduledThreadPoolExecutor构造 public static ScheduledExecutorService newSingleThreadScheduledExecutor() { return new DelegatedScheduledExecutorService (new ScheduledThreadPoolExecutor(1)); } public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) { return new ScheduledThreadPoolExecutor(corePoolSize); }} 9.4 ThreadPoolExecutor&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在孤尽的《阿里巴巴Java开发手册》并发编程规约的第4条指出：“线程池不允许使用Executors创建，而是通过ThreadPoolExecutor的方式创建，这样的好处是，可以使开发人员更加明确线程池的运行规则，规避资源耗尽的风险”，从企业安全风险把控角度来看，这样的规约是严谨且可被接纳的，当然如果开发人员对原理“烂熟于心”，可以通过Executors快速创建线程池也未尝不可。 构造器 线程池在构建时需要注意以下参数：核心线程数corePoolSize、最大线程数maximumPoolSize、心跳周期keepAliveTime、心跳周期时间单位TimeUnit、维护线程调度的阻塞队列BlockingQueue&lt;Runnable&gt;、线程工厂类ThreadFactory和拒绝策略RejectedExecutionHandler。 1234567891011121314151617181920212223242526272829// 构造函数public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler); // 自定义参数构建线程池BlockingQueue queue = new LinkedBlockQueue(10);// 线程工厂UserThreadFactory threadFactory = new ThreadFactory((task) -&gt; { String threadName = getThreadName(); Thread thread = new Thread(task,threadName); return thread;});// 自定义符合业务能力的拒绝策略UserRejectHandler rejectHandler = new RejectedExecutionHandler((task, executor) -&gt; { // 具体拒绝策略实现});// 或者使用提供的拒绝策略RejectedExecutionHandler rejectHandler = new ThreadPoolExecutor.AbortPolicy();// 默认策略// 核心线程数5、最大线程数20、心跳周期60sThreadPoolExecutor threadPool = new ThreadPoolExecutor(5, 20, 60, TimeUnit.SECONDS, queue, threadFactory, rejectHandler ); 拒绝策略 当不需要用户自定义拒绝策略时，可以使用ThreadPoolExecutor提供的拒绝策略： ThreadPoolExecutor.AbortPolicy：默认策略，丢弃任务并抛出异常RejectedExecutionException。 ThreadPoolExecutor.DiscardPolicy：丢弃任务不抛出异常。 ThreadPoolExecutor.DiscardOldestPolicy：丢弃等待队列中时间等待最长的任务，加入新任务。 ThreadPoolExecutor.CallerRunsPolicy：调用run方法时绕过线程池直接执行。 阻塞队列 阻塞队列是并发编程的重要数据结构，主要用于保存工作线程Worker，在后续详细介绍，此处列出常用阻塞队列实现对象： LinkedBlockingQueue：单向FIFO，因为基于链表实现，所以在队列频繁出队、入队过程中效率高所以是最常用的阻塞队列，当不显式定义该队列对象容量时，默认大小为Integer.MAX_VALUE。 ArrayBlockingQueue：单向FIFO，基于数组实现的有界队列，由于出队、入队涉及数组移位所以效率不如阻塞队列，当大容量时会严重影响吞吐量。 PriorityBlockingQueue：基本数据结构同LinkedBlockingQueue，区别在于队列内部排序（即调度优先级）由比较器Comparator维护。 SynchronousQueue：同步队列，也是“生产者-消费者”模型队列，产生的队列元素入队之后需要被另一端消费出队从而完成一个周期。 线程池对象状态属性 123456789101112131415161718192021// 将整型32位ctl按位拆分，高3位用于表示线程池状态，低29位用于表示工作线程数// 当线程数超过整型低29位可表示范围时，采用长整型AtomicLong代替// 比如，高三位2进制可以表示0~7不同的值private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));private static final int COUNT_BITS = Integer.SIZE - 3;private static final int COUNT_MASK = (1 &lt;&lt; COUNT_BITS) - 1;// runState is stored in the high-order bitsprivate static final int RUNNING = -1 &lt;&lt; COUNT_BITS; //private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;private static final int STOP = 1 &lt;&lt; COUNT_BITS;private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS;// Packing and unpacking ctl// 取反、与操作，获得当前线程池状态private static int runStateOf(int c) { return c &amp; ~COUNT_MASK; }// 与操作，获得工作线程数private static int workerCountOf(int c) { return c &amp; COUNT_MASK; }// 高3位和低29位或运算，合并成一个值private static int ctlOf(int rs, int wc) { return rs | wc; } ctl高3位表示了线程池的运行状态，包括： RUNNING：运行状态，接收新任务，111； SHUTDOWN：关闭状态，不接收新任务但可以继续执行队列内已有任务，000； STOP：停止状态，拒接接收新任务并中断所有任务，001； TIDYING：整顿状态，表示所有任务已被终止，010； TERMINATED：结束状态，表示已清理完所有现场，011； execute方法 execute方法是顶级接口Executor中唯一定义的方法，也是执行Runnable线程的方法。通过官方注释首先明确处理步骤： 如果运行的线程少于corePoolSize，会将入参任务通过调用addWorker启动执行，当然addWorker会检查运行状态和数量从而通过返回false防止在不应该添加线程的情况下添加线程。 当任务被添加进入工作队列，为了避免在第一次检查之后线程销毁或者线程池关闭，所以需要进行双重检查以确保进入工作队列，否则会执行拒绝策略。 如果无法添加任务进入工作队列，那么就执行拒绝策略。 12345678910111213141516171819202122public void execute(Runnable command) { if (command == null) throw new NullPointerException(); int c = ctl.get(); // 步骤1 if (workerCountOf(c) &lt; corePoolSize) { if (addWorker(command, true)) return; c = ctl.get(); } // 步骤2 if (isRunning(c) &amp;&amp; workQueue.offer(command)) { int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); } // 步骤3 else if (!addWorker(command, false)) reject(command);} addWorker方法 addWorker是线程池的重要方法，首先检查是否可以添加新任务，如果可以添加并最终添加成功会返回true；当线程池状态不为RUNNING或者县城创建失败会返回false。 特别说明：源码中用到了改进后的goto语句，关于goto语句争论很多，但需要说明的是虽然它弊端很多，但在多重循环中，恰当的使用可以快速跳过不必要的操作，节省时间提高效率，这也是为什么Java作为后起之秀依然没有彻底放弃的原因，反而进行了改进，我个人支持恰当的使用goto，虽然这样违反编码规约（微笑脸）。 （源码中都使用了，我在工作中是不是也可以使用的挺直腰杆，理直气壮 [doge]） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273private final HashSet&lt;Worker&gt; workers = new HashSet&lt;&gt;(); // 工作线程容器private final ReentrantLock mainLock = new ReentrantLock(); // 线程池主锁，敏感操作时使用/** * @Param firstTask:外部启动线程时构造的第一个线程 * @Param core:为true时表示新增工作线程，判断RUNNING状态下线程数是否达到corePollSize * 为false时表示新增工作线程，判断RUNNING状态下线程是否少与maximumPoolSize */private boolean addWorker(Runnable firstTask, boolean core) { retry: for (int c = ctl.get();;) { // 判断运行状态是否为RUNNING if (runStateAtLeast(c, SHUTDOWN) &amp;&amp; (runStateAtLeast(c, STOP) || firstTask != null || workQueue.isEmpty())) return false; for (;;) { // 最大线程数不能超过2^29，否则低29位无法保存线程 if (workerCountOf(c) &gt;= ((core ? corePoolSize : maximumPoolSize) &amp; COUNT_MASK)) return false; // CAS增加工作线程数 if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // 反复读取状态判断 // 如果线程池SHUNTDOWN，跳转至retry标签执行 if (runStateAtLeast(c, SHUTDOWN)) continue retry; } } // 开始创建工作线程 boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try { w = new Worker(firstTask); // 使用线程工厂创建线程并封装进Worker对象中 final Thread t = w.thread; if (t != null) { // 敏感操作，持有线程池主锁，避免添加和启动时产生并发 final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { int c = ctl.get(); // 当属于RUNNING状态或者STOP状态下firstTask为空，说明需要添加工作线程 if (isRunning(c) || (runStateLessThan(c, STOP) &amp;&amp; firstTask == null)) { if (t.isAlive()) // 如果添加工作线程时，该线程已经被创建则抛出异常 throw new IllegalThreadStateException(); workers.add(w); // 设定线程池最大并发个数 int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; } } finally { mainLock.unlock(); // 新增完毕，解锁 } // 工作线程添加成功，启动线程 if (workerAdded) { t.start(); workerStarted = true; } } } finally { if (! workerStarted) addWorkerFailed(w); } return workerStarted;} 最后，总结一下使用线程池的注意点： 必须通过线程池创建和调用线程，不能自定义线程执行，方便维护和管理。 根据实际业务场景使用ThreadPoolExecutor构建线程池。 对每个线程应该赋名，方便jstack分析。 9.5 线程异常捕获&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在子线程中抛出的异常是无法被主线程探知的，需要通过UncaughtExceptionHandler线程异常处理接口来捕获异常并进行处理。 12345678910111213141516171819202122232425262728public class Test { public static void main(String[] args) { Thread t = new Thread(() -&gt; { // 主动抛出异常 throw new RuntimeException(\"sub Thread excute error\"); }, \"subThread\"); // 线程启动前设置异常捕获器 t.setUncaughtExceptionHandler(new DefinedUncaughtExceptionHandler()); t.start(); try { TimeUnit.SECONDS.sleep(1); // 主线程sleep 1s } catch (Exception e) { e.printStackTrace(); } System.out.println(\"main thread over\"); } // 自定义线程异常捕获器 private static class DefinedUncaughtExceptionHandler implements Thread.UncaughtExceptionHandler { @Override public void uncaughtException(Thread t, Throwable e) { System.out.println(t.getName() + \" find Exception：\" + e.getMessage()); } }}","link":"/2020/05/27/5%20%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E5%A4%9A%E7%BA%BF%E7%A8%8B%EF%BC%88%E4%BA%8C%EF%BC%89/"},{"title":"多线程基础（三）","text":"多线程基础（三） 10 AQS和JUC简介10.1 AQS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AbstracQueuedSynchronizer类（简称AQS）是一个用于构建锁和同步器的抽象类，JUC中的很多同步器实现就是以AQS为基础创建而来。它解决了在实现同步器时涉及的大量细节问题，比如等待队列使用FIFO、获取操作和释放操作等。基于AQS构建的同步器不仅极大地减少了实现工作，而且也不必处理竞争问题，它只能在一个时刻发生阻塞，从而降低上下文切换开销，提高吞吐量。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AQS还提供管理同步器中的状态，该状态是一个整型值，可以又具体的实现器赋予状态的含义：比如ReentrantLock中用状态表示线程获取锁的次数，比如Semaphore表示剩余信号量，比如FutureTask表示任务状态等。该状态是volatile的，可以通过getState()、setState()和compareAndSetState()进行操作。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AQS定义了两种资源共享方式，即：Exclusive独占方式，如ReentranLock和Share共享方式如JUC中的Semaphore、CountDownLatch等，不同的同步器实现采用的资源共享方式不同。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AQS内部采用CLH队列实现阻塞队列，CLH（Craig，Landin，and Hagersten）队列是一个虚拟的双向队列，即没有队列实体，而是通过定义内部类Node节点来实现锁的分配。 AQS获取和释放 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当有新的线程需要竞争资源，首先通过获取state的方式竞争，如果成功持有状态则直接调用线程方法；如果持有失败则进入CLH双向队列末尾等待。AQS采用模板方法设计模式，在抽象类中提供 aquire() 方法执行该过程，而首次尝试获得state的 tryAcquire() 方法则交给具体实现的同步器。 123456789101112131415161718192021222324252627public final void acquire(int arg) { if (!tryAcquire(arg) &amp;&amp; // 尝试获得state,获得成功返回true，否则返回false acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) // 没有获得state，则进入阻塞队列末尾并再次获得锁 selfInterrupt(); // 最终没有获得锁，中断当前线程，等待阻塞队列的调度}// 交由具体同步器的实现类完成protected boolean tryAcquire(int arg) { throw new UnsupportedOperationException();}private Node addWaiter(Node mode) { Node node = new Node(mode); for (;;) { Node oldTail = tail; if (oldTail != null) { node.setPrevRelaxed(oldTail); if (compareAndSetTail(oldTail, node)) { // 将当前节点假如CLH队列末尾 oldTail.next = node; return node; } } else { initializeSyncQueue(); } }} 接下来可以看看ReentranLock的非公平锁和Semaphore是如何实现 tryAcquire() 方法的： 12345678910111213141516171819202122232425262728293031323334353637383940414243/* ReentranLock */// Sync extends AbstractQueuedSynchronizerstatic final class NonfairSync extends Sync { private static final long serialVersionUID = 7316153563782823691L; protected final boolean tryAcquire(int acquires) { return nonfairTryAcquire(acquires); }}@ReservedStackAccessfinal boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (compareAndSetState(0, acquires)) { // CAS设置state状态 setExclusiveOwnerThread(current); // 如果成功，则将当前线程设为互斥锁 return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(\"Maximum lock count exceeded\"); setState(nextc); return true; } return false;}/* Semaphore */public boolean tryAcquire() { return sync.nonfairTryAcquireShared(1) &gt;= 0;}final int nonfairTryAcquireShared(int acquires) { for (;;) { int available = getState(); int remaining = available - acquires; if (remaining &lt; 0 || compareAndSetState(available, remaining)) return remaining; }} 自定义同步器的实现 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;自定义实现一个锁，该锁内部维护一个私有的同步器对象，锁的实现是简写，主要体现同步器的实现。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class DefinedLock implements Lock { private final Synchronizer sync; public DefinedLock() { sync = new Synchronizer(); } // lock和unlock属于简写，这里只为强调自定义同步器 // state = 1表示获得，0释放。 @Override public void lock() { sync.acquire(1); } @Override public void unlock() { sync.release(0); } // Lock接口其他实现方法，省略 // 推荐使用内部同步器对象 private class Synchronizer extends AbstractQueuedSynchronizer { @Override protected boolean tryAcquire(int arg) { boolean flag = false; if (compareAndSetState(0, arg)) { // CAS给state赋值 setExclusiveOwnerThread(Thread.currentThread()); // 当前线设置为独占锁 flag = true; } return flag; } @Override protected boolean tryRelease(int arg) { if (isHeldExclusively()) { // 判断当前线程是否是独占锁的线程 setState(arg); setExclusiveOwnerThread(null); } return false; } @Override protected boolean isHeldExclusively() { return getExclusiveOwnerThread() == Thread.currentThread(); } }} 10.2 JUC&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;JUC（java.util.concurrent）指的是jdk5之后官方提供的同步包，其中包含了众多实际开发中用到的概念，包括： 锁Lock及其实现类； AQS及其衍生实现； 原子特性对象如AtomicInteger、AtomicLong等； 并发集合如ConcurrentHashMap等(同步集合)、CopyOnWriteList等(COW集合)； 线程池Executor及其实现类ThreadPoolExecutor、ExecutorService等。 阻塞队列BlockingQueue及其实现类。 可以说JUC是现代多线程并发开发的集大成者，前文已经陆续介绍了JUC的相关组件（锁、原子特性、线程池、AQS、并发集合跳转“源码标签”或”Java源码系列收看”）。接下来最后简要介绍常用的并发器工具类：CountDownLatch、Semaphore、CyclicBarrier 10.2.1 CountDownLatch&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（官方解释）允许一个或多个线程等待直到在其他线程中执行的一组操作完成的同步辅助。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CountDownLatch是一个基于执行时间的同步类，其中通过维护一个全局的 count 变量计数，其操作都具有原子性，主要通过 countDown() 和 await() 两个方法实现。 从字面意思理解，CountDown就是整数自减，直到count=0，Latch就是门闩的意思，意味着count=0时上闩关门。 CountDownLatch是一次性使用的，当count=0时，对象即不可复用，需要重新创建。 使用CountDownLatch的await方法可以保证多个不同步线程最终可以统一回归到一个时间点继续向下执行。 12345678910111213141516171819202122232425262728293031323334353637383940414243public class CountDownLatchDemo { private static final Random random = new Random(); public static void main(String[] args) throws InterruptedException { // 初始计数10 CountDownLatch countDownLatch = new CountDownLatch(10); // 创建10个线程，每个线程执行完毕，计数器自减 for (int i = 1; i &lt;= 10; i++) { new Thread(() -&gt; { try { TimeUnit.SECONDS.sleep(random.nextInt(5)); // 每个线程休眠随机时间秒 System.out.println(Thread.currentThread().getName() + \" execute done\"); countDownLatch.countDown(); } catch (InterruptedException e) { e.printStackTrace(); } }, \"Thread-\" + i).start(); } // 等待count=0，即10个线程执行完毕，此时主线程阻塞 countDownLatch.await(); // 主线程阻塞结束，所有线程都执行完毕，开始继续执行主线程 System.out.println(\"count = \" + countDownLatch.getCount()); System.out.println(\"main thread end\"); }}/* * output:Thread-8 execute doneThread-4 execute doneThread-6 execute doneThread-5 execute doneThread-1 execute doneThread-2 execute doneThread-3 execute doneThread-10 execute doneThread-7 execute doneThread-9 execute donecount = 0main thread end */ 10.2.2 Semaphore&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（官方解释）Semaphore用于限制可以访问某些资源（物理或逻辑的）的线程数目，他维护了一个许可证集合，有多少资源需要限制就维护多少许可证集合。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;简而言之，Semaphore可以控制线程的并发数量，它初始化定义一个信号量上限，通过调用 acquire() 方法获得令牌和 release() 释放令牌。内部通过维护一个整型的 state 状态来表示信号量（即可用令牌个数），acquire()时会判断state，从而确定是获取令牌亦或阻塞等待获取令牌，获取成功state自减。release()时会将state自增，并从阻塞队列全部唤醒等待线程竞争获得令牌，重复acquire过程，这其中都是通过CAS修改state计数。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class SemaphoreDemo { private static final Random random = new Random(); private static final int SEMAPHORE_PERMIT = 3; public static void main(String[] args) { Semaphore semaphore = new Semaphore(SEMAPHORE_PERMIT); final Semaphore tmpSemaphore = semaphore; // 创建10个线程依次获取令牌，执行完毕释放令牌 for (int i = 1; i &lt;= 10; i++) { new Thread(() -&gt; { try { tmpSemaphore.acquire(); // 获取令牌 // 打印当前可用令牌数和阻塞线程个数 System.out.printf(\"current token count: %d, wait queue count: %d\", tmpSemaphore.availablePermits(), tmpSemaphore.getQueueLength()); TimeUnit.SECONDS.sleep(random.nextInt(5)); // 每个线程休眠随机时间秒 System.out.println(Thread.currentThread().getName() + \" execute done\"); } catch (InterruptedException e) { e.printStackTrace(); } finally { tmpSemaphore.release(); // 释放令牌 } }, \"Thread-\" + i).start(); } System.out.println(\"main thread done\"); // 使用完毕，销毁信号量对象 while (semaphore.getQueueLength() == 0 &amp;&amp; semaphore.availablePermits() == SEMAPHORE_PERMIT) { semaphore = null; } }}/* * output:current token count: 0, wait queue count: 0current token count: 0, wait queue count: 0current token count: 0, wait queue count: 0main thread doneThread-5 execute donecurrent token count: 0, wait queue count: 6Thread-2 execute donecurrent token count: 0, wait queue count: 5Thread-4 execute doneThread-3 execute donecurrent token count: 0, wait queue count: 4current token count: 0, wait queue count: 3Thread-10 execute donecurrent token count: 0, wait queue count: 2Thread-8 execute donecurrent token count: 0, wait queue count: 1Thread-7 execute doneThread-9 execute donecurrent token count: 1, wait queue count: 0Thread-6 execute doneThread-1 execute done */ Semaphore和CountDownLatch都属于信号量类型工具，并且初始化时都规定了信号量上限h，两者的区别在于： Semaphore内部维护state表示可用令牌个数，从0开始增长到h；CountDownLatch内部维护计数器count，从h递减到0结束。 Semaphore通过acquire获得令牌state-1，通过release释放令牌state+1，state是可变化的；CountDownLatch通过countDown实现count递减直到为0； Semaphore可复用，CountDownLatch不可复用，再次使用需要重新创建对象。 CountDownLatch可以通过阻塞保证多个线程最终回归同一时间点并继续向下执行，Semaphore不可以。 10.2.3 CyclicBarrier&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（官方解释）允许一组线程全部等待彼此达到共同屏障点的同步辅助。 循环阻塞在涉及固定大小的线程方的程序中很有用，这些线程必须偶尔等待彼此。 屏障被称为循环 ，因为它可以在等待的线程被释放之后重新使用。 简而言之，就是会让所有线程都等待完成后才会继续下一步行动。线程通过调用 await() 方法表示当前线程已经到达“集合点”位置，进入阻塞，直到所有线程都到达“结合点”位置后，继续各自执行。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class CyclicBarrierDemo { private static final Random random = new Random(); public static void main(String[] args) { // CyclicBarrier初始化集合数量，并允许设置集合后的动作 CyclicBarrier cyclicBarrier = new CyclicBarrier(3, () -&gt; { System.out.println(\"arrive the safe point together\"); }); // 创建3个线程，每个线程依次执行，通过调用await()阻塞等待其他线程集合后方可继续执行 for (int i = 1; i &lt;= 3; i++) { new Thread(() -&gt; { try { // 第一次执行和等待集合 TimeUnit.SECONDS.sleep(random.nextInt(5)); System.out.println(Thread.currentThread().getName() + \" arrival\"); cyclicBarrier.await(); // 等待其他线程到达集合点 // 第二次执行和等待集合 TimeUnit.SECONDS.sleep(random.nextInt(5)); System.out.println(Thread.currentThread().getName() + \" next arrival\"); cyclicBarrier.await(); // 最后一次执行和等待集合 TimeUnit.SECONDS.sleep(random.nextInt(5)); System.out.println(Thread.currentThread().getName() + \" finished\"); cyclicBarrier.await(); } catch (InterruptedException | BrokenBarrierException e) { e.printStackTrace(); } }, \"Thread-\" + i).start(); } }}/* * output:Thread-3 arrival Thread-1 arrivalThread-2 arrivalarrive the safe point togetherThread-2 next arrivalThread-3 next arrivalThread-1 next arrivalarrive the safe point togetherThread-2 finishedThread-3 finishedThread-1 finishedarrive the safe point together */ CyclicBarrier 与 CountDownLatch 区别： CountDownLatch 是一次性的，CyclicBarrier 是可循环利用的。 CountDownLatch 参与的线程的职责是不一样的，各自执行结束之后，最终回归初始线程（主线程）。CyclicBarrier 参与的线程职责是一样的。 11 阻塞队列&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BlockingQueue即阻塞队列，基于ReentrantLock实现线程安全。他是“生产者-消费者”模型的主要调度容器。最为基础接口它实现类如下图所示： ArrayBlockingQueue：是一个用数组实现的有界阻塞队列，此队列按照先进先出（FIFO）的原则对元素进行排序。支持公平锁和非公平锁。 LinkedBlockingQueue：一个由链表结构组成的有界队列，此队列的长度为Integer.MAX_VALUE。 PriorityBlockingQueue： 一个支持线程优先级排序的无界队列，默认自然序进行排序，也可以自定义实现compareTo()方法来指定元素排序规则，不能保证同优先级元素的顺序。 DelayQueue： 基于PriorityBlockingQueue实现延迟获取的无界队列，在创建元素时，可以指定多久才能从队列中获取当前元素。只有延时期满后才能从队列中获取元素。 SynchronousQueue： 不存储元素的阻塞队列，每一个put操作必须等待take操作，否则不能添加元素。支持公平锁和非公平锁。是一种单线程模型的阻塞队列。 LinkedBlockingDeque： 一个由链表结构组成的双向阻塞队列。队列头部和尾部都可以添加和移除元素，多线程并发时，可以将锁的竞争最多降到一半。 阻塞队列的方法 异常反馈 null反馈 阻塞 超时 压入 add() offer() put() offer() 弹出 remove() poll() take() poll() 队首元素 element peek 生产者消费者简单实现 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;生产者-消费者是经典的同步模型，包含：多生产多消费、多生产单消费、单生产多消费和单生产单消费。以下以单生产多消费介绍。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public class ProducerAndConsumerDemo { private static final BlockingQueue&lt;String&gt; queue = new LinkedBlockingQueue&lt;&gt;(); private static final int INVENTORY = 50; public static void main(String[] args) { new Thread(new Producer()).start(); new Thread(new Consumer(), \"Consumer1\").start(); new Thread(new Consumer(), \"Consumer2\").start(); } // 生产者对象 private static class Producer implements Runnable { @Override public void run() { int count = 1; while (queue.size() &lt; INVENTORY) { try { TimeUnit.MILLISECONDS.sleep(500); String goods = \"goods-\" + count++; queue.put(goods); System.out.printf(\"produce %s \\n\", goods); } catch (InterruptedException e) { e.printStackTrace(); } } System.out.println(\"# Produce over #\"); } } // 消费者对象 private static class Consumer implements Runnable { @Override public void run() { while (true) { try { TimeUnit.SECONDS.sleep(2); System.out.printf(\"%s consume %s \\n\", Thread.currentThread().getName(), queue.take()); } catch (InterruptedException e) { e.printStackTrace(); } } } }}/* * outputproduce goods-1 produce goods-2 produce goods-3 Consumer1 consume goods-1 Consumer2 consume goods-2 produce goods-4 produce goods-5 produce goods-6 produce goods-7 Consumer1 consume goods-3 Consumer2 consume goods-4 produce goods-8 produce goods-9 produce goods-10 produce goods-11 Consumer1 consume goods-5 Consumer2 consume goods-6... */ 12 协程和纤程&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Java中的线程是一种系统级线程，它运行在用户态（User），从用户态通过JVM调起内核态的线程运行，这其中每个线程的创建，以及在用户态和内核态的状态切换开销都是会消耗可观资源的，为了节省资源开销Java采用线程池的方式管理有限的线程资源。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;协程（Coroutine），是一种轻量级的线程(lightweight thread)，它只存在于用户态，一个线程可以包含多个协程，目前在GO语言中广泛使用，通过go关键字就可以创建一个协程，它的工作原理是：协程负责执行线程中的一段代码片段，当需要切换协程时，会将当前协程执行的上下文进行保存，同时让出CPU资源，恢复被调用协程的上下文继续执行。与线程最大的区别在于：协程只工作在用户态，减少了和内核态切换的开销；线程由系统调度器执行，抢占式调度策略难以控制线程的有序调用，协程可以完全依赖开发者控制调用。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Windows平台运行或者由微软命名的协程称为纤程（Fiber），从2018年开始，由OpenJDK主导的Loom项目旨在从JVM层面实现协程，但作为程序语言的“老者”，考虑到众多程序的稳定，Java想要在协程上做出突破是比较困难的，但作为未来并发编程的主导者，协程的上岸只是时间问题（Java的很多问题都是时间问题）。目前Java生态中三方库形式的协程支持可以看Quasar 项目，JVM层面的协程实现可以关注AJDK（Alibaba JDK）、作为JVM语言的佼佼者，Kotlin也通过编译器和类库提供支持。 作为Java宣传的保留项目，Oracle这一次喊出了：“Code like sync，Work like async。” 总结： 使用协程的优点： 完全由应用程序控制。 运行在用户态，减少与内核态转换的系统开销。 基于上下文切换协程，转换速度快。 因为协程调度可控，所以更符合人类语义且避免了线程安全。 协程的缺点： 因为在线程中创建和运行，无法充分利用多核CPU能力，即CPU密集型。 协程的阻塞必将导致所属线程的阻塞。","link":"/2020/05/27/5%20%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E5%A4%9A%E7%BA%BF%E7%A8%8B%EF%BC%88%E4%B8%89%EF%BC%89/"},{"title":"Java源码——CopyOnWriteArrayList","text":"CopyOnWriteArrayList &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Copy-On-Write（COW）技术是Linux底层采用的策略，称为写时复制技术，它的核心思想是针对读多写少的场景，多线程读时不上锁，只有当写时会对数据进行拷贝并加锁，在产生的副本上修改，并最终合入源数据，在JUC中，CopyOnWriteArrayList是针对ArrayList的COW版本，CopyOnWriteArraySet是针对Set的COW版本，以前者为例，在读多写少的场景时，比如订单地址、缓存系统等，读取列表是支持多线程同时访问的，并不会产生读阻塞（与读写锁的区别），当修改列表数据时会创建当前集合的副本，加锁并修改，最终将数据集成到集合中。由此可以看出： CopyOnWriteArrayList在多线程读操作时，与ArrayList一致，不会产生读阻塞。 修改时创建原集合大小的副本，会占用更多的系统内存，如果并发写过大时就有可能产生频繁的Major GC，甚至出现OOM。此时需要考虑替换其他集合。 存在并发写操作时，读操作仍旧只能读取旧数据。 成员变量 123456public class CopyOnWriteArrayList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable { final transient Object lock = new Object(); // 内置监视器，使用synchronized private transient volatile Object[] array; //数组容器} get 方法 12345678// 通过下标正常读取数组中的数据public E get(int index) { return elementAt(getArray(), index);}static &lt;E&gt; E elementAt(Object[] a, int index) { return (E) a[index];} add 、set 和 remove 方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869// 不指定索引新增public boolean add(E e) { synchronized (lock) { // 写入时加锁 Object[] es = getArray(); int len = es.length; es = Arrays.copyOf(es, len + 1); // 创建当前数组的副本并修改元素 es[len] = e; setArray(es); return true; }}// 指定索引新增，原理相同public void add(int index, E element) { synchronized (lock) { Object[] es = getArray(); int len = es.length; if (index &gt; len || index &lt; 0) throw new IndexOutOfBoundsException(outOfBounds(index, len)); Object[] newElements; int numMoved = len - index; if (numMoved == 0) newElements = Arrays.copyOf(es, len + 1); else { newElements = new Object[len + 1]; System.arraycopy(es, 0, newElements, 0, index); System.arraycopy(es, index, newElements, index + 1, numMoved); } newElements[index] = element; setArray(newElements); }}// set方法，同理public E set(int index, E element) { synchronized (lock) { // 修改前加锁 Object[] es = getArray(); E oldValue = elementAt(es, index); if (oldValue != element) { es = es.clone(); // 创建副本 es[index] = element; setArray(es); } return oldValue; }}// 删除方法同理public E remove(int index) { synchronized (lock) { Object[] es = getArray(); int len = es.length; E oldValue = elementAt(es, index); int numMoved = len - index - 1; Object[] newElements; if (numMoved == 0) newElements = Arrays.copyOf(es, len - 1); else { newElements = new Object[len - 1]; System.arraycopy(es, 0, newElements, 0, index); System.arraycopy(es, index + 1, newElements, index, numMoved); } setArray(newElements); return oldValue; }} COWIterator 迭代器 我们知道每一个集合都拥有迭代器，CopyOnWriteArrayList采用COWIterator迭代器，它是通过使用当前数组的快照实现，也就是说对并发写不敏感。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364static final class COWIterator&lt;E&gt; implements ListIterator&lt;E&gt; { private final Object[] snapshot; // 当前列表的快照对象 private int cursor; //迭代器的游标 COWIterator(Object[] es, int initialCursor) { cursor = initialCursor; snapshot = es; } public boolean hasNext() { return cursor &lt; snapshot.length; } public boolean hasPrevious() { return cursor &gt; 0; } @SuppressWarnings(\"unchecked\") public E next() { if (! hasNext()) throw new NoSuchElementException(); return (E) snapshot[cursor++]; } @SuppressWarnings(\"unchecked\") public E previous() { if (! hasPrevious()) throw new NoSuchElementException(); return (E) snapshot[--cursor]; } public int nextIndex() { return cursor; } public int previousIndex() { return cursor - 1; } // 不支持remove 、 set 、 add方法 public void remove() { throw new UnsupportedOperationException(); } public void set(E e) { throw new UnsupportedOperationException(); } public void add(E e) { throw new UnsupportedOperationException(); } // lambda方法的实现 @Override public void forEachRemaining(Consumer&lt;? super E&gt; action) { Objects.requireNonNull(action); final int size = snapshot.length; int i = cursor; cursor = size; for (; i &lt; size; i++) action.accept(elementAt(snapshot, i)); } }","link":"/2020/05/11/4%20%E6%BA%90%E7%A0%81/Java%E6%BA%90%E7%A0%81%E2%80%94%E2%80%94CopyOnWriteArrayList/"}],"tags":[{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"MySQL优化","slug":"MySQL优化","link":"/tags/MySQL%E4%BC%98%E5%8C%96/"},{"name":"数据结构","slug":"数据结构","link":"/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"缓存","slug":"缓存","link":"/tags/%E7%BC%93%E5%AD%98/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"源码","slug":"源码","link":"/tags/%E6%BA%90%E7%A0%81/"},{"name":"多线程","slug":"多线程","link":"/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"AQS","slug":"AQS","link":"/tags/AQS/"},{"name":"JUC","slug":"JUC","link":"/tags/JUC/"},{"name":"线程池","slug":"线程池","link":"/tags/%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{"name":"协程","slug":"协程","link":"/tags/%E5%8D%8F%E7%A8%8B/"},{"name":"纤程","slug":"纤程","link":"/tags/%E7%BA%A4%E7%A8%8B/"},{"name":"COW","slug":"COW","link":"/tags/COW/"}],"categories":[]}